{
  
    
        "post0": {
            "title": "2022/01/17/MON",
            "content": "&#44400;&#51665; &#54217;&#44032; . 군집화는 분류와 유사해 보일 수 있으나 성격이 많이 다르다. 데이터 내에 숨어 있는 별도의 그룹을 찾아서 의미를 부여하거나 동일한 분류 값에 속하더라도 그 안에서 더 세분화된 군집화를 추구하거나 서로 다른 분류 값의 데이터도 더 넓은 군집화 레벨화 등의 영역을 가지고 있다. 군집화는 비지도 학습 특성상 어떠한 지표라도 정확하게 성능을 평가하기는 어렵다. 그럼에도 불구하고 군집화의 성능을 평가하는 대표적인 방법으로 실루엣 분석이 있다. . 실루엣 분석 : 각 군집 간의 거리가 얼마나 효율적으로 분리돼 있는지를 나타낸다. 효율적으로 잘 분리됐다는 것은 다른 군집과의 거리는 떨어져 있고 동일 군집기리의 데이터는 서로 가깝게 잘 뭉쳐 있다는 의미이다. 군집화가 잘 될수록 개별 군집은 비슷한 정도의 여유공간을 가지고 떨어져 있을 것이다. 실루엣 분석은 실루엣 계수를 기반으로 한다. 실루엣 계수는 개별 데이터가 각각이 가지는 군집화 지표이다. 개별 데이터가 가지는 실루엣 계수는 해당 데이터가 같은 군집 내의 데이터와 얼마나 가깝게 군집화돼 있고, 다른 군집에 있는 데이터와는 얼마나 멀리 분리돼 있는지를 나타내는 지표이다. | 특정 데이터 포인트의 실루엣 계수 값은 a(i),b(i)를 기반으로 계산된다. 두 군집 간의 거리가 얼마나 떨어져 있는가의 값은 b(i)-a(i)이며 이 값을 정규화하기 위해 MAX(a(i),b(i))값으로 나눈다. | 실루엣 계수는 -1에서 1 사이의 값을 가지며, 1로 가까워질수록 근처의 군집과 더 멀리 떨어져 있다는 것이고 0에 가까울수록 근처의 군집과 가까워진다는 것이다. - 값은 아예 다른 군집에 데이터 포인트가 할당됐음을 뜻한다. | . 좋은 군집화가 되기 위한 조건 . 전체 실루엣 계수의 평균값, 즉 사이킷런의 silhouette_score() 값은 0~1사이의 값을 가지며, 1에 가까울수록 좋다. | 전체 실루엣 계수의 평균값과 더불어 개별 군집의 평균값의 편차가 크진 않아야 한다. 즉, 개별 군집의 실루엣 계수 평균값이 전체 실루엣 계수 평균값에서 크게 벗어나지 않는 것이 중요하다. | . 앞의 붓꽃 데이터 세트의 군집화 결과를 실루엣 분석으로 평가해보자 . from sklearn.preprocessing import scale from sklearn.datasets import load_iris from sklearn.cluster import KMeans # 실루엣 분석 metric 값을 구하기 위한 API 추가 from sklearn.metrics import silhouette_samples, silhouette_score import matplotlib.pyplot as plt import numpy as np import pandas as pd %matplotlib inline iris = load_iris() feature_names = [&#39;sepal_length&#39;,&#39;sepal_width&#39;,&#39;petal_length&#39;,&#39;petal_width&#39;] irisDF = pd.DataFrame(data=iris.data, columns=feature_names) kmeans = KMeans(n_clusters=3, init=&#39;k-means++&#39;, max_iter=300,random_state=0).fit(irisDF) irisDF[&#39;cluster&#39;] = kmeans.labels_ # iris 의 모든 개별 데이터에 실루엣 계수값을 구함. score_samples = silhouette_samples(iris.data, irisDF[&#39;cluster&#39;]) print(&#39;silhouette_samples( ) return 값의 shape&#39; , score_samples.shape) # irisDF에 실루엣 계수 컬럼 추가 irisDF[&#39;silhouette_coeff&#39;] = score_samples # 모든 데이터의 평균 실루엣 계수값을 구함. average_score = silhouette_score(iris.data, irisDF[&#39;cluster&#39;]) print(&#39;붓꽃 데이터셋 Silhouette Analysis Score:{0:.3f}&#39;.format(average_score)) irisDF.head(3) . silhouette_samples( ) return 값의 shape (150,) 붓꽃 데이터셋 Silhouette Analysis Score:0.553 . sepal_length sepal_width petal_length petal_width cluster silhouette_coeff . 0 5.1 | 3.5 | 1.4 | 0.2 | 1 | 0.852955 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 1 | 0.815495 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 1 | 0.829315 | . 평균 실루엣 계수 값이 약 0.553이다. 1번 군집의 경우 0.8 이상의 높은 실루엣 계수 값을 나타내고 있다. 이는 1번 군집이 아닌 다른 군집의 경우 실루엣 계수 값이 평균 0.553 보다 낮기 때문일 것이다. | 군집별 평균 실루엣 계수 값을 알아보자 | . irisDF.groupby(&#39;cluster&#39;)[&#39;silhouette_coeff&#39;].mean() . cluster 0 0.451105 1 0.798140 2 0.417320 Name: silhouette_coeff, dtype: float64 . 1번 군집은 실루엣 계수 평균 값이 약 0.79인 반면, 다른 군집의 평균 실루엣 계수 값은 낮다. | . &#44400;&#51665;&#48324; &#54217;&#44512; &#49892;&#47336;&#50659; &#44228;&#49688;&#51032; &#49884;&#44033;&#54868;&#47484; &#53685;&#54620; &#44400;&#51665; &#44060;&#49688; &#52572;&#51201;&#54868; &#48169;&#48277; . 전체 데이터의 평균 실루엣 계수 값이 높다고 해서 반드시 최적의 군집 개수로 군집화가 잘 됐다고 볼 수 없다. 특정 군집 내의 실루엣 계수 값만 너무 높고, 다른 군집은 데이터끼리의 거리가 너무 떨어져 있어 실루엣 계수 값이 낮아져도 평균적으로 높은 값을 가질 수 있다. 개별 군집별로 적당히 분리된 거리를 유지하면서도 군집 내의 데이터가 서로 뭉쳐 있는 경우에 K-평균의 적절한 군집 개수가 설정됐다고 할 수 있다. . def visualize_silhouette(cluster_lists, X_features): from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score import matplotlib.pyplot as plt import matplotlib.cm as cm import math # 입력값으로 클러스터링 갯수들을 리스트로 받아서, 각 갯수별로 클러스터링을 적용하고 실루엣 개수를 구함 n_cols = len(cluster_lists) # plt.subplots()으로 리스트에 기재된 클러스터링 수만큼의 sub figures를 가지는 axs 생성 fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols) # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 실루엣 개수 시각화 for ind, n_cluster in enumerate(cluster_lists): # KMeans 클러스터링 수행하고, 실루엣 스코어와 개별 데이터의 실루엣 값 계산. clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0) cluster_labels = clusterer.fit_predict(X_features) sil_avg = silhouette_score(X_features, cluster_labels) sil_values = silhouette_samples(X_features, cluster_labels) y_lower = 10 axs[ind].set_title(&#39;Number of Cluster : &#39;+ str(n_cluster)+&#39; n&#39; &#39;Silhouette Score :&#39; + str(round(sil_avg,3)) ) axs[ind].set_xlabel(&quot;The silhouette coefficient values&quot;) axs[ind].set_ylabel(&quot;Cluster label&quot;) axs[ind].set_xlim([-0.1, 1]) axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10]) axs[ind].set_yticks([]) # Clear the yaxis labels / ticks axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1]) # 클러스터링 갯수별로 fill_betweenx( )형태의 막대 그래프 표현. for i in range(n_cluster): ith_cluster_sil_values = sil_values[cluster_labels==i] ith_cluster_sil_values.sort() size_cluster_i = ith_cluster_sil_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i) / n_cluster) axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values, facecolor=color, edgecolor=color, alpha=0.7) axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) y_lower = y_upper + 10 axs[ind].axvline(x=sil_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;) . from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1, center_box=(-10.0, 10.0), shuffle=True, random_state=1) # cluster 개수를 2개, 3개, 4개, 5개 일때의 클러스터별 실루엣 계수 평균값을 시각화 visualize_silhouette([ 2, 3, 4, 5], X) . 위 그림에서 첫 번째 그림을 살펴보자. 해당 그림은 개별 군집에 속하는 데이터의 실루엣 계수를 2차원으로 나타낸 것이다. X축은 실루엣 계수 값이고, Y축은 개별 군집과 이에 속하는 데이터이다. 개별 군집은 Y축에 숫자값으로 0,1로 표시돼 있으며 점선으로 표시된 선은 전체 평균 실루엣 계수 값을 나타낸다. 이로 판단해 볼 때 해당 그림의 1번 군집의 모든 데이터는 평균 실루엣 계수 값 이상이지만, 2번 군집의 경우는 평균보다 적은 데이터 값이 매우 많다. 그에 반해 세 번째 그림을 살펴보자. 첫 번째 그림의 평균 실루엣 계수 값보다는 낮은 값이지만 개별 군집의 평균 실루엣 계수 값이 비교적 균일하게 위치하고 있다. 즉 군집이 2개인 경우보다는 평균 실루엣 계수 값이 작지만 4개인 경우가 가장 이상적인 군집화 개수로 판단할 수 있다. | . 이번에는 붓꽃 데이터를 이용해 K-평균 수행 시 최적의 군집 개수를 알아보자 | . from sklearn.datasets import load_iris iris=load_iris() visualize_silhouette([ 2, 3, 4,5 ], iris.data) . 붓꽃 데이터를 K-평균으로 군집화할 경우에는 군집 개수를 2개로 하는 것이 가장 좋아 보인다. | . 실루엣 계수를 통한 K-평균 군집 평가 방법은 직관적으로 이해하기 쉽지만, 각 데이터별로 다른 데이터와의 거리를 반복적으로 계산해야 하므로 데이터의 양이 늘어나면 수행 시간이 크게 늘어난다. 군집별로 임의의 데이터를 샘플링해 실루엣 계수를 평가하는 방안을 고민하여 대용량의 데이터에 대해 대응할 수 있어야 한다. . &#54217;&#44512; &#51060;&#46041; (Mean Shift) . K-평균과 유사하게 중심을 군집의 중심으로 지속적으로 움직이면서 군집화를 수행하지만 K-평균이 중심에 소속된 데이터의 평균 거리 중심으로 이동하는 데 반해, 평균 이동은 중심을 데이터가 모여 있는 밀도가 가장 높은 곳으로 이동시킨다. 평균 이동 군집화는 데이터의 분포도를 이용해 군집 중심점을 찾는다. 확률 밀도 함수를 이용하며 가장 집중적으로 데이터가 모여있어 확률 밀도 함수가 피크인 점을 군집 중심점으로 선정하며, 일반적으로 주어진 모델의 확률 밀도 함수를 찾기 위해서 KDE를 이용한다. 평균 이동 군집화는 특정 데이터를 반경 내의 데이터 분포 확률 밀도가 가장 높은 곳으로 이동하기 위해 주변 데이터와의 거리 값을 KDE 함수 값으로 입력한 뒤 그 반환 값을 현재 위치에서 업데이트하면서 이동하는 방식을 취한다. 이러한 방식을 전체 데이터에 반복적으로 적용하면서 데이터의 군집 중심점을 찾아낸다. . KDE는 커널 함수를 통해 어떤 변수의 확률 밀도 함수를 측정하는 대표적인 방법이다. 관측된 데이터 각각에 커널 함수를 적용한 값을 모두 더한 뒤 데이터 건수로 나눠 확률 밀도 함수를 추정한다. 확률 밀도 함수는 확률 변수의 분포를 나타내는 함수로 널리 알려진 정규 분포 함수를 포함해 감마 분포, t-분포 등이 있다. 확률 밀도 함수를 알면 특정 변수가 어떤 값을 갖게 될지에 대한 확률을 알게 되므로 이를 통해 변수의 특성, 확률 분포 등 변수의 많은 요소를 알 수 있다. 대표적인 커널 함수로서 가우시안 분포 함수가 사용된다. . KDE에서 대역폭은 KDE의 형태를 부드럽게 하거나 평활화하는 데 적용되며, 이 대역폭을 어떻게 설정하느냐에 따라 확률 밀도 추정 성능을 크게 좌우할 수 있다. 작은 대역폭 값은 좁고 뾰족한 KDE를 가지게 되며, 이는 변동성이 큰 방식으로 확률 밀도 함수를 추정하므로 과적합하기 쉽다. 따라서 적절한 KDE의 대역폭을 계산하는 것인 KDE 기반의 평균 이동 군집화에서 매우 중요하다. . 일반적으로 평균 이동 군집화는 대역폭이 클수록 평활화된 KDE로 인해 적은 수의 군집 중심점을 가지며 대역폭이 작을 수록 많은 수의 군집 중심점을 갖는다. 또한 평균 이동 군집화는 군집의 개수를 지정하지 않으며 오직 대역폭의 크기에 따라 군집화를 수행한다. 대역폭 크기 설정이 군집화의 품질에 큰 영향을 미치기 때문에 사이킷런은 최적의 대역폭 계산을 위해 estimate_bandwidth() 함수를 제공한다. . import numpy as np from sklearn.datasets import make_blobs from sklearn.cluster import MeanShift X, y = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=0.7, random_state=0) meanshift= MeanShift(bandwidth=0.8) cluster_labels = meanshift.fit_predict(X) print(&#39;cluster labels 유형:&#39;, np.unique(cluster_labels)) . cluster labels 유형: [0 1 2 3 4 5] . 군집이 6개로 분류됐다. 지나치게 세분화돼 군집화됐다. 일반적으로 bandwidth 값을 작게 할수록 군집 개수가 많아진다. 이번에 bandwidth를 살짝 높인 1로 해서 수행해보자 | . meanshift= MeanShift(bandwidth=1) cluster_labels = meanshift.fit_predict(X) print(&#39;cluster labels 유형:&#39;, np.unique(cluster_labels)) . cluster labels 유형: [0 1 2] . 군집화가 적절히 잘 이루어진 것으로 보인다. | 사이킷런은 최적화된 bandwidth 값을 찾기 위해서 estimate_bandwidth() 함수를 제공한다. | . from sklearn.cluster import estimate_bandwidth bandwidth = estimate_bandwidth(X) print(&#39;bandwidth 값:&#39;, round(bandwidth,3)) . bandwidth 값: 1.816 . import pandas as pd clusterDF = pd.DataFrame(data=X, columns=[&#39;ftr1&#39;, &#39;ftr2&#39;]) clusterDF[&#39;target&#39;] = y # estimate_bandwidth()로 최적의 bandwidth 계산 best_bandwidth = estimate_bandwidth(X, quantile=0.25) meanshift= MeanShift(best_bandwidth) cluster_labels = meanshift.fit_predict(X) print(&#39;cluster labels 유형:&#39;,np.unique(cluster_labels)) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages sklearn utils validation.py:67: FutureWarning: Pass bandwidth=1.5182908825307768 as keyword args. From version 0.25 passing these as positional arguments will result in an error warnings.warn(&#34;Pass {} as keyword args. From version 0.25 &#34; . cluster labels 유형: [0 1 2] . 3개의 군집으로 구성됨을 알 수 있다. | 구성된 3개의 군집을 시각화해보자 | . import matplotlib.pyplot as plt %matplotlib inline clusterDF[&#39;meanshift_label&#39;] = cluster_labels centers = meanshift.cluster_centers_ unique_labels = np.unique(cluster_labels) markers=[&#39;o&#39;, &#39;s&#39;, &#39;^&#39;, &#39;x&#39;, &#39;*&#39;] for label in unique_labels: label_cluster = clusterDF[clusterDF[&#39;meanshift_label&#39;]==label] center_x_y = centers[label] # 군집별로 다른 marker로 scatter plot 적용 plt.scatter(x=label_cluster[&#39;ftr1&#39;], y=label_cluster[&#39;ftr2&#39;], edgecolor=&#39;k&#39;, marker=markers[label] ) # 군집별 중심 시각화 plt.scatter(x=center_x_y[0], y=center_x_y[1], s=200, color=&#39;white&#39;, edgecolor=&#39;k&#39;, alpha=0.9, marker=markers[label]) plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color=&#39;k&#39;, edgecolor=&#39;k&#39;, marker=&#39;$%d$&#39; % label) plt.show() . target값과 군집 label값을 비교해보자. | 1:1로 잘 매칭됐음을 알 수 있다. | . print(clusterDF.groupby(&#39;target&#39;)[&#39;meanshift_label&#39;].value_counts()) . target meanshift_label 0 2 67 1 0 67 2 1 66 Name: meanshift_label, dtype: int64 . 평균 이동의 장점은 데이터 세트의 형태를 특정 형태로 가정한다든가, 특정 분포도 기반의 모델로 가정하지 않기 때문에 좀 더 유연한 군집화가 가능한 것이다. 또한 이상치의 영향력도 크지 않으며, 미리 군집의 개수를 정할 필요도 없다. 하지만 알고리즘의 수행시간이 오래 걸리고 무엇보다도 band-width의 크기에 따른 군집화 영향도가 매우 크다. | 이 같은 특징 때문에 일반적으로 평균 이동 군집화 기법은 분석 업무 기반의 데이터 세트보다는 컴퓨터 비전 영역에서 더 많이 사용된다. 이미지나 영상 데이터에서 특정 개체를 구분하거나 움직임을 추적하는 데 뛰어난 역할을 수행하는 알고리즘이다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/17/intro.html",
            "relUrl": "/2022/01/17/intro.html",
            "date": " • Jan 17, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "2022/01/16/SUN",
            "content": "SVD . PCA와 유사한 행렬 분해 기법을 이용한다. SVD는 정방행렬뿐만 아니라 행과 열의 크기가 다른 행렬에도 적용할 수 있다. SVD는 특이값 분해로서 행렬 U와 V에 속한 벡터는 특이벡터 모든 특이벡터는 서로 직교하는 성질을 갖는다. 시그마는 대각행렬이며 시그마가 위치한 0이 아닌 값이 바로 행렬 A의 특이값이다. 일반적으로는 시그마의 비대각인 부분과 대각원소 중에 특이값이 0인 부분도 모두 제거하고 제거된 시그마에 대응되는 U와 V원소도 함께 제거해 차원을 줄인 형태로 SVD를 적용한다. SVD를 적용하면 A읭 차원이 MxN일때 U의 차원을 M x P, 시그마의 차원을 P x P 행렬 V의 차원을 P x N으로 분해한다. . Truncated SVD . 시그마의 대각원소 중에 상위 몇 개만 추출해서 여기에 대응되는 U와 V의 원소도 함께 제거해 더욱 차원을 줄인 형태로 분해하는 것이다. . import numpy as np from numpy.linalg import svd # 4X4 Random 행렬 a 생성 # 행렬의 개별 로우끼리의 의존성을 없애기 위해 랜덤행렬을 생선한다. np.random.seed(121) a = np.random.randn(4,4) print(np.round(a, 3)) . [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.014 0.63 1.71 -1.327] [ 0.402 -0.191 1.404 -1.969]] . 이렇게 생성된 a행렬에 SVD를 적용해 U,Sigma,Vt를 도출해보자. 시그마 행렬의 경우 0이 아닌 값의 경우만 1차원 행렬로 표현한다. | . U, Sigma, Vt = svd(a) print(U.shape, Sigma.shape, Vt.shape) print(&#39;U matrix: n&#39;,np.round(U, 3)) print(&#39;Sigma Value: n&#39;,np.round(Sigma, 3)) print(&#39;V transpose matrix: n&#39;,np.round(Vt, 3)) . (4, 4) (4,) (4, 4) U matrix: [[-0.079 -0.318 0.867 0.376] [ 0.383 0.787 0.12 0.469] [ 0.656 0.022 0.357 -0.664] [ 0.645 -0.529 -0.328 0.444]] Sigma Value: [3.423 2.023 0.463 0.079] V transpose matrix: [[ 0.041 0.224 0.786 -0.574] [-0.2 0.562 0.37 0.712] [-0.778 0.395 -0.333 -0.357] [-0.593 -0.692 0.366 0.189]] . 분해된 이 U,Sigma,Vt를 이용해 다시 원본 행렬로 정확히 복원되는지 확인해보자. U,Sigma,Vt를 내적하면 된다. 한 가지 유희할 것은 Sigma의 경우 0이 아닌 값만 1차원으로 추출했었으므로 다시 0을 포함한 대칭행렬로 변환한 뒤에 내적을 수행해야 한다는 점이다. | . Sigma_mat = np.diag(Sigma) a_ = np.dot(np.dot(U, Sigma_mat), Vt) print(np.round(a_, 3)) . [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.014 0.63 1.71 -1.327] [ 0.402 -0.191 1.404 -1.969]] . 지금까진 로우간 의존성이 없는 경우였으며 로우간 의존성이 있을 경우에 대해 살펴보자 | 의존성을 부여하기 위해 일부 조작해주자 | . a[2] = a[0] + a[1] a[3] = a[0] print(np.round(a,3)) . [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.542 0.899 1.041 -0.073] [-0.212 -0.285 -0.574 -0.44 ]] . U, Sigma, Vt = svd(a) print(U.shape, Sigma.shape, Vt.shape) print(&#39;Sigma Value: n&#39;,np.round(Sigma,3)) . (4, 4) (4,) (4, 4) Sigma Value: [2.663 0.807 0. 0. ] . 이전과 차원은 같지만 Sigma 값 중 2개가 0으로 변했다. 즉, 선형 독립인 로우 벡터의 개수가 2개라는 의미이다. 즉 행렬의 Rank가 2개이다. 이번에는 U,Sigma,Vt의 전체 데이터를 이용하지 않고 Sigma의 0에 대응되는 U,Sigma,Vt의 데이터를 제외하고 복원해보자. | . U_ = U[:, :2] Sigma_ = np.diag(Sigma[:2]) # V 전치 행렬의 경우는 앞 2행만 추출 Vt_ = Vt[:2] print(U_.shape, Sigma_.shape, Vt_.shape) # U, Sigma, Vt의 내적을 수행하며, 다시 원본 행렬 복원 a_ = np.dot(np.dot(U_,Sigma_), Vt_) print(np.round(a_, 3)) . (4, 2) (2, 2) (2, 4) [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.542 0.899 1.041 -0.073] [-0.212 -0.285 -0.574 -0.44 ]] . Truncated SVD를 이용해 행렬을 분해해보자. 인위적으로 더 작은 차원으로 분해하기 때문에 원본 행렬을 정확하기 다시 원상복구할 순 없다. 하지만 데이터 정보가 압축되어 분해됨에도 불구하고 상당한 수준으로 원본행렬을 근사할 수 있다. 당연한 얘기일테지만. 원래 차원의 차수에 가깝게 잘라낼수록(Truncate) 원본 행렬에 더 가깝게 복원할 수 있다. | Truncated SVD는 희소 행렬로만 지원돼서 scipy.sparse.linalg.svds를 이용해야 한다. | 임의의 행렬 6x6을 Normal SVD로, Truncated SVD로 분해해본뒤 결과들을 비교해보자 | . import numpy as np from scipy.sparse.linalg import svds from scipy.linalg import svd # 원본 행렬을 출력하고, SVD를 적용할 경우 U, Sigma, Vt 의 차원 확인 np.random.seed(121) matrix = np.random.random((6, 6)) print(&#39;원본 행렬: n&#39;,matrix) U, Sigma, Vt = svd(matrix, full_matrices=False) print(&#39; n분해 행렬 차원:&#39;,U.shape, Sigma.shape, Vt.shape) print(&#39; nSigma값 행렬:&#39;, Sigma) # Truncated SVD로 Sigma 행렬의 특이값을 4개로 하여 Truncated SVD 수행. num_components = 5 U_tr, Sigma_tr, Vt_tr = svds(matrix, k=num_components) print(&#39; nTruncated SVD 분해 행렬 차원:&#39;,U_tr.shape, Sigma_tr.shape, Vt_tr.shape) print(&#39; nTruncated SVD Sigma값 행렬:&#39;, Sigma_tr) matrix_tr = np.dot(np.dot(U_tr,np.diag(Sigma_tr)), Vt_tr) # output of TruncatedSVD print(&#39; nTruncated SVD로 분해 후 복원 행렬: n&#39;, matrix_tr) . 원본 행렬: [[0.11133083 0.21076757 0.23296249 0.15194456 0.83017814 0.40791941] [0.5557906 0.74552394 0.24849976 0.9686594 0.95268418 0.48984885] [0.01829731 0.85760612 0.40493829 0.62247394 0.29537149 0.92958852] [0.4056155 0.56730065 0.24575605 0.22573721 0.03827786 0.58098021] [0.82925331 0.77326256 0.94693849 0.73632338 0.67328275 0.74517176] [0.51161442 0.46920965 0.6439515 0.82081228 0.14548493 0.01806415]] 분해 행렬 차원: (6, 6) (6,) (6, 6) Sigma값 행렬: [3.2535007 0.88116505 0.83865238 0.55463089 0.35834824 0.0349925 ] Truncated SVD 분해 행렬 차원: (6, 5) (5,) (5, 6) Truncated SVD Sigma값 행렬: [0.35834824 0.55463089 0.83865238 0.88116505 3.2535007 ] Truncated SVD로 분해 후 복원 행렬: [[0.11368271 0.19721195 0.23106956 0.15961551 0.82758207 0.41695496] [0.55500167 0.75007112 0.24913473 0.96608621 0.95355502 0.48681791] [0.01789183 0.85994318 0.40526464 0.62115143 0.29581906 0.92803075] [0.40782587 0.55456069 0.24397702 0.23294659 0.035838 0.58947208] [0.82711496 0.78558742 0.94865955 0.7293489 0.67564311 0.73695659] [0.5136488 0.45748403 0.64231412 0.82744766 0.14323933 0.0258799 ]] . 사이킷런의 Truncated SVD클래스는 U,Sigma,Vt 행렬을 반환하진 않는다. 사이킷런의 Truncated SVD 클래스는 PCA와 유사하게 작동. 원본데이터를 Truncated SVD 방식으로 분해된 U*Sigma행렬에 선형 변환해 생성한다. | . from sklearn.decomposition import TruncatedSVD, PCA from sklearn.datasets import load_iris import matplotlib.pyplot as plt %matplotlib inline iris = load_iris() iris_ftrs = iris.data # 2개의 주요 component로 TruncatedSVD 변환 tsvd = TruncatedSVD(n_components=2) tsvd.fit(iris_ftrs) iris_tsvd = tsvd.transform(iris_ftrs) # Scatter plot 2차원으로 TruncatedSVD 변환 된 데이터 표현. 품종은 색깔로 구분 plt.scatter(x=iris_tsvd[:,0], y= iris_tsvd[:,1], c= iris.target) plt.xlabel(&#39;TruncatedSVD Component 1&#39;) plt.ylabel(&#39;TruncatedSVD Component 2&#39;) . Text(0, 0.5, &#39;TruncatedSVD Component 2&#39;) . 왼쪽에 있는 그림이 Truncated SVD로 변환된 붓꽃 데이터 세트. 오른쪽은 비교를 위해 PCA로 변환된 붓꽃 데이터 세트이다. | 사이킷런의 Truncated SVD와 PCA 클래스 구현을 조금 더 자세히 들여다보면 두 개 클래스 모두 SVD를 이용해 행렬을 분해한다. 붓꽃 데이터 세트를 스케일링을 정규분포 변환한 뒤에 Truncated SVD와 PCA 클래스 변환을 해보면 두 개가 거의 동일함을 알 수 있다. | . from sklearn.preprocessing import StandardScaler # iris 데이터를 StandardScaler로 변환 scaler = StandardScaler() iris_scaled = scaler.fit_transform(iris_ftrs) # 스케일링된 데이터를 기반으로 TruncatedSVD 변환 수행 tsvd = TruncatedSVD(n_components=2) tsvd.fit(iris_scaled) iris_tsvd = tsvd.transform(iris_scaled) # 스케일링된 데이터를 기반으로 PCA 변환 수행 pca = PCA(n_components=2) pca.fit(iris_scaled) iris_pca = pca.transform(iris_scaled) # TruncatedSVD 변환 데이터를 왼쪽에, PCA변환 데이터를 오른쪽에 표현 fig, (ax1, ax2) = plt.subplots(figsize=(9,4), ncols=2) ax1.scatter(x=iris_tsvd[:,0], y= iris_tsvd[:,1], c= iris.target) ax2.scatter(x=iris_pca[:,0], y= iris_pca[:,1], c= iris.target) ax1.set_title(&#39;Truncated SVD Transformed&#39;) ax2.set_title(&#39;PCA Transformed&#39;) . Text(0.5, 1.0, &#39;PCA Transformed&#39;) . 두 개의 변환 행렬 값과 원본 속성별 컴포넌트 비율값을 실제로 서로 비교해 보면 거의 같음을 알 수 있다. | . print((iris_pca - iris_tsvd).mean()) print((pca.components_ - tsvd.components_).mean()) . 2.3278716917059703e-15 5.2909066017292616e-17 . 모두 0에 가까운 값이므로 2개의 변환이 서로 동일함을 알 수 있다. 즉 데이터 세트가 스케일링으로 데이터 중심이 동일해지면 사이킷런의 SVD와 PCA는 동일한 변환을 수행한다. 이는 PCA가 SVD 알고리즘으로 구현됐음을 의미한다. 하지만 PCA는 밀집 행렬에 대한 변환만 가능하며 SVD는 희소행렬에 대한 변환도 가능하다 | 희소 행렬 : 행렬의 값 대부분이 0인 행렬 | . NMF(Non-Negative Matrix Factorization) . Truncated SVD와 같이 낮은 랭크를 통한 행렬 근사 방식의 변형이다. NMF는 원본 행렬 내의 모든 원소 값이 모두 양수라는 게 보장되면 두 개의 기반 양수 행렬로 분해될 수 있는 기법을 지칭한다. 행렬 분해는 일반적으로 SVD롸 같은 행렬 분해 기법을 통칭하는 것이다. W행렬과 H행렬은 일반적으로 길고 가는 행렬인 W와 작고 넓은 행렬인 H로 분해된다. 이렇게 분해된 행렬은 잠재 요소를 특성으로 갖는다. 분해 행렬 W는 원본 행에 대해서 이 잠재 요소의 값이 얼마나 되는지에 대응하며 분해 행렬 H는 이 잠재 요소가 원본 열로 어떻게 구성됐는지를 나타내는 행렬이다. . from sklearn.decomposition import NMF from sklearn.datasets import load_iris import matplotlib.pyplot as plt %matplotlib inline iris = load_iris() iris_ftrs = iris.data nmf = NMF(n_components=2) nmf.fit(iris_ftrs) iris_nmf = nmf.transform(iris_ftrs) plt.scatter(x=iris_nmf[:,0], y= iris_nmf[:,1], c= iris.target) plt.xlabel(&#39;NMF Component 1&#39;) plt.ylabel(&#39;NMF Component 2&#39;) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages sklearn decomposition _nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(&#34;Maximum number of iterations %d reached. Increase it to&#34; . Text(0, 0.5, &#39;NMF Component 2&#39;) . Conclusion . 무엇보다도 차원 축소는 단순히 feature의 개수를 줄이는 개념보다는 이를 통해 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하는 데 큰 의미가 있다. 이 때문에 많은 차원을 가지는 이미지나 텍스트에서 PCA나 SVD 등의 차원 축소 알고리즘이 활발하게 사용된다. . | . &#44400;&#51665;&#54868; . K-평균 군집화(Clustering)에서 가장 일반적으로 사용되는 알고리즘이다. K-평균은 군집 중심점(centroid)이라는 특정한 임의의 지점을 선택해 해당 중심에 가장 가까운 포인트들을 선택하는 군집화 기번이다. . step - 2개의 군짐 중심점을 설정, 각 데이터는 가장 가까운 중심점에 소속, 중심점에 할당된 데이터들의 평균 중심으로 중심점 이동, 각 데이터는 이동된 중심점을 기분으로 가장 가까운 중심점에 소속, 다시 중심점에 할당딘 데이터들의 평균 중심으로 중심점 이동, 중심점을 이동하였지만 데이터들의 중심점 소속 변경이 없으면 군집화 완료 . 임의 위치에 군집 중심점을 가져다 놓으면 반복적인 이동 수행을 너무 많이 해서 수행 시간이 오래 걸리기 때문에 초기화 알고리즘으로 적합한 위치에 중심점을 가져다 놓지만, 여기서는 설명을 위해 임의 위치로 가정하겠다고 한 것 뿐 . K-평균의 장점 일반적인 군집화에서 가장 많이 활용되는 알고리즘이다. | 알고리즘이 쉽고 간결하다. | . | K-평균의 단점 거리 기반 알고리즘으로서 속성의 개수가 매우 많을 경우 군집화 정확도가 떨어진다. 이를 위해 PCA로 차원 감소를 적용해야 할 수도 있다. | 반복을 수행하는데, 횟수가 많을 경우 시간이 많이 소요 | 몇개의 군집을 선택해야 할지 가이드하기 어렵다. | . | . K-평균을 이용해 붓꽃 데이터 세트를 군집화해보자 | . from sklearn.datasets import load_iris from sklearn.cluster import KMeans import matplotlib.pyplot as plt import numpy as np import pandas as pd %matplotlib inline iris = load_iris() # 보다 편리한 데이터 Handling을 위해 DataFrame으로 변환 irisDF = pd.DataFrame(data=iris.data, columns=[&#39;sepal_length&#39;,&#39;sepal_width&#39;,&#39;petal_length&#39;,&#39;petal_width&#39;]) irisDF.head(3) . sepal_length sepal_width petal_length petal_width . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 붓꽃 데이터 세트를 3개 그룸으로 군집화해보자. | . kmeans = KMeans(n_clusters=3, init=&#39;k-means++&#39;, max_iter=300,random_state=0) kmeans.fit(irisDF) . KMeans(n_clusters=3, random_state=0) . print(kmeans.labels_) print(kmeans.predict(irisDF)) . [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0 0 2] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0 0 2] . irisDF[&#39;cluster&#39;]=kmeans.labels_ irisDF[&#39;target&#39;] = iris.target iris_result = irisDF.groupby([&#39;target&#39;,&#39;cluster&#39;])[&#39;sepal_length&#39;].count() print(iris_result) . target cluster 0 1 50 1 0 2 2 48 2 0 36 2 14 Name: sepal_length, dtype: int64 . 다른 건 양호하나 Target 2값 데이터는 0번 군집에 14개, 2번 군집에 36개로 분산돼 그루핑됐다. | 붓꽃 데이터 세트의 속성이 4개이므로 2차원 평면에 적합치 않아 PCA를 이용해 4개의 속성을 2개로 차원 축소한 뒤에 각 좌표로 개별 데이터를 표현하도록 해보자 | . from sklearn.decomposition import PCA pca = PCA(n_components=2) pca_transformed = pca.fit_transform(iris.data) irisDF[&#39;pca_x&#39;] = pca_transformed[:,0] irisDF[&#39;pca_y&#39;] = pca_transformed[:,1] irisDF.head(3) . sepal_length sepal_width petal_length petal_width cluster target pca_x pca_y . 0 5.1 | 3.5 | 1.4 | 0.2 | 1 | 0 | -2.684126 | 0.319397 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 1 | 0 | -2.714142 | -0.177001 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 1 | 0 | -2.888991 | -0.144949 | . marker0_ind = irisDF[irisDF[&#39;cluster&#39;]==0].index marker1_ind = irisDF[irisDF[&#39;cluster&#39;]==1].index marker2_ind = irisDF[irisDF[&#39;cluster&#39;]==2].index # cluster값 0, 1, 2에 해당하는 Index로 각 cluster 레벨의 pca_x, pca_y 값 추출. o, s, ^ 로 marker 표시 plt.scatter(x=irisDF.loc[marker0_ind,&#39;pca_x&#39;], y=irisDF.loc[marker0_ind,&#39;pca_y&#39;], marker=&#39;o&#39;) plt.scatter(x=irisDF.loc[marker1_ind,&#39;pca_x&#39;], y=irisDF.loc[marker1_ind,&#39;pca_y&#39;], marker=&#39;s&#39;) plt.scatter(x=irisDF.loc[marker2_ind,&#39;pca_x&#39;], y=irisDF.loc[marker2_ind,&#39;pca_y&#39;], marker=&#39;^&#39;) plt.xlabel(&#39;PCA 1&#39;) plt.ylabel(&#39;PCA 2&#39;) plt.title(&#39;3 Clusters Visualization by 2 PCA Components&#39;) plt.show() . Cluster0과 Cluster2는 상당 수준 분리돼 있지만 Cluster1만큼 명확하게 분리돼 있지 않음을 알 수 있다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/16/intro.html",
            "relUrl": "/2022/01/16/intro.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "2022/01/15/SAT",
            "content": "&#52264;&#50896; &#52629;&#49548; . 차원 축소 알고리즘 :PCA, LDA, SVD, NMF / 차원 축소는 매우 많은 feature로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것이다. 일반적으로 차원이 증가할수록 데이터 포인트 간의 거리가 기하급수적으로 멀어지게 되고 희소한 구조를 갖게 된다. 수백 개 이상의 feature로 구성된 데이터 세트의 경우 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도가 떨어진다. 또한 feature가 많을 경우 개별 feature간에 상관관계가 높을 가능성이 크다. 선형 회귀와 같은 선형 모델에서는 입력 변수 간의 상관관계가 높을 경우 이로 인한 다중 공선성 문제로 모델의 예측 성능이 저하된다. 이렇게 매우 많은 다차원이 feature를 차원 축소해 feature수를 죽이면 더 직관적으로 데이터를 해석할 수 있다. 또한 차원 축소를 할 경우 학습 데이터의 크기가 줄어들어서 학습에 필요한 처리 능력도 줄일 수 있다. . 일반적으로 차원 축소는 feature 선택과 feature 추출로 나눌 수 있다. feature selection : 특정 feature에 종속성이 강한 불필요한 feature는 아예 제거하고 데이터의 특징을 잘 나타내는 주요 feature만 선택. feature extraction : 기존 feature를 저차원의 중요 feature로 암축해서 추출하는 것. 따라서 기존 feature와는 완전히 상이한 값이 됨. 차원 축소를 통해 좀 더 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하는 게 중요 . 먼저 PCA(Principal Conponent Analysis) : 가장 대포ㅛ적인 차원 축소 기법이다. 여러 변수 간에 존재하는 상관관계를 이용해 이를 대표하는 주성분을 추출해 차원을 축소하는 기법 | 가장 높은 분산을 가지는 데이터의 축을 찾아 이 축으로 차원을 축소. 이것이 PCA의 주성분이 된다. 즉, 분산이 데이터의 특성을 가장 잘 나타내는 것으로 간주한다. | 제일 먼저 가장 큰 데이터 변동성(Variance)을 기반으로 첫 번째 벡터 축을 생성하고 두 번째 축은 이 벡터 축에 직각이 되는 벡터를 축으로 한다. 세 번째 축은 다시 두 번째 축과 직각이 되는 벡터를 설정하는 방식으로 축을 생성한다. 이렇게 생성된 벡터 축에 원본 데이터를 투영하면 벡터의 축의 개수만큼의 차원으로 원본 데이터가 차원 축소된다. | . PCA를 선형대수 관점에서 해석해놓은 380p 꼭 참고해보자 . 보통 PCA는 다음과 같은 스텝으로 수행된다. . 입력 데이터 세트의 공분산 행렬을 생성한다. | 공분산 행렬의 고유벡터와 고유값을 계산한다. | 고유값이 가장 큰 순으로 K개(PCA 변환 차수만큼)만큼 고유벡터를 추출한다. | 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환한다. | 붓꽃 데이터 세트는 총 네개의 속성으로 되어 있는데 이 4개의 속성을 2개의 PCA 차원으로 압축해 원래 데이터 세트와 압축된 데이터 세트가 어떻게 달라졌는지 확인해보자 | . from sklearn.datasets import load_iris import pandas as pd import matplotlib.pyplot as plt %matplotlib inline # 사이킷런 내장 데이터 셋 API 호출 iris = load_iris() # 넘파이 데이터 셋을 Pandas DataFrame으로 변환 columns = [&#39;sepal_length&#39;,&#39;sepal_width&#39;,&#39;petal_length&#39;,&#39;petal_width&#39;] irisDF = pd.DataFrame(iris.data , columns=columns) irisDF[&#39;target&#39;]=iris.target irisDF.head(3) . sepal_length sepal_width petal_length petal_width target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 각 품종에 따라 원본 붓꽃 데이터 세트가 어떻게 분포돼 있는지 2차원으로 시각화해보자 | 두 개의 속성인 sepal length와 sepal width를 두 축으로 해 품종 데이터 분포를 나타낸다. | . markers=[&#39;^&#39;, &#39;s&#39;, &#39;o&#39;] #setosa의 target 값은 0, versicolor는 1, virginica는 2. 각 target 별로 다른 shape으로 scatter plot for i, marker in enumerate(markers): x_axis_data = irisDF[irisDF[&#39;target&#39;]==i][&#39;sepal_length&#39;] y_axis_data = irisDF[irisDF[&#39;target&#39;]==i][&#39;sepal_width&#39;] plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i]) plt.legend() plt.xlabel(&#39;sepal length&#39;) plt.ylabel(&#39;sepal width&#39;) plt.show() . setosa 품종의 경우 sepal width가 3.0보다 크고, sepal length가 6.0 이하인 곳에 일정하게 분포돼 있다. 나머지 두 품종은 두 축만으로는 분류가 어려운 복잡한 조건임을 알 수 있다. | 이제 PCA로 4개의 속성을 2개로 압축한 뒤 앞의 예제와 비슷하게 2개의 PCA 속성으로 붓꽃 데이터의 품종 분포를 2차원으로 시각화해보자 | . PCA는 여러 속성의 값을 연산해야 하므로 속성의 스케일에 영향을 받는다. 따라서 여러 속성을 PCA로 압축하기 전에 각 속성값을 동일한 스케일로 변환하는 것이 필요 | . from sklearn.preprocessing import StandardScaler # 스케일링 적용 중 iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:, :-1]) . 이제 PCA 데이터로 변환해보자 | . iris_scaled.shape . (150, 4) . from sklearn.decomposition import PCA pca = PCA(n_components=2) #fit( )과 transform( ) 을 호출하여 PCA 변환 데이터 반환 pca.fit(iris_scaled) iris_pca = pca.transform(iris_scaled) . (150, 2) . from sklearn.decomposition import PCA pca = PCA(n_components=2) #fit( )과 transform( ) 을 호출하여 PCA 변환 데이터 반환 pca.fit(iris_scaled) iris_pca = pca.transform(iris_scaled) . print(iris_pca.shape) . (150, 2) . iris_pca는 넘파이 행렬임. DataFrame으로 변환한 뒤 데이터값을 확인해보자 | . pca_columns=[&#39;pca_component_1&#39;,&#39;pca_component_2&#39;] irisDF_pca = pd.DataFrame(iris_pca,columns=pca_columns) irisDF_pca[&#39;target&#39;]=iris.target irisDF_pca.head(3) . pca_component_1 pca_component_2 target . 0 -2.264703 | 0.480027 | 0 | . 1 -2.080961 | -0.674134 | 0 | . 2 -2.364229 | -0.341908 | 0 | . 이제 2개의 속성으로 PCA 변환된 데이터 세트를 2차원상에서 시각화해보자. | . markers=[&#39;^&#39;, &#39;s&#39;, &#39;o&#39;] #pca_component_1 을 x축, pc_component_2를 y축으로 scatter plot 수행. for i, marker in enumerate(markers): x_axis_data = irisDF_pca[irisDF_pca[&#39;target&#39;]==i][&#39;pca_component_1&#39;] y_axis_data = irisDF_pca[irisDF_pca[&#39;target&#39;]==i][&#39;pca_component_2&#39;] plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i]) plt.legend() plt.xlabel(&#39;pca_component_1&#39;) plt.ylabel(&#39;pca_component_2&#39;) plt.show() . setosa는 여전히 구분이 잘 되고 나머지 두 품종도 setosa만큼은 아니지만 그래도 전 보다는 비교적 잘 구분이 되고 있다. 이는 PCA의 첫 번째 새로운 축인 x축이 원본 데이터의 변동성을 잘 반영했기 때문이다. PCA Component별로 원본 데이터의 변동성을 얼마나 반영하고 있는지 확인해보자. PCA 변환을 수행한 PCA 객체의 explained_varianceratio 속성은 전체 변동성에서 개별 PCA 컴포넌트별로 차지하는 변동성 비율을 제공하고 있다. | . print(pca.explained_variance_ratio_) . [0.72962445 0.22850762] . PCA를 2개 요소로만 변환해도 두 변동성을 더한 값인 약 95%의 변동성을 설명할 수 있다는 것을 알았다. | PCA를 적용하기 전과 적용하고 난 후를 각각 분류를 실행한 뒤 결과를 비교해보자 | . from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np rcf = RandomForestClassifier(random_state=156) scores = cross_val_score(rcf, iris.data, iris.target,scoring=&#39;accuracy&#39;,cv=3) print(&#39;원본 데이터 교차 검증 개별 정확도: &#39;,scores) print(&#39;원본 데이터 평균 정확도 :&#39;,np.mean(scores)) . 원본 데이터 교차 검증 개별 정확도: [0.98 0.94 0.96] 원본 데이터 평균 정확도 : 0.96 . 이번에는 PCA 변환한 데이터 세트에 랜덤 포레스트를 적용해보자 | . pca_X = irisDF_pca[[&#39;pca_component_1&#39;, &#39;pca_component_2&#39;]] scores_pca = cross_val_score(rcf, pca_X, iris.target, scoring=&#39;accuracy&#39;, cv=3 ) print(scores_pca) print(np.mean(scores_pca)) . [0.88 0.88 0.88] 0.88 . 원본 데이터 세트 대비 예측 정확도는 PCA 변환 차원 개수에 따라 예측 성능이 떨어질 수밖에 없다. | 다음으로는 좀 더 많은 feature를 가진 데이터 세트를 적은 PCA 컴포넌트 기반으로 변환한 뒤 예측 영향도가 어떻게 되는지 변환된 PCA 데이터 세트에 기반해서 비교해보자 | 새로운 데이터 세트먼저 불러와 보자 | . import pandas as pd df = pd.read_excel(&#39;pca_credit_card.xls&#39;, sheet_name=&#39;Data&#39;, header=1).iloc[0:,1:] print(df.shape) df.head(3) . (30000, 24) . LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 20000 | 2 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | ... | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 120000 | 2 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | 0 | ... | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 90000 | 2 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | 0 | ... | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 3 rows × 24 columns . 맨 마지막 column이 target 값이며 원본 데이터 세트에 PAY_0 다음에 불규칙적으로 PAY_2 column이 있으므로 PAY_0을 PAY_1로 변경해주고 일부 긴 column명은 축약해주자 | . df.rename(columns={&#39;PAY_0&#39;:&#39;PAY_1&#39;,&#39;default payment next month&#39;:&#39;default&#39;}, inplace=True) y_target = df[&#39;default&#39;] # default 컬럼 Drop X_features = df.drop(&#39;default&#39;, axis=1) . 해당 데이터 세트는 23개의 속성 데이터 세트가 있으나 각 속성끼리상관도가 매우 높다. DataFrame의 corr()를 이용해 각 속성 간의 상관도를 구한 뒤 이를 시각화해보자 | . import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline corr = X_features.corr() plt.figure(figsize=(14,14)) sns.heatmap(corr, annot=True, fmt=&#39;.1g&#39;) . &lt;AxesSubplot:&gt; . 높은 상관도를 가진 속성들은 소수의 PCA만으로도 자연스럽게 이 속성들의 변동성을 수용할 수 있다. | 높은 상관도를 가진 BILL_AMT1 ~ BILL_AMT6까지 6개의 속성을 2개의 컴포넌트로 PCA 변환한 뒤 개별 컴포넌트의 변동성을 explained_variance_ratio_속성으로 알아보자 | . from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler #BILL_AMT1 ~ BILL_AMT6까지 6개의 속성명 생성 cols_bill = [&#39;BILL_AMT&#39;+str(i) for i in range(1, 7)] print(&#39;대상 속성명:&#39;, cols_bill) # 2개의 PCA 속성을 가진 PCA 객체 생성하고, explained_variance_ratio_ 계산을 위해 fit( ) 호출 scaler = StandardScaler() df_cols_scaled = scaler.fit_transform(X_features[cols_bill]) pca = PCA(n_components=2) pca.fit(df_cols_scaled) print(&#39;PCA Component별 변동성:&#39;, pca.explained_variance_ratio_) . 대상 속성명: [&#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;] PCA Component별 변동성: [0.90555253 0.0509867 ] . 단 2개의 PCA 컴포넌트만으로도 6개 속성의 변동성을 약 95% 이상 설명할 수 있으며 특히 첫 번째 PCA 축으로 90%의 변동성을 수용할 정도로 이 6개 속성의 상관도가 매우 높다. | 원본 데이터 세트와 6개의 컴포넌트로 PCA변환한 데이터 세트의 분류 예측 결과를 상호 비교해보자 | . import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score rcf = RandomForestClassifier(n_estimators=300, random_state=156) scores = cross_val_score(rcf, X_features, y_target, scoring=&#39;accuracy&#39;, cv=3 ) print(&#39;CV=3 인 경우의 개별 Fold세트별 정확도:&#39;,scores) print(&#39;평균 정확도:{0:.4f}&#39;.format(np.mean(scores))) . CV=3 인 경우의 개별 Fold세트별 정확도: [0.8083 0.8196 0.8232] 평균 정확도:0.8170 . from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler # 원본 데이터셋에 먼저 StandardScaler적용 scaler = StandardScaler() df_scaled = scaler.fit_transform(X_features) # 6개의 Component를 가진 PCA 변환을 수행하고 cross_val_score( )로 분류 예측 수행. pca = PCA(n_components=6) df_pca = pca.fit_transform(df_scaled) scores_pca = cross_val_score(rcf, df_pca, y_target, scoring=&#39;accuracy&#39;, cv=3) print(&#39;CV=3 인 경우의 PCA 변환된 개별 Fold세트별 정확도:&#39;,scores_pca) print(&#39;PCA 변환 데이터 셋 평균 정확도:{0:.4f}&#39;.format(np.mean(scores_pca))) . CV=3 인 경우의 PCA 변환된 개별 Fold세트별 정확도: [0.7911 0.7965 0.8026] PCA 변환 데이터 셋 평균 정확도:0.7967 . 예측 성능 차이가 1~2% 정도 차이난다. | 1~2% 성능 차이를 미비한 성능차이로 보긴 어렵지만 전체 속성의 1/4수준으로도 이정도 수치의 예측 성능을 유지할 수 있다는 것은 PCA의 뛰어난 압축 능력을 잘 보여주는 것이라고 생각해도 좋다 | PCA는 차원 축소를 통해 데이터를 쉽게 인지하는 데 활용할 수 있지만 이보다 더 활발하게 적용되는 영역은 컴퓨터 비전(Computer Vision)분야이다. 특히 얼굴 인식의 경우 Eigen-face라고 불리는 PCA 변환으로 원본 얼굴 이미지를 변환해 사용하는 경우가 많다. | . LDA :LinearDiscriminantAnalysis &#49440;&#54805; &#54032;&#48324; &#48516;&#49437;&#48277;. PCA&#50752; &#50976;&#49324;&#54616;&#44172; &#51077;&#47141; &#45936;&#51060;&#53552; &#49464;&#53944;&#47484; &#51200;&#52264;&#50896; &#44277;&#44036;&#50640; &#53804;&#50689;&#54644; &#52264;&#50896;&#51012; &#52629;&#49548;&#54616;&#45716; &#44592;&#48277;&#51060;&#51648;&#47564; &#51473;&#50836;&#54620; &#52264;&#51060;&#45716; LDA&#45716; &#51648;&#46020;&#54617;&#49845;&#51032; &#48516;&#47448;&#50640;&#49436; &#49324;&#50857;&#54616;&#44592; &#49789;&#46020;&#47197; &#44060;&#48324; &#53364;&#47000;&#49828;&#47484; &#48516;&#48324;&#54624; &#49688; &#51080;&#45716; &#44592;&#51456;&#51012; &#52572;&#45824;&#54620; &#50976;&#51648;&#54616;&#47732;&#49436; &#52264;&#50896;&#51012; &#52629;&#49548;&#54620;&#45796;. PCA&#45716; &#51077;&#47141; &#45936;&#51060;&#53552;&#51032; &#48320;&#46041;&#49457;&#51032; &#44032;&#51109; &#53360; &#52629;&#51012; &#52286;&#50520;&#51648;&#47564; LDA&#45716; &#51077;&#47141; &#45936;&#51060;&#53552;&#51032; &#44208;&#51221; &#44050; &#53364;&#47000;&#49828;&#47484; &#52572;&#45824;&#54620;&#51004;&#47196; &#48516;&#47532;&#54624; &#49688; &#51080;&#45716; &#52629;&#51012; &#52286;&#45716;&#45796;. LDA&#45716; &#53945;&#51221; &#44277;&#44036;&#49345;&#50640;&#49436; &#53364;&#47000;&#49828; &#48516;&#47532;&#47484; &#52572;&#45824;&#54868;&#54616;&#45716; &#52629;&#51012; &#52286;&#44592; &#50948;&#54644; &#53364;&#47000;&#49828; &#44036; &#48516;&#49328;&#44284; &#53364;&#47000;&#49828; &#45236;&#48512; &#48516;&#49328;&#51032; &#48708;&#50984;&#51012; &#52572;&#45824;&#54868;&#54616;&#45716; &#48169;&#49885;&#51004;&#47196; &#52264;&#50896;&#51012; &#52629;&#49548;&#54620;&#45796;. &#51593; &#53364;&#47000;&#49828; &#44036; &#48516;&#49328;&#51008; &#52572;&#45824;&#54620; &#53356;&#44172; &#44032;&#51256;&#44032;&#44256; &#53364;&#47000;&#49828; &#45236;&#48512;&#51032; &#48516;&#49328;&#51008; &#52572;&#45824;&#54620; &#51089;&#44172; &#44032;&#51256;&#44032;&#45716; &#48169;&#49885;&#51060;&#45796;. . 일반적으로 LDA를 구하는 절차는 PCA와 유사하나, 가장 큰 차이점은 공분산 행렬이 아니라 위에 설명한 클래스 간 분산과 클래스 내부 분산 행렬을 생성한 뒤 이 행렬에 기반해 고유벡터를 구하고 입력 데이터를 투영한다는 점이다. . 절차 | . 클래스 내부와 클래스 간 분산 행렬을 구한다. 이 두개의 행렬은 입력 데이터의 결정 값 클래스별로 개별 feature의 평균 벡터를 기반으로 구한다. | 클래스 내부 분산 행렬을 $S_W$, 클래스 간 분산 행렬을 $S_B$라고 하면 다음 식으로 두 행렬을 고유벡터로 분해할 수 있다. (394p 참고) | 고유값이 가장 큰 순으로 K개(LDA 변환 차수만큼) 추출한다. | 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환한다. | from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.preprocessing import StandardScaler from sklearn.datasets import load_iris iris = load_iris() iris_scaled = StandardScaler().fit_transform(iris.data) . 2개의 컴포넌트로 붓꽃 데이터를 LDA 변환하겠다. PCA와 다르게 LDA에서 한 가지 유의해야 할 점은 LDA는 실제로는 PCA와 다르게 비지도 학습이 아닌 지도학습이라는 것이다. 즉 클래스의 결정값이 변환 시에 필요하다. | . lda = LinearDiscriminantAnalysis(n_components=2) # fit()호출 시 target값 입력 lda.fit(iris_scaled, iris.target) iris_lda = lda.transform(iris_scaled) print(iris_lda.shape) . (150, 2) . import pandas as pd import matplotlib.pyplot as plt %matplotlib inline lda_columns=[&#39;lda_component_1&#39;,&#39;lda_component_2&#39;] irisDF_lda = pd.DataFrame(iris_lda,columns=lda_columns) irisDF_lda[&#39;target&#39;]=iris.target #setosa는 세모, versicolor는 네모, virginica는 동그라미로 표현 markers=[&#39;^&#39;, &#39;s&#39;, &#39;o&#39;] #setosa의 target 값은 0, versicolor는 1, virginica는 2. 각 target 별로 다른 shape으로 scatter plot for i, marker in enumerate(markers): x_axis_data = irisDF_lda[irisDF_lda[&#39;target&#39;]==i][&#39;lda_component_1&#39;] y_axis_data = irisDF_lda[irisDF_lda[&#39;target&#39;]==i][&#39;lda_component_2&#39;] plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i]) plt.legend(loc=&#39;upper right&#39;) plt.xlabel(&#39;lda_component_1&#39;) plt.ylabel(&#39;lda_component_2&#39;) plt.show() .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/15/intro.html",
            "relUrl": "/2022/01/15/intro.html",
            "date": " • Jan 15, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "2022/01/13/THU",
            "content": "&#52880;&#44544; &#51452;&#53469; &#44032;&#44201;:&#44256;&#44553; &#54924;&#44480; &#44592;&#48277; &#49457;&#45733; &#54217;&#44032;&#45716; RMSLE&#47484; &#44592;&#48152;&#51004;&#47196; &#54620;&#45796;. &#44032;&#44201;&#51060; &#48708;&#49916; &#51452;&#53469;&#51068;&#49688;&#47197; &#50696;&#52769; &#44208;&#44284; &#50724;&#47448;&#44032; &#51204;&#52404; &#50724;&#47448;&#50640; &#48120;&#52824;&#45716; &#48708;&#51473;&#51060; &#45458;&#51004;&#48064;&#47196; &#51060;&#44163;&#51012; &#49345;&#49604;&#54616;&#44592; &#50948;&#54644; &#50724;&#47448; &#44050;&#51012; &#47196;&#44536; &#48320;&#54872;&#54620; RMSLE&#47484; &#51060;&#50857;&#54620;&#45796;. . 데이터 사전 처리 | . import warnings warnings.filterwarnings(&#39;ignore&#39;) import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline house_df_org = pd.read_csv(&#39;house_price.csv&#39;) house_df = house_df_org.copy() house_df.head(3) . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 rows × 81 columns . Target 값은 맨 마지막 column인 SalePrice이다. | . print(&#39;데이터 세트의 Shape:&#39;, house_df.shape) print(&#39; n전체 feature 들의 type n&#39;,house_df.dtypes.value_counts()) isnull_series = house_df.isnull().sum() print(&#39; nNull 컬럼과 그 건수: n &#39;, isnull_series[isnull_series &gt; 0].sort_values(ascending=False)) . 데이터 세트의 Shape: (1460, 81) 전체 feature 들의 type object 43 int64 35 float64 3 dtype: int64 Null 컬럼과 그 건수: PoolQC 1453 MiscFeature 1406 Alley 1369 Fence 1179 FireplaceQu 690 LotFrontage 259 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 BsmtExposure 38 BsmtFinType2 38 BsmtFinType1 37 BsmtCond 37 BsmtQual 37 MasVnrArea 8 MasVnrType 8 Electrical 1 dtype: int64 . null값이 너무 많은 feature는 drop | 회귀 모델을 적용하기 전에 Target 값의 분포도가 정규 분포인지 확인해보자 | . plt.title(&#39;Original Sale Price Histogram&#39;) sns.distplot(house_df[&#39;SalePrice&#39;]) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Original Sale Price Histogram&#39;}, xlabel=&#39;SalePrice&#39;, ylabel=&#39;Density&#39;&gt; . 데이터 값의 분포가 중심에서 벗어나 왼쪽으로 치우친 형태로 정규 분포에서 벗어나 있음을 알 수 있다. | 정규 분포가 아닌 결괏값을 정규 분포 형태로 변환하기 위해 로그 변환을 적용하자. 로그로 변환된 값은 추루에 expm1()함수를 통해 환원하면 된다. | . plt.title(&#39;Log Transformed Sale Price Histogram&#39;) log_SalePrice = np.log1p(house_df[&#39;SalePrice&#39;]) sns.distplot(log_SalePrice) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Log Transformed Sale Price Histogram&#39;}, xlabel=&#39;SalePrice&#39;, ylabel=&#39;Density&#39;&gt; . 정규 분포 형태로 결괏값이 분포함을 확인했으니 Target값을 로그 변환한 뒤 DataFrame에 반영하자 | . original_SalePrice = house_df[&#39;SalePrice&#39;] house_df[&#39;SalePrice&#39;] = np.log1p(house_df[&#39;SalePrice&#39;]) # Null 이 너무 많은 컬럼들과 불필요한 컬럼 삭제 house_df.drop([&#39;Id&#39;,&#39;PoolQC&#39; , &#39;MiscFeature&#39;, &#39;Alley&#39;, &#39;Fence&#39;,&#39;FireplaceQu&#39;], axis=1 , inplace=True) # Drop 하지 않는 숫자형 Null컬럼들은 평균값으로 대체 house_df.fillna(house_df.mean(),inplace=True) # Null 값이 있는 피처명과 타입을 추출 null_column_count = house_df.isnull().sum()[house_df.isnull().sum() &gt; 0] print(&#39;## Null 피처의 Type : n&#39;, house_df.dtypes[null_column_count.index]) . ## Null 피처의 Type : MasVnrType object BsmtQual object BsmtCond object BsmtExposure object BsmtFinType1 object BsmtFinType2 object Electrical object GarageType object GarageFinish object GarageQual object GarageCond object dtype: object . house_df.mean() 함수는 숫자형 칼럼만 자동으로 mean처리 하기 때문에 애초에 문자형 column은 그대로 null이 유지되고 있다. | 문자형 feature는 모두 원-핫 인코딩으로 변환하자. | 판다스의 get_dummies()는 자동으로 문자열 feature를 원-핫 인코딩 변환하면서 Null 값은 None으로 대체해주기 때문에 별도의 Null값을 대체하는 로직이 필요없다. | 웟-핫 인코딩을 적용하면 당연히 column은 증가한다. | . print(&#39;get_dummies() 수행 전 데이터 Shape:&#39;, house_df.shape) house_df_ohe = pd.get_dummies(house_df) print(&#39;get_dummies() 수행 후 데이터 Shape:&#39;, house_df_ohe.shape) null_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() &gt; 0] print(&#39;## Null 피처의 Type : n&#39;, house_df_ohe.dtypes[null_column_count.index]) . get_dummies() 수행 전 데이터 Shape: (1460, 75) get_dummies() 수행 후 데이터 Shape: (1460, 271) ## Null 피처의 Type : Series([], dtype: object) . column이 늘어났고 이제 Null값을 가진 feature는 존재하지 않는다. 이 정도에서 데이터 세트의 기본적인 가공은 마치고 회귀 모델을 생성해 학습한 후 예측 결과를 평가해보자 | 앞에서 예측 평가는 RMSLE를 이용한다 했으나 이미 Target값인 SalePrice를 로그 변환했다. 예측값 역시 로스 변환된 SalePrice 값을 기반으로 예측하므로 원본 SalePrice 예측값의 로그 변환 값이다. 실제 값도 로스 변환됐고, 예측값도 이를 반영한 로그 변환 값이므로 예측 결과 오류에 RMSE만 적용하면 RMSLE가 자동으로 측정된다. | . 여러 모델의 로그 변환된 RMSE를 측정할 것이므로 이를 계산하는 함수를 먼저 생성하자. | . def get_rmse(model): pred = model.predict(X_test) mse = mean_squared_error(y_test , pred) rmse = np.sqrt(mse) print(&#39;{0} 로그 변환된 RMSE: {1}&#39;.format(model.__class__.__name__,np.round(rmse, 3))) return rmse def get_rmses(models): rmses = [ ] for model in models: rmse = get_rmse(model) rmses.append(rmse) return rmses . 첫번째 함수는 단일 모델의 RMSE 값을 두번째 함수는 첫번째 함수를 이용해 여러 모델의 RMSE 값을 반환한다. | 이제 선형 회귀 모델을 학습하고 예측, 평가해보자 | . from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error y_target = house_df_ohe[&#39;SalePrice&#39;] X_features = house_df_ohe.drop(&#39;SalePrice&#39;,axis=1, inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156) # LinearRegression, Ridge, Lasso 학습, 예측, 평가 lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge() ridge_reg.fit(X_train, y_train) lasso_reg = Lasso() lasso_reg.fit(X_train, y_train) models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) . LinearRegression 로그 변환된 RMSE: 0.132 Ridge 로그 변환된 RMSE: 0.128 Lasso 로그 변환된 RMSE: 0.176 . [0.131895765791545, 0.12750846334053068, 0.17628250556471392] . 라쏘 회귀의 경우 회귀 성능이 타 회귀 방식보다 많이 떨어지는 결과 나왔다 $ to$ 최적 하이퍼 파라미타 튜닝이 필요해 보임 $ to$ alpha 하이퍼 파라미터 최적화를 릿지와 라쏘 모델에 대해서 수행하자. | feature별 회귀 계수를 시각화해서 모델별로 어떠한 feature의 회귀계수로 구성되는지 확인해보자. | . def get_top_bottom_coef(model): # coef_ 속성을 기반으로 Series 객체를 생성. index는 컬럼명. coef = pd.Series(model.coef_, index=X_features.columns) # + 상위 10개 , - 하위 10개 coefficient 추출하여 반환. coef_high = coef.sort_values(ascending=False).head(10) coef_low = coef.sort_values(ascending=False).tail(10) return coef_high, coef_low . 해당 함수를 이용해 모델별 회귀 계수를 시각화하자. 시각화를 위한 함수로 visualize_coefficient(models)를 생성한다. 이 함수는 list 객체로 모델을 입력 받아 모델별로 회귀 계수 상하위 10개를 추출해 가로 막대 그래프 형태로 출력한다. | . def visualize_coefficient(models): # 3개 회귀 모델의 시각화를 위해 3개의 컬럼을 가지는 subplot 생성 fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=3) fig.tight_layout() # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 회귀 계수 시각화. for i_num, model in enumerate(models): # 상위 10개, 하위 10개 회귀 계수를 구하고, 이를 판다스 concat으로 결합. coef_high, coef_low = get_top_bottom_coef(model) coef_concat = pd.concat( [coef_high , coef_low] ) # 순차적으로 ax subplot에 barchar로 표현. 한 화면에 표현하기 위해 tick label 위치와 font 크기 조정. axs[i_num].set_title(model.__class__.__name__+&#39; Coeffiecents&#39;, size=25) axs[i_num].tick_params(axis=&quot;y&quot;,direction=&quot;in&quot;, pad=-120) for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()): label.set_fontsize(22) sns.barplot(x=coef_concat.values, y=coef_concat.index , ax=axs[i_num]) # 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 회귀 계수 시각화. models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) . 모델별 회귀 계수를 보면 OLS기반의 LinearRegression과 Ridge의 경우는 회귀 계수가 유사한 형태로 분포돼 있다. 하지만 Lasso는 전체적으로 회귀 계수 값이 매우 작고 YearBuilt feature가 유난히 크고 나머지 feature의 회귀계수는 작다. | Lasso $ to$ 학습 데이터의 데이터 분할에 문제일 수도 있기에 train_test_split()으로 분할하지 않고 전체 데이터 세트인 X_features와 y_target을 5개의 교차검증폴드세트로 분할해 평균RMSE를 측정해보자 | . from sklearn.model_selection import cross_val_score def get_avg_rmse_cv(models): for model in models: # 분할하지 않고 전체 데이터로 cross_val_score( ) 수행. 모델별 CV RMSE값과 평균 RMSE 출력 rmse_list = np.sqrt(-cross_val_score(model, X_features, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5)) rmse_avg = np.mean(rmse_list) print(&#39; n{0} CV RMSE 값 리스트: {1}&#39;.format( model.__class__.__name__, np.round(rmse_list, 3))) print(&#39;{0} CV 평균 RMSE 값: {1}&#39;.format( model.__class__.__name__, np.round(rmse_avg, 3))) # 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 CV RMSE값 출력 models = [lr_reg, ridge_reg, lasso_reg] get_avg_rmse_cv(models) . LinearRegression CV RMSE 값 리스트: [0.135 0.165 0.168 0.111 0.198] LinearRegression CV 평균 RMSE 값: 0.155 Ridge CV RMSE 값 리스트: [0.117 0.154 0.142 0.117 0.189] Ridge CV 평균 RMSE 값: 0.144 Lasso CV RMSE 값 리스트: [0.161 0.204 0.177 0.181 0.265] Lasso CV 평균 RMSE 값: 0.198 . 그래도 여전히 라쏘의 경우가 OLS 모델이나 릿지 모델보다 성능이 떨어진다. 릿지와 라쏘 모델에 대허서 alpha 하이퍼 파라미터 변화를 통해 최적값을 도출해보자 | 먼저 모델별로 최적화 하이퍼 파라미터 작업을 반복적으로 진행하기 위해 이를 위한 별도의 함수를 생성하자. | . from sklearn.model_selection import GridSearchCV def get_best_params(model, params): grid_model = GridSearchCV(model, param_grid=params, scoring=&#39;neg_mean_squared_error&#39;, cv=5) grid_model.fit(X_features, y_target) rmse = np.sqrt(-1* grid_model.best_score_) print(&#39;{0} 5 CV 시 최적 평균 RMSE 값: {1}, 최적 alpha:{2}&#39;.format(model.__class__.__name__, np.round(rmse, 4), grid_model.best_params_)) return grid_model.best_estimator_ ridge_params = { &#39;alpha&#39;:[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] } lasso_params = { &#39;alpha&#39;:[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] } best_rige = get_best_params(ridge_reg, ridge_params) best_lasso = get_best_params(lasso_reg, lasso_params) . Ridge 5 CV 시 최적 평균 RMSE 값: 0.1418, 최적 alpha:{&#39;alpha&#39;: 12} Lasso 5 CV 시 최적 평균 RMSE 값: 0.142, 최적 alpha:{&#39;alpha&#39;: 0.001} . 라쏘 모델의 경우 alpha 값 최적화 이후 예측 성능이 많이 좋아졌다. 선형 모델이 최적 alpha 값을 설정한 뒤 train_test_split()으로 분할된 학습 데이터와 테스트 데이터를 이용해 학습,예측,평가를 수행하고 모델별 회귀 계수를 시각화해보자 | . lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge(alpha=12) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 모든 모델의 RMSE 출력 models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) # 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) . LinearRegression 로그 변환된 RMSE: 0.132 Ridge 로그 변환된 RMSE: 0.124 Lasso 로그 변환된 RMSE: 0.12 . alpha 값 최적화 후 테스트 데이터 세트의 예측 성능이 더 좋아졌으며 모델별 회귀 계수도 많이 달라졌다. | 다만 라쏘의 경우 릿지에 비해 동일 feature라도 회귀 계수의 값이 상당히 작다. | 데이터 세트를 추가적으로 가공하여 모델 튜닝을 좀 더 진행해보자. | 첫 번째는 feature 데이터 세트의 데이터 분포도이고 두 번째는 이상치 데이터 처리이다. | . 앞 부분에서는 Target 데이터 세트의 데이터 분포도의 왜곡을 확인했다. feature 데이터 세트의 경우도 지나치게 왜곡된 feature가 존재할 경우 회귀 예측 성능을 저하시킬 수 있다. 모든 숫자형 feature의 데이터 분포도를 확인해 분포도가 어느 정도로 왜곡됐는지 알아보자 . 사이파이의 stats 모듈의 skew() 함수를 이용해 칼럼의 데이터 세트의 왜곡된 정도를 쉽게 추출할 수 있다. 일반적으로 skew() 함수의 반환 값이 1 이상인 경우를 왜곡 정도가 높다고 판단하지만 상황에 따라 편차는 있다. 여기서는 1 이상의 값을 반환하는 feature만 추출해 왜곡 정도를 완화하기 위해 로그 변환을 적용하겠다. *주의 : skew()를 적용하는 숫자형 feature에서 원-핫 인코딩된 카테고리 숫자형 feature는 제외해야 한다. 카테고리 feature는 코드성 feature이므로 인코딩시 당연히 왜곡될 가능성이 높다. 예를 들어 화장실 여부가 1과 0으로 표현됐는데 1로 1000건 0으로 10건일 때 이는 왜곡과 무관하므로. 따라서 skew() 함수를 적용하는 DataFrame은 원-핫 인코딩이 적용된 house_df_ohe이 아니라 그냥 house_df여야 한다. . from scipy.stats import skew # object가 아닌 숫자형 피쳐의 컬럼 index 객체 추출. features_index = house_df.dtypes[house_df.dtypes != &#39;object&#39;].index # house_df에 컬럼 index를 [ ]로 입력하면 해당하는 컬럼 데이터 셋 반환. apply lambda로 skew( )호출 skew_features = house_df[features_index].apply(lambda x : skew(x)) # skew 정도가 1 이상인 컬럼들만 추출. skew_features_top = skew_features[skew_features &gt; 1] print(skew_features_top.sort_values(ascending=False)) . MiscVal 24.451640 PoolArea 14.813135 LotArea 12.195142 3SsnPorch 10.293752 LowQualFinSF 9.002080 KitchenAbvGr 4.483784 BsmtFinSF2 4.250888 ScreenPorch 4.117977 BsmtHalfBath 4.099186 EnclosedPorch 3.086696 MasVnrArea 2.673661 LotFrontage 2.382499 OpenPorchSF 2.361912 BsmtFinSF1 1.683771 WoodDeckSF 1.539792 TotalBsmtSF 1.522688 MSSubClass 1.406210 1stFlrSF 1.375342 GrLivArea 1.365156 dtype: float64 . 이제 추출된 왜곡 정도가 높은 feature를 로그 변환하자 | . house_df[skew_features_top.index] = np.log1p(house_df[skew_features_top.index]) . 원-핫 인코딩 적용 안 된 걸로 수행했기 때문에 다시 원-핫 인코딩 수행해주자 | . house_df_ohe = pd.get_dummies(house_df) y_target = house_df_ohe[&#39;SalePrice&#39;] X_features = house_df_ohe.drop(&#39;SalePrice&#39;,axis=1, inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156) # 피처들을 로그 변환 후 다시 최적 하이퍼 파라미터와 RMSE 출력 ridge_params = { &#39;alpha&#39;:[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] } lasso_params = { &#39;alpha&#39;:[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] } best_ridge = get_best_params(ridge_reg, ridge_params) best_lasso = get_best_params(lasso_reg, lasso_params) . Ridge 5 CV 시 최적 평균 RMSE 값: 0.1275, 최적 alpha:{&#39;alpha&#39;: 10} Lasso 5 CV 시 최적 평균 RMSE 값: 0.1252, 최적 alpha:{&#39;alpha&#39;: 0.001} . 릿지 모델의 경우 최적 alpha 값이 12에서 10으로 변경됐고 두 모델 모두 feature의 로그 변환 이전과 비교해 RMSE값이 향상됐다. | 다시 위의 train_test_split으로 분할된 학습 데이터와 테스트 데이터를 이용해 모델의 학습/예측/평가 및 모델별 회귀 계수를 시각화해보자. | . lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge(alpha=10) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 모든 모델의 RMSE 출력 models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) # 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) . LinearRegression 로그 변환된 RMSE: 0.128 Ridge 로그 변환된 RMSE: 0.122 Lasso 로그 변환된 RMSE: 0.119 . 세 모델 모두 GrLivArea, 즉 주거 공간 크기가 회귀 계수가 가장 높은 feature가 됐다. | . 다음으로 분석할 요소는 이상치 데이터이다. 특히 회귀 계수가 높은 feature, 즉 예측에 많은 영향을 미치는 중요 feature의 이상치 데이터 처리가 중요하다. 먼저 GrLivArea feature의 데이터 분포도를 살펴보자 . plt.scatter(x = house_df_org[&#39;GrLivArea&#39;], y = house_df_org[&#39;SalePrice&#39;]) plt.ylabel(&#39;SalePrice&#39;, fontsize=15) plt.xlabel(&#39;GrLivArea&#39;, fontsize=15) plt.show() . 일반적으로 주거 공간이 큰 집일수록 가격이 비싸기 때문에 양의 상관도가 매우 높음을 직관적으로 알 수 있다. 하지만 위 그림에서 x축 5000쪽에 데이터 값 두개 너무 어긋나 있음 | 이상치로 간주하고 삭제하자 | . cond1 = house_df_ohe[&#39;GrLivArea&#39;] &gt; np.log1p(4000) cond2 = house_df_ohe[&#39;SalePrice&#39;] &lt; np.log1p(500000) outlier_index = house_df_ohe[cond1 &amp; cond2].index print(&#39;아웃라이어 레코드 index :&#39;, outlier_index.values) print(&#39;아웃라이어 삭제 전 house_df_ohe shape:&#39;, house_df_ohe.shape) # DataFrame의 index를 이용하여 아웃라이어 레코드 삭제. house_df_ohe.drop(outlier_index, axis=0, inplace=True) print(&#39;아웃라이어 삭제 후 house_df_ohe shape:&#39;, house_df_ohe.shape) . 아웃라이어 레코드 index : [ 523 1298] 아웃라이어 삭제 전 house_df_ohe shape: (1460, 271) 아웃라이어 삭제 후 house_df_ohe shape: (1458, 271) . 업데이트된 house_df_ohe를 기반으로 feature데이터 세트와 target데이터 세트를 다시 생성하고 해당함수를 이용해 릿지와 라쏘 모델의 최적화를 수행하고 결과를 출력해보자 | . y_target = house_df_ohe[&#39;SalePrice&#39;] X_features = house_df_ohe.drop(&#39;SalePrice&#39;,axis=1, inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156) ridge_params = { &#39;alpha&#39;:[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] } lasso_params = { &#39;alpha&#39;:[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] } best_ridge = get_best_params(ridge_reg, ridge_params) best_lasso = get_best_params(lasso_reg, lasso_params) . Ridge 5 CV 시 최적 평균 RMSE 값: 0.1125, 최적 alpha:{&#39;alpha&#39;: 8} Lasso 5 CV 시 최적 평균 RMSE 값: 0.1122, 최적 alpha:{&#39;alpha&#39;: 0.001} . 단 두개의 이상치 데이터만 제거했는데 예측 수치가 매우 크게 향상 됐음을 알 수 있다. 웬만큼의 하이퍼 파라미터 튜닝을 해도 이 정도의 수치 개선은 어렵다. GrLivArea 속성이 회귀 모델에서 차지하는 영향도가 크기에 이 이상치를 개선하는 것이 성능 개선에 큰 의미를 가졌다. | 회귀에 중요한 영향을 미치는 feature를 위주로 이상치 데이터를 찾으려는 노력은 중요하다. | . lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge(alpha=8) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 모든 모델의 RMSE 출력 models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) # 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) . LinearRegression 로그 변환된 RMSE: 0.129 Ridge 로그 변환된 RMSE: 0.103 Lasso 로그 변환된 RMSE: 0.1 . 이번에는 회귀 트리를 이용해 회귀 모델을 만들어보자 . XGBoost | . from xgboost import XGBRegressor xgb_params = {&#39;n_estimators&#39;:[1000]} xgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05, colsample_bytree=0.5, subsample=0.8) best_xgb = get_best_params(xgb_reg, xgb_params) . XGBRegressor 5 CV 시 최적 평균 RMSE 값: 0.1178, 최적 alpha:{&#39;n_estimators&#39;: 1000} . LightGBM | . from lightgbm import LGBMRegressor lgbm_params = {&#39;n_estimators&#39;:[1000]} lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1) best_lgbm = get_best_params(lgbm_reg, lgbm_params) . LGBMRegressor 5 CV 시 최적 평균 RMSE 값: 0.1163, 최적 alpha:{&#39;n_estimators&#39;: 1000} . feature중요도 시각화 | . def get_top_features(model): ftr_importances_values = model.feature_importances_ ftr_importances = pd.Series(ftr_importances_values, index=X_features.columns ) ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] return ftr_top20 def visualize_ftr_importances(models): # 2개 회귀 모델의 시각화를 위해 2개의 컬럼을 가지는 subplot 생성 fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=2) fig.tight_layout() # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 피처 중요도 시각화. for i_num, model in enumerate(models): # 중요도 상위 20개의 피처명과 그때의 중요도값 추출 ftr_top20 = get_top_features(model) axs[i_num].set_title(model.__class__.__name__+&#39; Feature Importances&#39;, size=25) #font 크기 조정. for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()): label.set_fontsize(22) sns.barplot(x=ftr_top20.values, y=ftr_top20.index , ax=axs[i_num]) # 앞 예제에서 get_best_params( )가 반환한 GridSearchCV로 최적화된 모델의 피처 중요도 시각화 models = [best_xgb, best_lgbm] visualize_ftr_importances(models) . 회귀 모델의 예측 결과 혼합을 통한 최종 예측 . 이번에는 개별 회귀 모델의 예측 결괏값을 혼합해 이를 기반으로 최종 회귀 값을 예측하겠다. 가령 A모델과 B모델, 두 모델의 예측값이 있다면 각각 예측값의 40%,60%를 더해서 최종 회귀 값으로 예측하는 것이다. 기준은 딱히 없으나 두 개 중 성능이 조금 좋은 쪽에 가중치를 약간 더 뒀다. A회귀 모델의 예측값이 [100,80,60]이고 B회귀 모델의 예측값이 [150,80,50]이라면 각각의 값에 0.4,0.6을 곱하는 것이다. 최종 혼합 모델, 개별 모델의 RMSE값을 출력하는 함수를 생성하고 각 모델의 예측값을 계산한 뒤 개별 모델과 최종 혼합 모델의 RMSE를 구한다. . def get_rmse_pred(preds): for key in preds.keys(): pred_value = preds[key] mse = mean_squared_error(y_test , pred_value) rmse = np.sqrt(mse) print(&#39;{0} 모델의 RMSE: {1}&#39;.format(key, rmse)) # 개별 모델의 학습 ridge_reg = Ridge(alpha=8) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 개별 모델 예측 ridge_pred = ridge_reg.predict(X_test) lasso_pred = lasso_reg.predict(X_test) # 개별 모델 예측값 혼합으로 최종 예측값 도출 pred = 0.4 * ridge_pred + 0.6 * lasso_pred preds = {&#39;최종 혼합&#39;: pred, &#39;Ridge&#39;: ridge_pred, &#39;Lasso&#39;: lasso_pred} #최종 혼합 모델, 개별모델의 RMSE 값 출력 get_rmse_pred(preds) . 최종 혼합 모델의 RMSE: 0.10007930884470487 Ridge 모델의 RMSE: 0.1034517754660322 Lasso 모델의 RMSE: 0.10024170460890011 . 이번에는 XGBoost와 LightGBM을 혼합해 결과를 살펴보자 | . xgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05, colsample_bytree=0.5, subsample=0.8) lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1) xgb_reg.fit(X_train, y_train) lgbm_reg.fit(X_train, y_train) xgb_pred = xgb_reg.predict(X_test) lgbm_pred = lgbm_reg.predict(X_test) pred = 0.5 * xgb_pred + 0.5 * lgbm_pred preds = {&#39;최종 혼합&#39;: pred, &#39;XGBM&#39;: xgb_pred, &#39;LGBM&#39;: lgbm_pred} get_rmse_pred(preds) . 최종 혼합 모델의 RMSE: 0.10170077353447762 XGBM 모델의 RMSE: 0.10738295638346222 LGBM 모델의 RMSE: 0.10382510019327311 . 개별 모델보다 향상 됐음을 알 수 있다. | . 스태킹 앙상블 모델을 통한 회귀 예측 . 핵심은 여러 개별 모델의 예측 데이터를 각각 스태킹 형태로 결합해 최종 메타 모델의 학습용 feature 데이터 세트와 테스트용 feature 데이터 세트를 만드는 것이다. . from sklearn.model_selection import KFold from sklearn.metrics import mean_absolute_error # 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ): # 지정된 n_folds값으로 KFold 생성. kf = KFold(n_splits=n_folds, shuffle=False, random_state=0) #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 train_fold_pred = np.zeros((X_train_n.shape[0] ,1 )) test_pred = np.zeros((X_test_n.shape[0],n_folds)) print(model.__class__.__name__ , &#39; model 시작 &#39;) for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)): #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 print(&#39; t 폴드 세트: &#39;,folder_counter,&#39; 시작 &#39;) X_tr = X_train_n[train_index] y_tr = y_train_n[train_index] X_te = X_train_n[valid_index] #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행. model.fit(X_tr , y_tr) #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장. train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. test_pred[:, folder_counter] = model.predict(X_test_n) # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1) #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터 return train_fold_pred , test_pred_mean . X_train_n = X_train.values X_test_n = X_test.values y_train_n = y_train.values # 각 개별 기반(Base)모델이 생성한 학습용/테스트용 데이터 반환. ridge_train, ridge_test = get_stacking_base_datasets(ridge_reg, X_train_n, y_train_n, X_test_n, 5) lasso_train, lasso_test = get_stacking_base_datasets(lasso_reg, X_train_n, y_train_n, X_test_n, 5) xgb_train, xgb_test = get_stacking_base_datasets(xgb_reg, X_train_n, y_train_n, X_test_n, 5) lgbm_train, lgbm_test = get_stacking_base_datasets(lgbm_reg, X_train_n, y_train_n, X_test_n, 5) . Ridge model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 Lasso model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 XGBRegressor model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 LGBMRegressor model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 . Stack_final_X_train = np.concatenate((ridge_train, lasso_train, xgb_train, lgbm_train), axis=1) Stack_final_X_test = np.concatenate((ridge_test, lasso_test, xgb_test, lgbm_test), axis=1) # 최종 메타 모델은 라쏘 모델을 적용. meta_model_lasso = Lasso(alpha=0.0005) #기반 모델의 예측값을 기반으로 새롭게 만들어진 학습 및 테스트용 데이터로 예측하고 RMSE 측정. meta_model_lasso.fit(Stack_final_X_train, y_train) final = meta_model_lasso.predict(Stack_final_X_test) mse = mean_squared_error(y_test , final) rmse = np.sqrt(mse) print(&#39;스태킹 회귀 모델의 최종 RMSE 값은:&#39;, rmse) . 스태킹 회귀 모델의 최종 RMSE 값은: 0.09799152965189672 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/13/intro.html",
            "relUrl": "/2022/01/13/intro.html",
            "date": " • Jan 13, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "2022/01/12/WED",
            "content": "&#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . 로지스틱 회귀는 선형 회귀 방식을 분류에 적용한 알고리즘이다. 회귀가 선형인지 비선형인가는 독립 변수가 아닌 가중치 변수가 선형인지 아닌지를 따른다. 로지스틱 회귀가 선형 회귀와 다른 점은 학습을 통해 선형 함수의 회귀 최적선을 찾는 것이 아니라 시그모이드(sigmoid) 함수 최적선을 찾고 이 시그모이드 함수의 반환 값을 확률로 간주해 확률에 따라 분류를 결정한다는 것이다. 많은 자연, 사회 현상에서 특정 변수의 확률 값은 선형이 아니라 시그모이드 함수와 같이 S자 커브 형태를 가진다. 시그모이드 함수 :$y= frac{1}{1+e^(-x)}$ 시그모이드 함수는 $x$값이 아무리 음,양으로 커지거나 작아져도 $y$값은 항상 0과 1사이 값을 반환한다. $x$ 값이 커지면 1에 근사하며 $x$ 값이 작아지면 0에 근사한다. 지금까지는 부동산 가격과 같은 연속형 값을 구하는 데 회귀를 사용했다. 이번에는 약간 다르게 가령 종양의 크기에 따라 악성 종양인지 그렇지 않은지 1,0을 이용해 예측해보자. 로지스틱 회귀는 선형 회귀 방식을 기반으로 하되 시그모이드 함수를 이용해 분류를 수행하는 회귀이다. . x=np.arange(-6,6,0.1) y=1/(1+np.exp(-x)) import matplotlib.pyplot as plt plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x1ca3205da30&gt;] . 이제 위스콘신 유방암 데이터 세트를 이용해 로지스틱 회귀로 암 여부를 판단해 보자 | . import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression cancer = load_breast_cancer() . 선형 회귀 계열의 로지스틱 회귀는 데이터의 정규 분포도에 따라 예측 성능 영향을 받을 수 있으므로 데이터에 먼저 정규 분포 형태의 표준 스케일링을 적용하자. | . from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split # StandardScaler( )로 평균이 0, 분산 1로 데이터 분포도 변환 scaler = StandardScaler() data_scaled = scaler.fit_transform(cancer.data) X_train , X_test, y_train , y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0) . from sklearn.metrics import accuracy_score, roc_auc_score # 로지스틱 회귀를 이용하여 학습 및 예측 수행. lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) lr_preds = lr_clf.predict(X_test) # accuracy와 roc_auc 측정 print(&#39;accuracy: {:0.3f}&#39;.format(accuracy_score(y_test, lr_preds))) print(&#39;roc_auc: {:0.3f}&#39;.format(roc_auc_score(y_test , lr_preds))) . accuracy: 0.977 roc_auc: 0.972 . 사이킷런 LogisticRegression 클래스의 주요 하이퍼 파라미터로 penalty와 C가 있다. penalty는 규제의 유형을 설정하며 default는 l2이다. C는 규제 강도를 조절하는 alpha값의 역수이다. 즉 C 값이 작을 수록 규제 강도가 크다. | GridSearchCV를 이용해 위스콘신 데이터 세트에서 이 하이퍼 파라미터를 최적화 해보자 | . import warnings warnings.filterwarnings(&#39;ignore&#39;) from sklearn.model_selection import GridSearchCV params={&#39;penalty&#39;:[&#39;l2&#39;, &#39;l1&#39;], &#39;C&#39;:[0.01, 0.1, 1, 1, 5, 10]} grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring=&#39;accuracy&#39;, cv=3 ) grid_clf.fit(data_scaled, cancer.target) print(&#39;최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}&#39;.format(grid_clf.best_params_, grid_clf.best_score_)) . 최적 하이퍼 파라미터:{&#39;C&#39;: 1, &#39;penalty&#39;: &#39;l2&#39;}, 최적 평균 정확도:0.975 . 로지스틱 회귀는 가볍고 빠르지만 이진 분류 예측 성능도 뛰어나다. 이 때문에 로지스틱 회귀를 이진 분류의 기본 모델로 사용하는 경우가 많다. 또한 로지스틱 회귀는 희소한 데이터 세트 분류에도 뛰어난 성능을 보여서 텍스트 분류에서도 자주 사용된다. | . . 지금까지 선형 회귀에 대해 알아봤다. 선형 회귀는 회귀 계수의 관계를 모두 선형으로 가정하는 방식이다. 일반적으로 선형 회귀는 회귀 계수를 선형으로 결합하는 회귀 함수를 구해 여기에 독립 변수를 입력해 결괏값을 예측하는 것이다. 비선형 회귀 역시 비선형 회귀 함수를 통해 결괏값을 예측한다. 다만 비선형 회귀는 회귀 계수의 결합이 비선형일 뿐이다. 재차 언급하지만 머신러닝 기반의 회귀는 회귀계수를 기반으로 하는 최적 회귀 함수를 도출하는 것이 주요 목표이다. 이번엔 회귀 함수를 기반으로 하지 않고 결정 트리와 같이 트리를 기반으로 하는 회귀방식으로 소개하겠다. | . &#54924;&#44480; &#53944;&#47532; . 즉 회귀를 위한 트리를 생성하고 이를 기반으로 회귀 예측을 하는 것이다. 분류 트리와 크게 다르지 않다. 다만 리프 노드에서 예측 결정 값을 만드는 과정에 차이가 있는데 분류 트리가 특정 클래스 레이블을 결정하는 것과 달리 회귀 트리는 리프 노드에 속한 데이터 값을 평균값을 구해 회귀 예측값을 계산한다. 리프 노드 생성 기준에 부합하는 트리 분할이 완료됐다면 리프 노드에 소속된 데이터 값의 평균값을 구해서 최종적으로 리프 노드에 결정 값으로 할당한다. . 사이킷런의 랜덤 포레스트 회귀 트리인 RandomForestRegressor를 이용해 앞의 선형 회귀에서 다룬 보스턴 주택 가격 예측을 수행해보자. | . from sklearn.datasets import load_boston from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor import pandas as pd import numpy as np # 보스턴 데이터 세트 로드 boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target y_target = bostonDF[&#39;PRICE&#39;] X_data = bostonDF.drop([&#39;PRICE&#39;], axis=1,inplace=False) rf = RandomForestRegressor(random_state=0, n_estimators=1000) neg_mse_scores = cross_val_score(rf, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39; 5 교차 검증의 개별 Negative MSE scores: &#39;, np.round(neg_mse_scores, 2)) print(&#39; 5 교차 검증의 개별 RMSE scores : &#39;, np.round(rmse_scores, 2)) print(&#39; 5 교차 검증의 평균 RMSE : {0:.3f} &#39;.format(avg_rmse)) . 5 교차 검증의 개별 Negative MSE scores: [ -7.88 -13.14 -20.57 -46.23 -18.88] 5 교차 검증의 개별 RMSE scores : [2.81 3.63 4.54 6.8 4.34] 5 교차 검증의 평균 RMSE : 4.423 . 이번에는 랜덤 포레스트뿐만 아니라 결정 트리, GBM, XGBoost, LightGBM의 Regressor를 모두 이용해 예측을 수행해보자. | . 아래 함수는 입력 모델과 데이터 세트를 입력 받아 교차 검증으로 평균 RMSE를 계산해주는 함수이다. | . def get_model_cv_prediction(model, X_data, y_target): neg_mse_scores = cross_val_score(model, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;##### &#39;,model.__class__.__name__ , &#39; #####&#39;) print(&#39; 5 교차 검증의 평균 RMSE : {0:.3f} &#39;.format(avg_rmse)) . 이제 다양한 유형의 회귀 트리를 생성하고 이를 이용해 가격을 예측해보자 | . from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4) rf_reg = RandomForestRegressor(random_state=0, n_estimators=1000) gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000) xgb_reg = XGBRegressor(n_estimators=1000) lgb_reg = LGBMRegressor(n_estimators=1000) # 트리 기반의 회귀 모델을 반복하면서 평가 수행 models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg] for model in models: get_model_cv_prediction(model, X_data, y_target) . ##### DecisionTreeRegressor ##### 5 교차 검증의 평균 RMSE : 5.978 ##### RandomForestRegressor ##### 5 교차 검증의 평균 RMSE : 4.423 ##### GradientBoostingRegressor ##### 5 교차 검증의 평균 RMSE : 4.269 ##### XGBRegressor ##### 5 교차 검증의 평균 RMSE : 4.251 ##### LGBMRegressor ##### 5 교차 검증의 평균 RMSE : 4.646 . 회귀 트리 Regressor 클래스는 선형 회귀와 다른 처리 방식이므로 회귀 계수를 제공하는 coef_ 속성이 없다. 대신 feature별 중요도를 알아보자 | . import seaborn as sns %matplotlib inline rf_reg = RandomForestRegressor(n_estimators=1000) # 앞 예제에서 만들어진 X_data, y_target 데이터 셋을 적용하여 학습합니다. rf_reg.fit(X_data, y_target) feature_series = pd.Series(data=rf_reg.feature_importances_, index=X_data.columns ) feature_series = feature_series.sort_values(ascending=False) sns.barplot(x= feature_series, y=feature_series.index) . &lt;AxesSubplot:&gt; . 이번에는 회귀 트리 Regressor가 어떻게 예측값을 판단하는 선형 회귀와 비교해 시각화해보자. 결정 트리의 하이퍼 파라미터인 max_depth의 크기를 변화시키면서 어떻게 회귀 트리 예측선이 변화하는지 살펴보자. 회귀 예측선을 쉽게 표현하귀 위해서 단 1개의 변수만 추출하자. 일단 이 데이터 세트를 시각화 해보자 | . import matplotlib.pyplot as plt %matplotlib inline bostonDF_sample = bostonDF[[&#39;RM&#39;,&#39;PRICE&#39;]] bostonDF_sample = bostonDF_sample.sample(n=100,random_state=0) print(bostonDF_sample.shape) plt.figure() plt.scatter(bostonDF_sample.RM , bostonDF_sample.PRICE,c=&quot;darkorange&quot;) . (100, 2) . &lt;matplotlib.collections.PathCollection at 0x1ca1cd27880&gt; . 다음으로 보스턴 데이터 세트에 대해 LinearRegression과 DecisionTreeRegressor를 max_depth를 각각 2,7로 학습해보자. 이렇게 학습된 Regressor에 RM feature값을 4.5~8.5까지의 100개의 테스트 데이터 세트로 제공했을 때 예측값을 구하자. | . import numpy as np from sklearn.linear_model import LinearRegression # 선형 회귀와 결정 트리 기반의 Regressor 생성. DecisionTreeRegressor의 max_depth는 각각 2, 7 lr_reg = LinearRegression() rf_reg2 = DecisionTreeRegressor(max_depth=2) rf_reg7 = DecisionTreeRegressor(max_depth=7) # 실제 예측을 적용할 테스트용 데이터 셋을 4.5 ~ 8.5 까지 100개 데이터 셋 생성. X_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1) # 보스턴 주택가격 데이터에서 시각화를 위해 피처는 RM만, 그리고 결정 데이터인 PRICE 추출 X_feature = bostonDF_sample[&#39;RM&#39;].values.reshape(-1,1) y_target = bostonDF_sample[&#39;PRICE&#39;].values.reshape(-1,1) # 학습과 예측 수행. lr_reg.fit(X_feature, y_target) rf_reg2.fit(X_feature, y_target) rf_reg7.fit(X_feature, y_target) pred_lr = lr_reg.predict(X_test) pred_rf2 = rf_reg2.predict(X_test) pred_rf7 = rf_reg7.predict(X_test) . 학습된 Regressor에서 예측한 Price 회귀선을 그려보자 | . fig , (ax1, ax2, ax3) = plt.subplots(figsize=(14,4), ncols=3) # X축값을 4.5 ~ 8.5로 변환하며 입력했을 때, 선형 회귀와 결정 트리 회귀 예측 선 시각화 # 선형 회귀로 학습된 모델 회귀 예측선 ax1.set_title(&#39;Linear Regression&#39;) ax1.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=&quot;darkorange&quot;) ax1.plot(X_test, pred_lr,label=&quot;linear&quot;, linewidth=2 ) # DecisionTreeRegressor의 max_depth를 2로 했을 때 회귀 예측선 ax2.set_title(&#39;Decision Tree Regression: n max_depth=2&#39;) ax2.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=&quot;darkorange&quot;) ax2.plot(X_test, pred_rf2, label=&quot;max_depth:3&quot;, linewidth=2 ) # DecisionTreeRegressor의 max_depth를 7로 했을 때 회귀 예측선 ax3.set_title(&#39;Decision Tree Regression: n max_depth=7&#39;) ax3.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=&quot;darkorange&quot;) ax3.plot(X_test, pred_rf7, label=&quot;max_depth:7&quot;, linewidth=2) . [&lt;matplotlib.lines.Line2D at 0x1ca1d4d4280&gt;] . 선형 회귀는 직선으로 예측 회귀선을 표현하는 데 반해, 회귀 트리의 경우 분할되는 데이터 지점에 따라 브랜치를 만들면서 계단 형태로 회귀선을 만든다. max_depth가 7인 경우에는 학습 데이터 세트의 이상치 데이터도 학습하면서 복잡한 계단 형태의 회귀선을 만들어 과적합이 되기 쉬운 모델이 되었음을 알 수 있다. | . &#51088;&#51204;&#44144; &#45824;&#50668; &#49688;&#50836; &#50696;&#52769; . 데이터 클렌징 및 가공, 데이터 세트를 이용해 모델을 학습한 후 대여 횟수를 예측해보자 | . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&quot;ignore&quot;, category=RuntimeWarning) bike_df = pd.read_csv(&#39;./bike_train.csv&#39;) print(bike_df.shape) bike_df.head(3) . (10886, 12) . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | . bike_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB . null 데이터는 없으며, datetime만 type이 object이다. 일단 이 column에 대해 알아보자 | . bike_df[[&#39;datetime&#39;]] . datetime . 0 2011-01-01 00:00:00 | . 1 2011-01-01 01:00:00 | . 2 2011-01-01 02:00:00 | . 3 2011-01-01 03:00:00 | . 4 2011-01-01 04:00:00 | . ... ... | . 10881 2012-12-19 19:00:00 | . 10882 2012-12-19 20:00:00 | . 10883 2012-12-19 21:00:00 | . 10884 2012-12-19 22:00:00 | . 10885 2012-12-19 23:00:00 | . 10886 rows × 1 columns . 이렇게 년,월,일,시간 형식으로 돼 있으므로 이에 대한 적절한 가공이 필요하다. 이를 각각 분리하자. 판다스에서는 datetime과 같은 형태의 문자열을 년,월,일,시간,분,초로 편리하게 변환하려면 먼저 문자열을 datetime으로 변경해야한다. (우연히 판다스의 datetime 타입과 예제 데이터 세트의 datetime column명이 동일한 것이지 둘을 혼동해선 안 된다) | . bike_df[&#39;datetime&#39;] = bike_df.datetime.apply(pd.to_datetime) bike_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null datetime64[ns] 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: datetime64[ns](1), float64(3), int64(8) memory usage: 1020.7 KB . datetime column의 type이 datetime64로 적절히 변경된 것을 알 수 있다. | 간단히 살펴보자 | . bike_df[[&#39;datetime&#39;]].head(3) . datetime . 0 2011-01-01 00:00:00 | . 1 2011-01-01 01:00:00 | . 2 2011-01-01 02:00:00 | . 겉으로 보기엔 바뀐 게 없어보이지만 아마 type은 잘 바뀌었을 것이다. | . bike_df[&#39;year&#39;] = bike_df.datetime.apply(lambda x : x.year) bike_df[&#39;month&#39;] = bike_df.datetime.apply(lambda x : x.month) bike_df[&#39;day&#39;] = bike_df.datetime.apply(lambda x : x.day) bike_df[&#39;hour&#39;] = bike_df.datetime.apply(lambda x: x.hour) print(bike_df.info()) bike_df.head(3) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null datetime64[ns] 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 12 year 10886 non-null int64 13 month 10886 non-null int64 14 day 10886 non-null int64 15 hour 10886 non-null int64 dtypes: datetime64[ns](1), float64(3), int64(12) memory usage: 1.3 MB None . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count year month day hour . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | 2011 | 1 | 1 | 0 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | 2011 | 1 | 1 | 1 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | 2011 | 1 | 1 | 2 | . drop_columns = [&#39;datetime&#39;,&#39;casual&#39;,&#39;registered&#39;] bike_df.drop(drop_columns, axis=1,inplace=True) . 기존의 datetime column은 삭제했고, casual,resistered의 합이 count이므로 불필요하기에 같이 삭제하겠음. 오히려 상관도가 높아 예측을 저해할 우려가 있으므로 삭제함. | 다양한 회귀 모델을 데이터 세트에 적용해 예측 성능을 측정해보자 | 사이킷런에서는 RMSLE를 제공하지 않기 때문에 이를 수행하는 함수를 직접 만들어보자 | . from sklearn.metrics import mean_squared_error, mean_absolute_error # log 값 변환 시 언더플로우 영향으로 log() 가 아닌 log1p() 를 이용하여 RMSLE 계산 def rmsle(y, pred): log_y = np.log1p(y) log_pred = np.log1p(pred) squared_error = (log_y - log_pred) ** 2 rmsle = np.sqrt(np.mean(squared_error)) return rmsle # 사이킷런의 mean_square_error() 를 이용하여 RMSE 계산 def rmse(y,pred): return np.sqrt(mean_squared_error(y,pred)) # 책에서는 mean_absolute_error()를 MSE로 잘못 기재함. # MAE, RMSE, RMSLE 를 모두 계산 def evaluate_regr(y,pred): rmsle_val = rmsle(y,pred) rmse_val = rmse(y,pred) # MAE 는 scikit learn의 mean_absolute_error() 로 계산 mae_val = mean_absolute_error(y,pred) print(&#39;RMSLE: {0:.3f}, RMSE: {1:.3F}, MAE: {2:.3F}&#39;.format(rmsle_val, rmse_val, mae_val)) . &#47196;&#44536; &#48320;&#54872;, feature &#51064;&#53076;&#46377;&#44284; &#47784;&#45944; &#54617;&#49845;,&#50696;&#52769;,&#54217;&#44032; . 이제 회귀 모델을 이용해 자전거 대혀 횟수를 예측해보자. 회귀 모델을 적용하기 전에 데이터 세트에 대해서 먼저 처리해야할 사항이 있다. 결괏값이 정규 분포로 돼 있는지 확인하는 것과 카테고리형 회귀 모델의 경우 원-핫 인코딩으로 feature를 인코딩하는 것이다. . 먼저 사이킷런의 LinearRegression 객체를 이용해 회귀 예측을 해보자 | . from sklearn.model_selection import train_test_split , GridSearchCV from sklearn.linear_model import LinearRegression , Ridge , Lasso y_target = bike_df[&#39;count&#39;] X_features = bike_df.drop([&#39;count&#39;],axis=1,inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0) lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) pred = lr_reg.predict(X_test) evaluate_regr(y_test ,pred) . RMSLE: 1.165, RMSE: 140.900, MAE: 105.924 . 실제 Target 데이터 값인 횟수 (count)를 감안하면 예측 오류로서는 비교적 큰 값이다. 실제 값과 예측값이 어느 정도 차이가 나는지 DataFrame의 column으로 만들어서 오류 값이 가장 큰 순으로 5개만 확인해보자. | . def get_top_error_data(y_test, pred, n_tops = 5): # DataFrame에 컬럼들로 실제 대여횟수(count)와 예측 값을 서로 비교 할 수 있도록 생성. result_df = pd.DataFrame(y_test.values, columns=[&#39;real_count&#39;]) result_df[&#39;predicted_count&#39;]= np.round(pred) result_df[&#39;diff&#39;] = np.abs(result_df[&#39;real_count&#39;] - result_df[&#39;predicted_count&#39;]) # 예측값과 실제값이 가장 큰 데이터 순으로 출력. print(result_df.sort_values(&#39;diff&#39;, ascending=False)[:n_tops]) get_top_error_data(y_test,pred,n_tops=20) . real_count predicted_count diff 1618 890 322.0 568.0 3151 798 241.0 557.0 966 884 327.0 557.0 412 745 194.0 551.0 2817 856 310.0 546.0 2277 813 267.0 546.0 2314 766 222.0 544.0 454 721 177.0 544.0 1003 713 171.0 542.0 2394 684 142.0 542.0 1181 891 357.0 534.0 1379 745 212.0 533.0 2003 770 241.0 529.0 1029 901 378.0 523.0 3227 724 202.0 522.0 1038 873 353.0 520.0 3197 694 176.0 518.0 507 688 174.0 514.0 637 900 393.0 507.0 87 594 95.0 499.0 . 회귀에서 이렇게 큰 예측 오류가 발생할 경우 가장 먼저 살펴볼 것은 Target 값의 분포가 왜곡된 형태를 이루고 있는지 확인하는 것이다. Target 값의 분포는 정규 분포 형태가 가장 좋다. 그렇지 않고 왜곡된 경우에는 회귀 예측 성능이 저하되는 경우가 발생하기 쉽다. pandas의 DataFrame의 hist()를 이용해 자전거 대여 모델의 Target 값이 count column이 정규분포를 이루는지 확인해보자 | . y_target.hist() . &lt;AxesSubplot:&gt; . count column이 0~200사이에 왜곡돼 있는 것을 확인할 수 있다. 이렇게 왜곡도니 값을 정규 분포 형태로 바꾸는 가장 일반적인 방법을 로그를 적용해 변환하는 것이다. | 이렇게 로그값으로 변경된 Target값은 다시 expm1() 함수를 적용해 원래 scale 값으로 원상복구하면 된다. | . y_log_transform = np.log1p(y_target) y_log_transform.hist() . &lt;AxesSubplot:&gt; . 정규 분포와 흡사하진 않지만 변환하기 전보다는 왜곡 정도가 많이 향상됐음을 알 수 있다. | 다시 학습한 후 성능을 평가해보자 | . y_target_log = np.log1p(y_target) # 로그 변환된 y_target_log를 반영하여 학습/테스트 데이터 셋 분할 X_train, X_test, y_train, y_test = train_test_split(X_features, y_target_log, test_size=0.3, random_state=0) lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) pred = lr_reg.predict(X_test) # 테스트 데이터 셋의 Target 값은 Log 변환되었으므로 다시 expm1를 이용하여 원래 scale로 변환 y_test_exp = np.expm1(y_test) # 예측 값 역시 Log 변환된 타겟 기반으로 학습되어 예측되었으므로 다시 exmpl으로 scale변환 pred_exp = np.expm1(pred) evaluate_regr(y_test_exp ,pred_exp) . RMSLE: 1.017, RMSE: 162.594, MAE: 109.286 . RMSLE 오류는 줄어들었지만 RMSE는 오히려 더 늘어났다. 왜??????? | 일단 각 feature의 회귀 계수값을 시각화 해보자 | . coef = pd.Series(lr_reg.coef_, index=X_features.columns) coef_sort = coef.sort_values(ascending=False) sns.barplot(x=coef_sort.values, y=coef_sort.index) . &lt;AxesSubplot:&gt; . Year feature의 회귀 계수값이 독보적으로 큰 값을 가지고 있다. Year는 2011,2012년 두개의 값으로만 이루어졌는데 상식적으로 Year가 자전거 대여 횟수에 크게 영향을 준다(회귀 계수가 큼)는 것은 납득하기 어렵다. 그럼 왜 Year feature의 회귀계수가 클까? | Year feature는 datetime이라는 카테고리형 feature지만 숫자형 값으로 돼 있다. 더군다나 아주 큰 2011,2012로 이루어져있다. 사이킷런은 카테고리만을 위한 데이터 타입이 없으며 모두 숫자로 변환해야 한다. 하지만 이처럼 카테고리 값을 선형 회귀에 사용할 경우 회귀 계수를 연산할 때 이 숫자형 값에 크게 영향을 받는 경우가 발생할 수 있다. 따라서 선형 회귀에서는 이러한 feature 인코딩에 원-핫 인코딩을 적용해 변환해야한다. | 오랜만에 판다스의 get_dummies()를 이용해 year칼럼과 더불어 datetime칼럼 그리고 holiday 등등 모두 원-핫 인코딩 후 다시 예측 성능을 확인해보자 | . X_features_ohe = pd.get_dummies(X_features, columns=[&#39;year&#39;,&#39;month&#39;,&#39;hour&#39;, &#39;holiday&#39;, &#39;workingday&#39;,&#39;season&#39;,&#39;weather&#39;]) . X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=0) # 모델과 학습/테스트 데이터 셋을 입력하면 성능 평가 수치를 반환 def get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=False): model.fit(X_train, y_train) pred = model.predict(X_test) if is_expm1 : y_test = np.expm1(y_test) pred = np.expm1(pred) print(&#39;###&#39;,model.__class__.__name__,&#39;###&#39;) evaluate_regr(y_test, pred) # end of function get_model_predict # model 별로 평가 수행 lr_reg = LinearRegression() ridge_reg = Ridge(alpha=10) lasso_reg = Lasso(alpha=0.01) for model in [lr_reg, ridge_reg, lasso_reg]: get_model_predict(model,X_train, X_test, y_train, y_test,is_expm1=True) . ### LinearRegression ### RMSLE: 0.589, RMSE: 97.482, MAE: 63.105 ### Ridge ### RMSLE: 0.589, RMSE: 98.407, MAE: 63.648 ### Lasso ### RMSLE: 0.634, RMSE: 113.031, MAE: 72.658 . 선형 회귀에 예측 성능이 많이 향상 됐다. 원-핫 인코딩된 데이터에서 회귀 계수가 높은 feature를 다시 시각화해보자. 원-핫 인코딩으로 feature가 늘어났으므로 상위 10개만 추출해보자 | . coef = pd.Series(lr_reg.coef_ , index=X_features_ohe.columns) coef_sort = coef.sort_values(ascending=False)[:10] sns.barplot(x=coef_sort.values , y=coef_sort.index) . &lt;AxesSubplot:&gt; . 이번에는 회귀 트리를 이용해 회귀 예측을 수행해보자. 앞에서 적용한 Target 값의 로그 변환된 값과 원-핫 인코딩된 feature 데이터 세트를 그래도 이용해 여러 모델에 대한 성능을 평가해보자. | . from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor # 랜덤 포레스트, GBM, XGBoost, LightGBM model 별로 평가 수행 rf_reg = RandomForestRegressor(n_estimators=500) gbm_reg = GradientBoostingRegressor(n_estimators=500) xgb_reg = XGBRegressor(n_estimators=500) lgbm_reg = LGBMRegressor(n_estimators=500) for model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]: get_model_predict(model,X_train, X_test, y_train, y_test,is_expm1=True) . ### RandomForestRegressor ### RMSLE: 0.354, RMSE: 50.738, MAE: 31.493 ### GradientBoostingRegressor ### RMSLE: 0.340, RMSE: 55.822, MAE: 34.355 ### XGBRegressor ### RMSLE: 0.339, RMSE: 50.950, MAE: 30.891 ### LGBMRegressor ### RMSLE: 0.316, RMSE: 46.473, MAE: 28.777 . 앞의 선형 회귀 모델보다 회귀 예측 성능이 개선됐다. 하지만 이게 선형회귀 보다 회귀 트리가 더 낫다는 건 아니다. 데이터 세트의 유형에 따라 결과는 얼마든지 달라질 수 있다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/12/intro.html",
            "relUrl": "/2022/01/12/intro.html",
            "date": " • Jan 12, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "2022/01/11/TUE",
            "content": "&#45796;&#54637; &#54924;&#44480;&#50752; &#44284;(&#45824;)&#51201;&#54633;/&#44284;&#49548;&#51201;&#54633;&#51032; &#51060;&#54644; . 지금까지 설명한 회귀는 독립변수(feature)와 종속변수(target)의 관계가 일차방정식 형태로 표현된 회귀였다. 회귀가 독립변수의 단항식이 아닌, 2차 또는 3차 방정식과 같은 다항식으로 표현되는 다항 회귀에 대해 알아보자. 유의할 점은 다항회귀는 비선형 회귀가 아닌 선형회귀라는 점이다. (회귀에서 선형 회귀/ 비선형 회귀를 나누는 기준은 회귀 계수가 선형/비선형인지에 따른 것이지 독립 변수의 선형/비선형 여부와는 무관) 사이킷런은 다항 회귀를 위한 클래스를 명시적으로 제공하지 않는다. 대신 다항 회귀 역시 선형 회귀이기 때문에 비선형 함수를 선형 모델에 적용시키는 방법을 사용해 구현해보자. . 다음 예제는 PolynomialFeatures를 이용해 단항값 [$x_1,x_2$] 를 다항값 [$1 , x_1, x_2, x_1^2, x_1x_2, x_2^2$]으로 변환하는 예제이다. | . from sklearn.preprocessing import PolynomialFeatures import numpy as np # 다항식으로 변환할 단항식 생성, [[0,1],[2,3]]의 2X2 행렬 생성 X = np.arange(4).reshape(2,2) print(&#39;일차 단항식 계수 feature: n&#39;,X ) # degree = 2 인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용하여 변환 poly = PolynomialFeatures(degree=2) poly.fit(X) poly_ftr = poly.transform(X) print(&#39;변환된 2차 다항식 계수 feature: n&#39;, poly_ftr) . 일차 단항식 계수 feature: [[0 1] [2 3]] 변환된 2차 다항식 계수 feature: [[1. 0. 1. 0. 0. 1.] [1. 2. 3. 4. 6. 9.]] . 단항 계수 feature [$x_1,x_2$]를 2차 단항 계수 [$1,x_1,x_2,x_1^2,x_1x_2,x_2^2$]로 변경하므로 첫 번째 입력 단항 계수 feature[$x_1 = 0, x_2 = 1$]은 [$1, x_1=0, x_2= 1, x_1^2 = 0, x_1x_2 = 0, x_2^2 = 0$] 형태인 [$1,0,1,0,0,1$]로 변환된다. . | 두번 째 입력 단항 계수 feature [$x_1 = 2, x_2 = 3$]은 [$1, x_1=2, x_2= 3, x_1^2 = 4, x_1x_2 = 6, x_2^2 = 9$]로 변환된다. 이렇게 변환된 Polynomial feature에 선형 회귀를 적용해 다항회귀를 구현한다. Polynomial Feature 클래스가 어떻게 단항식 값을 다항식 값으로 변경하는지 설명했으니 이번에는 3차 다항 계수를 이용해 3차 다항 회귀 함수식을 PolynomialFeatures와 LinearRegression 클래스를 이용해 유도해보자. . | . . 3차 다항 회귀 함수를 임의로 설정하고 이의 회귀 계수를 예측하자. 먼저 3차 다항 회귀의 결정 함수식은 다음과 같이 $y=1+2x_1+3x_1^2+4x_2^3$로 설정하고 이를 위한 함수 polynomial_func()를 만들자. 해당 함수는 다항 계수 feature값이 입력되면 결정 값을 반환한다. | . def polynomial_func(X): y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 return y X = np.arange(0,4).reshape(2,2) print(&#39;일차 단항식 계수 feature: n&#39; ,X) y = polynomial_func(X) print(&#39;삼차 다항식 결정값: n&#39;, y) . 일차 단항식 계수 feature: [[0 1] [2 3]] 삼차 다항식 결정값: [ 5 125] . 이제 일차 단항식 계수를 삼차 다항식 계수로 변환하고, 이를 선형 회귀에 적용하면 다항 회귀로 구현된다. | PolynomialFeatures(degree=3)은 단항 계수 feature[$x_1,x_2$]를 3차 다항 계수[$1,x_1,x_2,x_1^2,x_1x_2,x_2^2,x_1^3,x_1^2x_2,x_1x_2^2,x_1^3$]과 같이 10개의 다항 계수로 변환한다. | . poly_ftr = PolynomialFeatures(degree=3).fit_transform(X) print(&#39;3차 다항식 계수 feature: n&#39;,poly_ftr) # Linear Regression에 3차 다항식 계수 feature와 3차 다항식 결정값으로 학습 후 회귀 계수 확인 from sklearn.linear_model import LinearRegression model = LinearRegression() model.fit(poly_ftr,y) print(&#39;Polynomial 회귀 계수 n&#39; , np.round(model.coef_, 2)) print(&#39;Polynomial 회귀 Shape :&#39;, model.coef_.shape) . 3차 다항식 계수 feature: [[ 1. 0. 1. 0. 0. 1. 0. 0. 0. 1.] [ 1. 2. 3. 4. 6. 9. 8. 12. 18. 27.]] Polynomial 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] Polynomial 회귀 Shape : (10,) . 일차 단항식 계수 feature는 2개였지만, 3차 다항식 Polynomial 변환 이후에는 다항식 계수 feature가 10개로 늘어난다. 이 feature데이터 세트에 LinearRegression을 통해 3차 다항 회귀 형태의 다항 회귀를 적용하면 회귀 계수가 10개로 늘어난다. 10개의 회귀계수는 [$0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34$]가 도출됐으며 원래 다항식 $y=1+2x_1+3x_1^2+4x_2^3$ 계수값인 [$1,2,0,3,0,0,0,0,0,4$]롸는 차이가 있지만 다항 회귀로 근사하고 있음을 알 수 있다. 이처럼 사이킷런은 PolynomialFeatures로 feature를 변환한 후에 LinearRegression클래스로 다항 회귀를 구현한다. | . 전처럼 feature 변환과 선형 회귀 적용을 각각 별도로 하는 것보다는 사이킷런의 Pipeline객체를 이용해 한 번에 다항 회귀를 구현해보자 | . from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline import numpy as np def polynomial_func(X): y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 return y # Pipeline 객체로 Streamline 하게 Polynomial Feature변환과 Linear Regression을 연결 model = Pipeline([(&#39;poly&#39;, PolynomialFeatures(degree=3)), (&#39;linear&#39;, LinearRegression())]) X = np.arange(4).reshape(2,2) y = polynomial_func(X) model = model.fit(X, y) print(&#39;Polynomial 회귀 계수 n&#39;, np.round(model.named_steps[&#39;linear&#39;].coef_, 2)) . Polynomial 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] . &#45796;&#54637; &#54924;&#44480;&#47484; &#51060;&#50857;&#54620; &#44284;&#49548;&#51201;&#54633; &#48143; &#44284;&#51201;&#54633; &#51060;&#54644; . 다항 회귀는 feature의 직선적 관계가 아닌 복잡한 다항 관계를 모델링할 수 있다. 다항식의 차수가 높아질수록 매우 복잡한 feature간의 관계까지 모델링이 가능하다. 하지만 다항 회귀의 차수(degree)를 높일수록 학습 데이터에만 너무 맞춘 학습이 이뤄져서 정작 테스트 데이터 환경에서는 오히려 예측 정확도가 떨어진다. 즉 차수가 높아질수록 과적합의 문제가 크게 발생. . 그 예를 살펴보자 | . 다항 회귀의 차수를 변화시키면서 그에 따른 회귀 예측 곡선과 예측 정확도를 비교해보자 | . import numpy as np import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score %matplotlib inline # random 값으로 구성된 X값에 대해 Cosine 변환값을 반환. def true_fun(X): return np.cos(1.5 * np.pi * X) # X는 0 부터 1까지 30개의 random 값을 순서대로 sampling 한 데이타 입니다. np.random.seed(0) n_samples = 30 X = np.sort(np.random.rand(n_samples)) # y 값은 cosine 기반의 true_fun() 에서 약간의 Noise 변동값을 더한 값입니다. y = true_fun(X) + np.random.randn(n_samples) * 0.1 . plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x26dcffe9910&gt; . 이제 예측 결과를 비교할 다항식 차수를 1,4,15로 변경하며서 예측 결과를 비교해보자 | 다항식 차수별로 학습을 수행한 뒤 cross_val_score()로 MSE 값을 수해 차수별 예측 성능을 평가한다. | 그리고 0부터 1까지 균일하게 구성된 100개의 테스트용 데이터 세트를 이용해 차수별 회귀 예측 곡선을 그려보자 | . plt.figure(figsize=(14, 5)) degrees = [1, 4, 15] # 다항 회귀의 차수(degree)를 1, 4, 15로 각각 변화시키면서 비교합니다. for i in range(len(degrees)): ax = plt.subplot(1, len(degrees), i + 1) plt.setp(ax, xticks=(), yticks=()) # 개별 degree별로 Polynomial 변환합니다. polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) linear_regression = LinearRegression() pipeline = Pipeline([(&quot;polynomial_features&quot;, polynomial_features), (&quot;linear_regression&quot;, linear_regression)]) pipeline.fit(X.reshape(-1, 1), y) # 교차 검증으로 다항 회귀를 평가합니다. scores = cross_val_score(pipeline, X.reshape(-1,1), y,scoring=&quot;neg_mean_squared_error&quot;, cv=10) coefficients = pipeline.named_steps[&#39;linear_regression&#39;].coef_ print(&#39; nDegree {0} 회귀 계수는 {1} 입니다.&#39;.format(degrees[i], np.round(coefficients),2)) print(&#39;Degree {0} MSE 는 {1:.2f} 입니다.&#39;.format(degrees[i] , -1*np.mean(scores))) # 0 부터 1까지 테스트 데이터 세트를 100개로 나눠 예측을 수행합니다. # 테스트 데이터 세트에 회귀 예측을 수행하고 예측 곡선과 실제 곡선을 그려서 비교합니다. X_test = np.linspace(0, 1, 100) # 예측값 곡선 plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=&quot;Model&quot;) # 실제 값 곡선 plt.plot(X_test, true_fun(X_test), &#39;--&#39;, label=&quot;True function&quot;) plt.scatter(X, y, edgecolor=&#39;b&#39;, s=20, label=&quot;Samples&quot;) plt.xlabel(&quot;x&quot;); plt.ylabel(&quot;y&quot;); plt.xlim((0, 1)); plt.ylim((-2, 2)); plt.legend(loc=&quot;best&quot;) plt.title(&quot;Degree {} nMSE = {:.2e}(+/- {:.2e})&quot;.format(degrees[i], -scores.mean(), scores.std())) plt.show() . Degree 1 회귀 계수는 [-2.] 입니다. Degree 1 MSE 는 0.41 입니다. Degree 4 회귀 계수는 [ 0. -18. 24. -7.] 입니다. Degree 4 MSE 는 0.04 입니다. Degree 15 회귀 계수는 [-2.98300000e+03 1.03899000e+05 -1.87415800e+06 2.03715960e+07 -1.44873157e+08 7.09315008e+08 -2.47065753e+09 6.24561150e+09 -1.15676562e+10 1.56895047e+10 -1.54006170e+10 1.06457389e+10 -4.91378211e+09 1.35919860e+09 -1.70381087e+08] 입니다. Degree 15 MSE 는 182493841.87 입니다. . 실선으로 표현된 예측 곡선은 다항 회귀 예측 곡선이며 점선으로 표현된 곡선은 실제 데이터 세트 X,Y,코사인 곡선이다. 학습 데이터는 0부터 1까지의 30개의 임의의 X값과 그에 따른 코사인 Y값에 잡음을 변동 값으로 추가해 구성했으며 MSE 평가는 학습 데이터를 10개의 교차 검증 세트로 나누어 측정해서 평균한 것이다. | . 세 그래프에서 볼 수 있듯이 가운데 Degree=4 예측 곡선이 실제 데이터 세트와 가장 유사함을 알 수 있다. 변동하는 잡음까진 예측하지 못하지만 MSE=0.043으로 가장 뛰어난 예측 성능을 나타냄 | Degree=15 예측곡선은 MSE값이 182493841이 될 정도로 어처구니 없는 오류가 발생. 예측 곡선을 보면 데이터 세트의 변동 잡음값까지 지나치게 반영한 결과 (과적합)예측 곡선이 학습 데이터세트만 정확히 예측하고 테스트값의 실제 곡선과는 완전히 다른 형태의 곡선이 만들어졌음. 결과적으로 학습 데이터에 너무 치중한 나머지 과적합이 심한 모델이 되었고 어이없는 수준의 MSE가 발생됐음. | . $ to$ 즉 좋은 예측 모델은 학습 데이터의 패턴을 지나치게 단순화한 과소적합 모델(Degree=1)도 아니고 학습 데이터의 패턴을 하나하나 감안한 지나치게 복잡한 과적합 모델(Degree=15)도 아닌 학습 데이터의 패턴을 잘 반영하면서도 복잡하지 않은 균형 잡힌 모델이다. . &#54200;&#54693;-&#48516;&#49328; &#53944;&#47112;&#51060;&#46300;&#50724;&#54532; . 머신러닝이 극복해야 할 가장 중요한 이슈 중 하나이다. 앞의 Degree=1 모델은 매우 단순화된 모델로서 지나치게 한 방향으로 치우친 경향이 있는 고편향(High Bias)성을 지님. 반대로 Degree=15과 같은 모델은 학습 데이터 하나하나의 특성을 반영하면서 매우 복잡한 모델이 되었고 지나치게 높은 변동성을 갖게 되었음. 즉 고분산(High Variance)성을 가졌다. . 일반적으로 편향과 분산은 한 쪽이 높으면 한 쪽이 낮아지는 경향이 있다. 즉, 편향이 높으면 분산은 낮아지고 반대로 분산이 높으면 편향이 낮아진다. 편향이 너무 높으면 전체 오류가 높으며 편향을 점점 낮추면 동시에 분산이 높아지고 전체 오류도 낮아진다. 편향을 낮추고 분산을 높이면서 전체오류가 가장 낮은 골디락스지점을 통과하면서 분산을 지속적으로 높이면 전체 오류값이 다시 증가하면서 예측 성능이 다시 저하된다. . | 높은 평향 낮은 분산에서 과소적합되기 쉬우며 낮은 평향 높은 분산에서 과적합되기 쉽다. 편향과 분산이 서로 트레이드오프를 이루면서 오류 Cost값이 최대로 낮아지는 모델을 구축하는 것이 가장 효율적인 머신러닝 예측 모델일 것이다. . | . &#44508;&#51228; &#49440;&#54805; &#47784;&#45944; - &#47551;&#51648;, &#46972;&#50136;, &#50648;&#47532;&#49828;&#54001;&#45367; . 회귀 모델은 적절히 데이터에 적합하면서도 회귀 계수가 기하급수적으로 커지는 것을 제어할 수 있어야한다. 이전까지 선형 모델의 비용함수는 RSS를 최소화하는, 즉 실제값과 예측값의 차이를 최소화하는 것만 고려했고 이는 학습 데이터에 지나치게 맞추게 되고 회귀계수가 쉽게 커짐. 이럴 경우 변동성은 높아지고 테스트 데이터 세트에서는 예측 성능이 저하되기 쉽다. 이를 반영해 비용함수는 학습 데이터의 잔차 오류값을 최소로 하는 RSS최소화 방법과 과접합을 방지하기 위해 계수 값이 커지지 않도록 하는 방법이 서로 균형을 이뤄야한다. . 320p~321p 참고! | 이를 요약하면 $alpha$값으로 페널티를 부여해 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식을 규제(Regularization)라고 부른다. 규제는 크게 L1,L2방식이 있다. L2 규제는 회귀 계수의 제곱에 대해 패널티를 부여(릿지 회귀), L1 규제는 릿쏘 회귀로서 회귀계수의 절댓값에 패널티를 부여한다. L1 규제를 적용하면 영향력이 크지 않은 회귀 계수 값을 0으로 변환한다. | . 릿지 회귀 . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # boston 데이타셋 로드 boston = load_boston() # boston 데이타셋 DataFrame 변환 bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names) # boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. bostonDF[&#39;PRICE&#39;] = boston.target print(&#39;Boston 데이타셋 크기 :&#39;,bostonDF.shape) y_target = bostonDF[&#39;PRICE&#39;] X_data = bostonDF.drop([&#39;PRICE&#39;],axis=1,inplace=False) ridge = Ridge(alpha = 10) neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39; 5 folds 의 개별 Negative MSE scores: &#39;, np.round(neg_mse_scores, 3)) print(&#39; 5 folds 의 개별 RMSE scores : &#39;, np.round(rmse_scores,3)) print(&#39; 5 folds 의 평균 RMSE : {0:.3f} &#39;.format(avg_rmse)) . Boston 데이타셋 크기 : (506, 14) 5 folds 의 개별 Negative MSE scores: [-11.422 -24.294 -28.144 -74.599 -28.517] 5 folds 의 개별 RMSE scores : [3.38 4.929 5.305 8.637 5.34 ] 5 folds 의 평균 RMSE : 5.518 . 규제가 없는 LinearRegression의 RMSE 평균보다 뛰어난 예측 성능을 보임 | alpha 값을 변화시키며 관찰해보자 - 회귀 계수값이 작아질 것임. | . alphas = [0 , 0.1 , 1 , 10 , 100] # alphas list 값을 iteration하면서 alpha에 따른 평균 rmse 구함. for alpha in alphas : ridge = Ridge(alpha = alpha) #cross_val_score를 이용하여 5 fold의 평균 RMSE 계산 neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores)) print(&#39;alpha {0} 일 때 5 folds 의 평균 RMSE : {1:.3f} &#39;.format(alpha,avg_rmse)) . alpha 0 일 때 5 folds 의 평균 RMSE : 5.829 alpha 0.1 일 때 5 folds 의 평균 RMSE : 5.788 alpha 1 일 때 5 folds 의 평균 RMSE : 5.653 alpha 10 일 때 5 folds 의 평균 RMSE : 5.518 alpha 100 일 때 5 folds 의 평균 RMSE : 5.330 . alpha가 100일 때 평균 RMSE가 가장 좋다. | 이번에는 alpha값의 변화에 따른 feature의 회귀 계수 값을 시각화해보자 | . import seaborn as sns # 각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성 fig , axs = plt.subplots(figsize=(18,6) , nrows=1 , ncols=5) # 각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 DataFrame 생성 coeff_df = pd.DataFrame() # alphas 리스트 값을 차례로 입력해 회귀 계수 값 시각화 및 데이터 저장. pos는 axis의 위치 지정 for pos , alpha in enumerate(alphas) : ridge = Ridge(alpha = alpha) ridge.fit(X_data , y_target) # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. coeff = pd.Series(data=ridge.coef_ , index=X_data.columns ) colname=&#39;alpha:&#39;+str(alpha) coeff_df[colname] = coeff # 막대 그래프로 각 alpha 값에서의 회귀 계수를 시각화. 회귀 계수값이 높은 순으로 표현 coeff = coeff.sort_values(ascending=False) axs[pos].set_title(colname) axs[pos].set_xlim(-3,6) sns.barplot(x=coeff.values , y=coeff.index, ax=axs[pos]) # for 문 바깥에서 맷플롯립의 show 호출 및 alpha에 따른 피처별 회귀 계수를 DataFrame으로 표시 plt.show() . alpha 값이 커질수록 회귀 계수 값은 작아짐을 알 수 있다. 특히 NOX feature의 경우 alpha 값을 계속 증가시킴에 따라 회귀 계수가 크게 작아지고 있다. | Dataframe에 저장된 alpha 값의 변화에 따른 릿지 회귀 계수 값을 구해보자 | . ridge_alphas = [0 , 0.1 , 1 , 10 , 100] sort_column = &#39;alpha:&#39;+str(ridge_alphas[0]) coeff_df.sort_values(by=sort_column, ascending=False) . alpha:0 alpha:0.1 alpha:1 alpha:10 alpha:100 . RM 3.809865 | 3.818233 | 3.854000 | 3.702272 | 2.334536 | . CHAS 2.686734 | 2.670019 | 2.552393 | 1.952021 | 0.638335 | . RAD 0.306049 | 0.303515 | 0.290142 | 0.279596 | 0.315358 | . ZN 0.046420 | 0.046572 | 0.047443 | 0.049579 | 0.054496 | . INDUS 0.020559 | 0.015999 | -0.008805 | -0.042962 | -0.052826 | . B 0.009312 | 0.009368 | 0.009673 | 0.010037 | 0.009393 | . AGE 0.000692 | -0.000269 | -0.005415 | -0.010707 | 0.001212 | . TAX -0.012335 | -0.012421 | -0.012912 | -0.013993 | -0.015856 | . CRIM -0.108011 | -0.107474 | -0.104595 | -0.101435 | -0.102202 | . LSTAT -0.524758 | -0.525966 | -0.533343 | -0.559366 | -0.660764 | . PTRATIO -0.952747 | -0.940759 | -0.876074 | -0.797945 | -0.829218 | . DIS -1.475567 | -1.459626 | -1.372654 | -1.248808 | -1.153390 | . NOX -17.766611 | -16.684645 | -10.777015 | -2.371619 | -0.262847 | . alpha 값이 증가하면서 회귀 계수가 지속적으로 작아지고 있음을 알 수 있다. 하지만 릿지 회귀의 경우에는 회귀 계수를 0으로 만들진 않는다. | . 라쏘 회귀 . 유의 : L1 규제는 불필요한 회귀 계수를 급격하게 감소시켜 0으로 만들고 제거한다. 이러한 측면에서 L1 규제는 적절한 feature만 회귀에 포함시키는 feature 선택의 특성을 갖고 있다. | . from sklearn.linear_model import Lasso, ElasticNet # alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환 def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True): coeff_df = pd.DataFrame() if verbose : print(&#39;####### &#39;, model_name , &#39;#######&#39;) for param in params: if model_name ==&#39;Ridge&#39;: model = Ridge(alpha=param) elif model_name ==&#39;Lasso&#39;: model = Lasso(alpha=param) elif model_name ==&#39;ElasticNet&#39;: model = ElasticNet(alpha=param, l1_ratio=0.7) neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores)) print(&#39;alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.3f} &#39;.format(param, avg_rmse)) # cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출 model.fit(X_data , y_target) # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. coeff = pd.Series(data=model.coef_ , index=X_data.columns ) colname=&#39;alpha:&#39;+str(param) coeff_df[colname] = coeff return coeff_df # end of get_linear_regre_eval . lasso_alphas = [ 0.07, 0.1, 0.5, 1, 3] coeff_lasso_df =get_linear_reg_eval(&#39;Lasso&#39;, params=lasso_alphas, X_data_n=X_data, y_target_n=y_target) . ####### Lasso ####### alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 5.612 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.615 alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 5.669 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.776 alpha 3일 때 5 폴드 세트의 평균 RMSE: 6.189 . alpha가 0.07일 때 가장 좋은 평균RMSE를 보인다. 앞의 릿지일때보단 저하됐다. 그렇지만 규제가 없을 때보단 낫다. | . sort_column = &#39;alpha:&#39;+str(lasso_alphas[0]) coeff_lasso_df.sort_values(by=sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.789725 | 3.703202 | 2.498212 | 0.949811 | 0.000000 | . CHAS 1.434343 | 0.955190 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.270936 | 0.274707 | 0.277451 | 0.264206 | 0.061864 | . ZN 0.049059 | 0.049211 | 0.049544 | 0.049165 | 0.037231 | . B 0.010248 | 0.010249 | 0.009469 | 0.008247 | 0.006510 | . NOX -0.000000 | -0.000000 | -0.000000 | -0.000000 | 0.000000 | . AGE -0.011706 | -0.010037 | 0.003604 | 0.020910 | 0.042495 | . TAX -0.014290 | -0.014570 | -0.015442 | -0.015212 | -0.008602 | . INDUS -0.042120 | -0.036619 | -0.005253 | -0.000000 | -0.000000 | . CRIM -0.098193 | -0.097894 | -0.083289 | -0.063437 | -0.000000 | . LSTAT -0.560431 | -0.568769 | -0.656290 | -0.761115 | -0.807679 | . PTRATIO -0.765107 | -0.770654 | -0.758752 | -0.722966 | -0.265072 | . DIS -1.176583 | -1.160538 | -0.936605 | -0.668790 | -0.000000 | . alpha의 크기가 증가함에 따라 일부 feature의 회귀 계수는 아예 0으로 바뀌고 있음을 알 수 있다. 이때 회귀 계수가 0인 feature는 회귀 식에서 제외되면서 feature선택의 효과를 얻을 수 있다. | . 엘라스틱넷 회귀 . L2와 L1규제를 결합한 회귀. 엘라스틱넷은 라쏘 회귀가 서로 상관관계가 높은 feature들의 경우에 이들 중요서 중요 feature만을 selection하고 다른 feature들은 모두 회귀 계수를 0으로 만드는 경향이 강하다. 이러한 성향으로 인해 alpha값에 따라 회귀 계수의 값이 급격히 변동할 수도 있는데 엘라스틱넷 회귀는 이를 완화하기 위해 L2 규제를 라쏘회귀에 추가한 것. 반대로 엘라스틱넷 회귀의 단점은 L1과 L2 규제가 결합된 규제로 인해 수행시간이 다소 오래 걸린다는 점이 있다. | 유의해야할 점 : ElasticNet 클래스에서의 alpha 파라미터 값은 L1과 L2규제의 alpha값인 a와 b를 더한 a+b값을 의미한다. ElasticNet 클래스의 11_ratio 파라미터 값은 a/(a+b)이다. 11_ratio가 0이면 a가 0이므로 L2규제와 동일, 1이면 L1규제와 동일하다. | . # l1_ratio는 0.7로 고정 elastic_alphas = [ 0.07, 0.1, 0.5, 1, 3] coeff_elastic_df =get_linear_reg_eval(&#39;ElasticNet&#39;, params=elastic_alphas, X_data_n=X_data, y_target_n=y_target) . ####### ElasticNet ####### alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 5.542 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.526 alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 5.467 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.597 alpha 3일 때 5 폴드 세트의 평균 RMSE: 6.068 . sort_column = &#39;alpha:&#39;+str(elastic_alphas[0]) coeff_elastic_df.sort_values(by=sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.574162 | 3.414154 | 1.918419 | 0.938789 | 0.000000 | . CHAS 1.330724 | 0.979706 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.278880 | 0.283443 | 0.300761 | 0.289299 | 0.146846 | . ZN 0.050107 | 0.050617 | 0.052878 | 0.052136 | 0.038268 | . B 0.010122 | 0.010067 | 0.009114 | 0.008320 | 0.007020 | . AGE -0.010116 | -0.008276 | 0.007760 | 0.020348 | 0.043446 | . TAX -0.014522 | -0.014814 | -0.016046 | -0.016218 | -0.011417 | . INDUS -0.044855 | -0.042719 | -0.023252 | -0.000000 | -0.000000 | . CRIM -0.099468 | -0.099213 | -0.089070 | -0.073577 | -0.019058 | . NOX -0.175072 | -0.000000 | -0.000000 | -0.000000 | -0.000000 | . LSTAT -0.574822 | -0.587702 | -0.693861 | -0.760457 | -0.800368 | . PTRATIO -0.779498 | -0.784725 | -0.790969 | -0.738672 | -0.423065 | . DIS -1.189438 | -1.173647 | -0.975902 | -0.725174 | -0.031208 | . alpha가 0.5일 때 RMSE가 5.468로 가장 좋은 예측 성능을 보이고 있다. alpha값에 따른 feature들의 회귀 계수들의 값이 라쏘보다는 상대적으로 0이 되는 값이 적음을 알 수 있다. | 지금까지 규제 선형 회귀의 가장 대표적인 기법인 릿지, 라쏘, 엘라스틱넷 회귀를 살펴봤다. 이들 중 어떤 것이 가장 좋은지는 상황에 따라 다르므로 각각의 알고리즘에서 하이퍼 파라미터를 변경해 가면서 최적의 예측 성능을 찾아내야 한다. 하지만 선형 회귀의 경우 최적의 하이퍼 파라미터를 찾아내는 것 못지않게 먼저 데이터 분포도의 정규화와 인코딩 방법이 매우 중요하다. | . 선형 회귀 모델을 위한 데이터 변환 . 선형 회귀 모델과 같은 선형 모델은 일반적으로 feature와 target값 간에 선형의 관계가 있다고 가정하고 이러한 최적의 선형함수를 찾아내 결과값을 예측한다. 또한 선형 회귀 모델을 적용하기 전에 먼저 데이터에 대한 스케일링/정규화 작업을 수행하는 것이 일반적이다. 이러한 스케일링과 정규화 작업이 무조건 성능향상으로 이어지는 것은 아니지만 중요 feature들이나 target값의 분포도가 심하게 왜곡됐을 경우에 이러한 변환 작업을 수행한다. . | 일반적으로 feature 데이터 세트와 target 데이터 세트에 적용하는 스케일링/정규화 방법이 상이하다. . feature 데이터 세트에는 StandardScaler 클래스를 이용해 표준 정규 분포를 가진 데이터 세트로 변환하거나 MinMaxScaler 클래스를 이용해 최솟값이 0이고 최댓값이 1인 값으로 정규화를 수행한다. 이때 성능 향상이 없다면 다항 특성을 적용하여 변환할 수 있으며 또는 로그 변환을 통해 변환을 수행할 수 있다. 로그 변환이 가장 유용한 방법이다. 타깃값의 경우엔 일반적으로 로그 변환을 수행한다. | . | . 보스턴 주택가격 feature 데이터 세트에 위에서 언급한 표준 정규 분포 변환, 최댓값/최솟값 정규화, 로그 변환을 차례로 적용한 후에 예측 성능을 평가해보자 | . from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures # method는 표준 정규 분포 변환(Standard), 최대값/최소값 정규화(MinMax), 로그변환(Log) 결정 # p_degree는 다향식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음. def get_scaled_data(method=&#39;None&#39;, p_degree=None, input_data=None): if method == &#39;Standard&#39;: scaled_data = StandardScaler().fit_transform(input_data) elif method == &#39;MinMax&#39;: scaled_data = MinMaxScaler().fit_transform(input_data) elif method == &#39;Log&#39;: scaled_data = np.log1p(input_data) else: scaled_data = input_data if p_degree != None: scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data) return scaled_data . 이제 Ridge 클래스의 alpha값을 변화시키면서 feature 데이터 세트를 여러가지 방법으로 변환한 데이터 세트를 입력 받을 경우에 RMSE값이 어떻게 변하는지 살펴보자 . alphas = [0.1, 1, 10, 100] #변환 방법은 모두 6개, 원본 그대로, 표준정규분포, 표준정규분포+다항식 특성 # 최대/최소 정규화, 최대/최소 정규화+다항식 특성, 로그변환 scale_methods=[(None, None), (&#39;Standard&#39;, None), (&#39;Standard&#39;, 2), (&#39;MinMax&#39;, None), (&#39;MinMax&#39;, 2), (&#39;Log&#39;, None)] for scale_method in scale_methods: X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=X_data) print(&#39; n## 변환 유형:{0}, Polynomial Degree:{1}&#39;.format(scale_method[0], scale_method[1])) get_linear_reg_eval(&#39;Ridge&#39;, params=alphas, X_data_n=X_data_scaled, y_target_n=y_target, verbose=False) . ## 변환 유형:None, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.788 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.653 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.518 alpha 100일 때 5 폴드 세트의 평균 RMSE: 5.330 ## 변환 유형:Standard, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.826 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.803 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.637 alpha 100일 때 5 폴드 세트의 평균 RMSE: 5.421 ## 변환 유형:Standard, Polynomial Degree:2 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 8.827 alpha 1일 때 5 폴드 세트의 평균 RMSE: 6.871 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.485 alpha 100일 때 5 폴드 세트의 평균 RMSE: 4.634 ## 변환 유형:MinMax, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.764 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.465 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.754 alpha 100일 때 5 폴드 세트의 평균 RMSE: 7.635 ## 변환 유형:MinMax, Polynomial Degree:2 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.298 alpha 1일 때 5 폴드 세트의 평균 RMSE: 4.323 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.185 alpha 100일 때 5 폴드 세트의 평균 RMSE: 6.538 ## 변환 유형:Log, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 4.770 alpha 1일 때 5 폴드 세트의 평균 RMSE: 4.676 alpha 10일 때 5 폴드 세트의 평균 RMSE: 4.836 alpha 100일 때 5 폴드 세트의 평균 RMSE: 6.241 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/11/intro.html",
            "relUrl": "/2022/01/11/intro.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "2022/01/10/MON",
            "content": "&#54924;&#44480; . 회귀 분석은 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향으르 이용한 통계학 기법이다. 회귀는 여러 개의 독립 변수와 한 개의 종속 변수 간의 상관관계를 모델링하는 기법을 통칭한다. 예를 들면, 아파트의 방 개수, 방 크기, 주변 학군, 등 여러 개의 독립변수에 따라 아파트 가격이라는 종속변수가 어떤 관계를 나타내는지를 모델링하고 예측하는 것이다. $Y = W_1*X_1 + W_2*X_2 + W_3*X_3 + dots + W_n*X_n$ 이라는 선형 회귀식을 예로 들면 $Y$는 종속변수, 즉 아파트 가격을 의미하며, 나머지 $X_1,X_2,X_3, dots,X_n$은 방 개수, 방 크기, 주변 학군 등의 독립 변수를 의미한다. 그리고 $W_1,W_2,W_3, dots,W_n$은 이 독립변수의 값에 영향을 미치는 회귀 계수이다. 머신 러닝의 관점에서 보면 독립변수는 feature에 해당하며 종속변수는 결정 값, 즉 레이블을 의미한다. 머신러닝 회귀 예측의 핵심은 주어진 feature와 결정 값 데이터 기반에서 학습을 통해 최적의 회귀계수를 찾아내는 것이다. . &#54924;&#44480; &#51333;&#47448; . 독립 변수의 개수가 1개이면 단일 회귀, 여러 개이면 다중 회귀이다. 또한 회귀계수의 결합이 선형이면 선형 회귀, 비선형이면 비선형 회귀이다. . 지도학습은 두 가지 유형으로 나뉘는데 바로 분류와 회귀이다. 이 두가지 기법의 가장 큰 차이는 분류는 예측값이 카테고리와 같은 이산형 클래스 값이고 회귀는 연속형 숫자값이라는 것이다. | 여러 가지 회귀 중에서 선형 회귀가 가장 많이 사용된다. 선형 회귀는 실제 값과 예측값의 차이(오류의 제곱 값)를 최소화하는 직선형 회귀선을 최적화하는 방식이다. 선형 회귀 모델은 규제 방법에 따라 또 나눌 수 있다. 여기서 규제란 일반적인 선형 회귀의 과적합 문제를 해결하기 위해서 회귀 계수에 페널티 값을 적용하는 것을 의미한다. | . . 단순 선형 회귀에 대해 알아보자 : 독립 변수도 하나 종속 변수도 하나인 선형 회귀를 의미한. 예를 들면 주택 가격이 주택의 크기로만 결정되는 것. | 예측값 $ hat{Y}$는 $w_0 + w_1*X$로 계산할 수 있다. 독립변수가 1개인 단순 선형 회귀에서는 이 기울기 $w_1$과 절편 $w_0$을 회귀 계수로 지칭한다. 그리고 회귀 모델을 $ hat{Y} = w_0 + w_1*X$와 같은 1차 함수로 모델링했다면 실제 주택 가격은 이러한 1차 함수 값에서 실제 값만큼의 오류 값을 뺀 또는 더한 값이 된다. ($w_0 + w_1*X +$ 오류값) | 이렇게 실제 값과 회귀 모델의 차이에 따른 오류 값을 남은 오류, 즉 잔차라고 부른다. 최적의 회귀 모델을 만든다는 것이 바로 전체 데이터의 잔차 합이 최소가 되는 모델을 만든다는 것이며 상쇄될 것을 고려해 대개 절댓값을 취하거나 제곱을 한 뒤 오류 합을 구한다. $Error^2 = RSS $ | RSS는 비용이며 $w$변수(회귀 계수)로 구성되는 $RSS$를 비용 함수라고 한다. 머신 러닝 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값(즉, 오류값)을 지속해서 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류값을 구하는 것이다. 비용함수를 손실함수라고도 한다. | . . &#44221;&#49324;&#54616;&#44053;&#48277; . 점진적으로 반복적인 계산을 통해 $W$ 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 $W$ 파라미터를 구하는 방식이다. 경사 하강법은 반복적으로 비용 함수의 반환 값 즉, 예측값과 실제 값의 차이가 작아지는 방향성을 가지고 $W$ 파라미터를 지속해서 보정해 나간다. 오류를 감소시키는 방향으로 $W$값을 계속 업데이트해 나가면서 더 이상 그 오류 값이 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그때의 $W$ 값을 최적 파라미터로 반환한다. . 예를 들어 비용 함수가 포물선 형태의 2차 함수라면 경사 하강법은 최초 $w$에서부터 미분을 적용한 뒤 이 미분 값이 계속 감소하는 방향으로 순차적으로 $w$를 업데이트한다. 마침내 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그때의 $w$를 반환한다. | . 경사 하강법을 파이썬 코드로 구현해보자 | . import numpy as np import matplotlib.pyplot as plt np.random.seed(0) # y = 4X + 6 식을 근사(w1=4, w0=6). random 값은 Noise를 위해 만듬 X = 2 * np.random.rand(100,1) # X의 범위 설정 y = 6 + 4 * X + np.random.randn(100,1) # X, y 데이터 셋 scatter plot으로 시각화 plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x1b994b27550&gt; . def get_weight_updates(w1, w0, X, y, learning_rate=0.01): N = len(y) # 먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화 w1_update = np.zeros_like(w1) w0_update = np.zeros_like(w0) # 예측 배열 계산하고 예측과 실제 값의 차이 계산 y_pred = np.dot(X, w1.T) + w0 diff = y-y_pred # w0_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성 w0_factors = np.ones((N,1)) # w1과 w0을 업데이트할 w1_update와 w0_update 계산 w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff)) w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff)) return w1_update, w0_update . def gradient_descent_steps(X, y, iters=10000): # w0와 w1을 모두 0으로 초기화. w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) # 인자로 주어진 iters 만큼 반복적으로 get_weight_updates() 호출하여 w1, w0 업데이트 수행. for ind in range(iters): w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . def get_cost(y, y_pred): N = len(y) cost = np.sum(np.square(y - y_pred))/N return cost w1, w0 = gradient_descent_steps(X, y, iters=1000) print(&quot;w1:{0:.3f} w0:{1:.3f}&quot;.format(w1[0,0], w0[0,0])) y_pred = w1[0,0] * X + w0 print(&#39;Gradient Descent Total Cost:{0:.4f}&#39;.format(get_cost(y, y_pred))) . w1:4.022 w0:6.162 Gradient Descent Total Cost:0.9935 . y_pred에 기반해 회귀선을 그려보자 . plt.scatter(X, y) plt.plot(X,y_pred) . [&lt;matplotlib.lines.Line2D at 0x1b9980f5940&gt;] . . 경사 하강법을 이용해 회귀선이 잘 만들어졌음을 알 수 있다. 일반적으로 경사 하강법은 모든 학습 데이터에 대해 반복적으로 비용함수 최소화를 위한 값을 업데이트하기 때문에 수행 시간이 매우 오래 걸린다. 이 때문에 실전에서는 확률적 경사 하강법(Stochastic Gradient Descent)을 이용한다. 확률적 경사 하강법을 일부 데이터만 이용해 w가 업데이트되는 값을 계산하므로 경사하강법에 비해 빠른 속도를 보장한다. 해보자. | . def stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) prev_cost = 100000 iter_index =0 for ind in range(iters): np.random.seed(ind) # 전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터 추출하여 sample_X, sample_y로 저장 stochastic_random_index = np.random.permutation(X.shape[0]) sample_X = X[stochastic_random_index[0:batch_size]] sample_y = y[stochastic_random_index[0:batch_size]] # 랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update, w0_update 계산 후 업데이트 w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . 만들어진 함수를 이용해 w1,w0 및 예측 오류 비용을 계산해 보자 . w1, w0 = stochastic_gradient_descent_steps(X, y, iters=1000) print(&quot;w1:&quot;,round(w1[0,0],3),&quot;w0:&quot;,round(w0[0,0],3)) y_pred = w1[0,0] * X + w0 print(&#39;Stochastic Gradient Descent Total Cost:{0:.4f}&#39;.format(get_cost(y, y_pred))) . w1: 4.028 w0: 6.156 Stochastic Gradient Descent Total Cost:0.9937 . 확률적 경사 하강법으로 구한 w0,w1 결과는 경사 하강법으로 구한 w1,w0과 큰 차이가 없으며 예측 오류 비용 또한 경사하강법으로 구한 것보다 아주 조금 높다. 즉 성능 차이가 미미하다. 데이터가 크면 확률적 경사 하강법을 이용하라. | . . &#49324;&#51060;&#53431;&#47088; LinearRegression&#51012; &#51060;&#50857;&#54620; &#48372;&#49828;&#53556; &#51452;&#53469; &#44032;&#44201; &#50696;&#52769; . LinearRegression 클래스는 예측값과 실제 값의 RSS를 최소화 해 OLS 추정 방식으로 구현한 클래스이다. . OLS 기반의 회귀 계수 계산은 입력 feature의 독립성에 많은 영향을 받는다. feature간의 상관관계가 매우 높은 경우 분산이 매우 커져서 오류에 매우 민감해진다. 이러한 현상을 다중공선성(multi-collinearity)문제라고 한다. 일반적으로 상관관계가 높은 feature가 많은 경우 독립적인 중요한 feature만 남기고 제거하거나 규제를 적용한다. 또한 매우 많은 feature가 다중 공선성 문제를 가지고 있다면 PCA를 통해 차원 축소를 수행하는 것도 고려해 볼 수 있다. | . 회귀를 위한 평가 지표 : 실제 값과 회귀 예측값의 차이 값을 기반으로 한 지표가 중심이다. | . 305p 참고! | . import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from scipy import stats from sklearn.datasets import load_boston %matplotlib inline # boston 데이타셋 로드 boston = load_boston() # boston 데이타셋 DataFrame 변환 bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names) # boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. bostonDF[&#39;PRICE&#39;] = boston.target print(&#39;Boston 데이타셋 크기 :&#39;,bostonDF.shape) bostonDF.head() . Boston 데이타셋 크기 : (506, 14) . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . 다음으로 각 column이 회귀 결과에 미치는 영향이 어느 정도인지 시각화해서 알아보자. 총 8개의 칼럼에 대해 값이 증가할수록 PRICE값이 어떻게 변화하는지 확인하자. . fig, axs = plt.subplots(figsize=(16,8) , ncols=4 , nrows=2) lm_features = [&#39;RM&#39;,&#39;ZN&#39;,&#39;INDUS&#39;,&#39;NOX&#39;,&#39;AGE&#39;,&#39;PTRATIO&#39;,&#39;LSTAT&#39;,&#39;RAD&#39;] for i , feature in enumerate(lm_features): row = int(i/4) col = i%4 # 시본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현 sns.regplot(x=feature , y=&#39;PRICE&#39;,data=bostonDF , ax=axs[row][col]) . 다른 칼럼보다 RM과 LSTAT의 PRICE 영향도가 가장 두드러지게 나타난다. RM은 양 방향의 선형성이 가장 크다. 즉 방의 크기가 클수록 방의 가격이 증가하는 모습을 확연히 보여준다. | . from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error , r2_score y_target = bostonDF[&#39;PRICE&#39;] X_data = bostonDF.drop([&#39;PRICE&#39;],axis=1,inplace=False) X_train , X_test , y_train , y_test = train_test_split(X_data , y_target ,test_size=0.3, random_state=156) # Linear Regression OLS로 학습/예측/평가 수행. lr = LinearRegression() lr.fit(X_train ,y_train ) y_preds = lr.predict(X_test) mse = mean_squared_error(y_test, y_preds) rmse = np.sqrt(mse) print(&#39;MSE : {0:.3f} , RMSE : {1:.3F}&#39;.format(mse , rmse)) print(&#39;Variance score : {0:.3f}&#39;.format(r2_score(y_test, y_preds))) . MSE : 17.297 , RMSE : 4.159 Variance score : 0.757 . print(&#39;절편 값:&#39;,lr.intercept_) print(&#39;회귀 계수값:&#39;, np.round(lr.coef_, 1)) . 절편 값: 40.99559517216445 회귀 계수값: [ -0.1 0.1 0. 3. -19.8 3.4 0. -1.7 0.4 -0. -0.9 0. -0.6] . coef_ 속성은 회귀 계수 값만 가지고 있으므로 이를 feature별 회귀 계수 값으로 다시 mapping하고 높은 값 순으로 출력해보자. 이를 위해 pandas Series의 sort_values() 함수를 이용한다. . coeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns ) coeff.sort_values(ascending=False) . RM 3.4 CHAS 3.0 RAD 0.4 ZN 0.1 INDUS 0.0 AGE 0.0 TAX -0.0 B 0.0 CRIM -0.1 LSTAT -0.6 PTRATIO -0.9 DIS -1.7 NOX -19.8 dtype: float64 . from sklearn.model_selection import cross_val_score y_target = bostonDF[&#39;PRICE&#39;] X_data = bostonDF.drop([&#39;PRICE&#39;],axis=1,inplace=False) lr = LinearRegression() # cross_val_score( )로 5 Fold 셋으로 MSE 를 구한 뒤 이를 기반으로 다시 RMSE 구함. neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) # cross_val_score(scoring=&quot;neg_mean_squared_error&quot;)로 반환된 값은 모두 음수 print(&#39; 5 folds 의 개별 Negative MSE scores: &#39;, np.round(neg_mse_scores, 2)) print(&#39; 5 folds 의 개별 RMSE scores : &#39;, np.round(rmse_scores, 2)) print(&#39; 5 folds 의 평균 RMSE : {0:.3f} &#39;.format(avg_rmse)) . 5 folds 의 개별 Negative MSE scores: [-12.46 -26.05 -33.07 -80.76 -33.31] 5 folds 의 개별 RMSE scores : [3.53 5.1 5.75 8.99 5.77] 5 folds 의 평균 RMSE : 5.829 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/10/intro.html",
            "relUrl": "/2022/01/10/intro.html",
            "date": " • Jan 10, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "2022/01/09/SUN",
            "content": "print(&#39;학습 데이터 레이블 값 비율&#39;) print(y_train.value_counts()/y_train.shape[0] * 100) print(&#39;테스트 데이터 레이블 값 비율&#39;) print(y_test.value_counts()/y_test.shape[0] * 100) . 학습 데이터 레이블 값 비율 0 99.827451 1 0.172549 Name: Class, dtype: float64 테스트 데이터 레이블 값 비율 0 99.826785 1 0.173215 Name: Class, dtype: float64 . 학습데이터 레이블과 테스트 레이블을 살펴본 결 과 잘 분할 됐음. 이제 로지스틱 회귀와 LightGBM 기반의 모델이 데이터 가공을 수행하면서 예측 성능이 어떻게 변하는지 살펴보자 . 먼저 로지스틱 회귀를 이용해 신용 카드 사기 여부를 예측해보자 . from sklearn.linear_model import LogisticRegression lr_clf = LogisticRegression() lr_clf.fit( X_train, y_train) lr_pred = lr_clf.predict(X_test) lr_pred_proba = lr_clf.predict_proba(X_test)[:, 1] # 3장에서 사용한 get_clf_eval() 함수를 이용하여 평가 수행. get_clf_eval(y_test, lr_pred, lr_pred_proba) . 이번에는 LightGBM을 이용한 모델을 만들어보자 . (앞으로 수행할 예제 코드에서 반복적으로 모델을 변경해 학습/예측/평가할 것이므로 이를 위한 별도의 함수를 생성해보자) . def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None): model.fit(ftr_train, tgt_train) pred = model.predict(ftr_test) pred_proba = model.predict_proba(ftr_test)[:, 1] get_clf_eval(tgt_test, pred, pred_proba) . 먼저, 본 데이터 세트는 극도로 불균형한 레이블 값 분포를 지녔음 $ to$ LightGBMClassifier객체 생성 시 boost_from_average=False로 파라미터를 설정해야한다. | . LightGBM으로 모델을 학습한 뒤 별도의 테스트 데이터 세트에서 예측 평가를 수행해보자 | . from lightgbm import LGBMClassifier lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . 오차 행렬 [[85290 5] [ 36 112]] 정확도: 0.9995, 정밀도: 0.9573, 재현율: 0.7568, F1: 0.8453, AUC:0.9790 . 재현율과 ROC-AUC가 회귀보다 높은 수치 기록. | . 왜곡된 분포도를 가지는 데이터를 재가공한 뒤 모델을 다시 테스트해보자. | 대부분의 선형 모델은 중요 feature들의 값이 정규 분포 형태를 유지하는 것을 선호. | Amount feature는 중요 feature일 가능성이 높음. | . import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings import seaborn as sns warnings.filterwarnings(&quot;ignore&quot;) %matplotlib inline card_df = pd.read_csv(&#39;./creditcard.csv&#39;) card_df.head(3) plt.figure(figsize=(8, 4)) plt.xticks(range(0, 30000, 1000), rotation=60) sns.distplot(card_df[&#39;Amount&#39;]) . &lt;AxesSubplot:xlabel=&#39;Amount&#39;, ylabel=&#39;Density&#39;&gt; . # Amount를 정규분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 수행. X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) print(&#39;### 로지스틱 회귀 예측 성능 ###&#39;) lr_clf = LogisticRegression() get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) print(&#39;### LightGBM 예측 성능 ###&#39;) lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . 성능이 크게 개선되지 X $ to$ 이번에는 StandardScaler가 아니라 로그 변환을 수행해보자. | 로그 변환은 데이터 분포도가 심하게 왜곡되어 있을 경우 적용하는 중요 기법 중 하나.(원래 값을 log 값으로 변환해 원래 큰 값을 상대적으로 작은 값으로 변환하기 때문에 데이터 분포도의 왜곡을 상당수준 개선해준다.) | . def get_preprocessed_df(df=None): df_copy = df.copy() # 넘파이의 log1p( )를 이용하여 Amount를 로그 변환 amount_n = np.log1p(df_copy[&#39;Amount&#39;]) df_copy.insert(0, &#39;Amount_Scaled&#39;, amount_n) df_copy.drop([&#39;Time&#39;,&#39;Amount&#39;], axis=1, inplace=True) return df_copy . 이제 Amount feature를 log 변환한 후 다시 로지스틱 회귀와 LightGBM 모델을 적용한 후 예측 성능을 확인해보자 | . . # log1p 와 expm1 설명 import numpy as np print(1e-1000 == 0.0) print(np.log(1e-1000)) print(np.log(1e-1000 + 1)) print(np.log1p(1e-1000)) . True -inf 0.0 0.0 . var_1 = np.log1p(100) var_2 = np.expm1(var_1) print(var_1, var_2) . 4.61512051684126 100.00000000000003 . . X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) print(&#39;### 로지스틱 회귀 예측 성능 ###&#39;) get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) print(&#39;### LightGBM 예측 성능 ###&#39;) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . ### 로지스틱 회귀 예측 성능 ### 오차 행렬 [[85283 12] [ 59 89]] 정확도: 0.9992, 정밀도: 0.8812, 재현율: 0.6014, F1: 0.7149, AUC:0.9727 ### LightGBM 예측 성능 ### 오차 행렬 [[85290 5] [ 35 113]] 정확도: 0.9995, 정밀도: 0.9576, 재현율: 0.7635, F1: 0.8496, AUC:0.9796 . 두 모델 모두 정밀도, 재현율, ROC-AUC에서 약간씩 성능이 개선되었음을 알 수 있음 | . &#51060;&#49345;&#52824; &#45936;&#51060;&#53552; :Outlier ?? &#51204;&#52404; &#45936;&#51060;&#53552;&#51032; &#54056;&#53556;&#50640;&#49436; &#48279;&#50612;&#45212; &#51060;&#49345; &#44050;&#51012; &#44032;&#51652; &#45936;&#51060;&#53552;. IQR&#48169;&#49885;&#51060; &#51080;&#51020;. IQR&#51008; &#49324;&#48516;&#50948; &#44050;&#51032; &#54200;&#52264;&#47484; &#51060;&#50857;&#54616;&#45716; &#44592;&#48277;&#51004;&#47196;&#49436; &#55124;&#55176; &#48149;&#49828;&#54540;&#46991;&#51004;&#47196; &#49884;&#44033;&#54868; &#54624; &#49688; &#51080;&#45796;. . 먼저 사분위란 전체 데이터를 값이 높은 순으로 정렬하고 이를 25%씩 구간 분할하는 것을 지칭. 가령 100명의 시험 성적이 0점부터 100점까지 있다면 이를 100등부터 1등까지 성적순으로 정렬한 뒤 1/4구간으로 나누는 것. 여기서 25%~75% 범위를 IQR이라고 한다. IQR에 보통은 1.5를 공해서 75%지점에 더해주고 25%지점에서 빼준 뒤 그 범위를 넘어가면 outlier로 간주!. 이를 시각화한 도표가 boxplot이다. . 먼저 어떤 feature의 이상치 데이터를 검출할 것인지 선택. | 매우 많은 feature가 있을 경우 상관성이 높은 feature들을 위주로 이상치를 검출하자. | . import seaborn as sns plt.figure(figsize=(9, 9)) corr = card_df.corr() sns.heatmap(corr, cmap=&#39;RdBu&#39;) . &lt;AxesSubplot:&gt; . 양의 상관관계가 높을수록 색이 진한 파란색. 음의 상관관계가 높을수록 색이 진한 빨간색. . | 이중 결정 레이블에 속하는 Class feature와 음의 상관관계가 가장 높은 feature는 V14와 V17, 이중 V14에서 outlier를 제거해보자 . | . import numpy as np def get_outlier(df=None, column=None, weight=1.5): # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함. fraud = df[df[&#39;Class&#39;]==1][column] quantile_25 = np.percentile(fraud.values, 25) quantile_75 = np.percentile(fraud.values, 75) # IQR을 구하고, IQR에 1.5를 곱하여 최대값과 최소값 지점 구함. iqr = quantile_75 - quantile_25 iqr_weight = iqr * weight lowest_val = quantile_25 - iqr_weight highest_val = quantile_75 + iqr_weight # 최대값 보다 크거나, 최소값 보다 작은 값을 아웃라이어로 설정하고 DataFrame index 반환. outlier_index = fraud[(fraud &lt; lowest_val) | (fraud &gt; highest_val)].index return outlier_index . outlier_index = get_outlier(df=card_df, column=&#39;V14&#39;, weight=1.5) print(&#39;이상치 데이터 인덱스:&#39;, outlier_index) . 이상치 데이터 인덱스: Int64Index([8296, 8615, 9035, 9252], dtype=&#39;int64&#39;) . 이를 삭제하는 로직을 추가 | . def get_preprocessed_df(df=None): df_copy = df.copy() amount_n = np.log1p(df_copy[&#39;Amount&#39;]) df_copy.insert(0, &#39;Amount_Scaled&#39;, amount_n) df_copy.drop([&#39;Time&#39;,&#39;Amount&#39;], axis=1, inplace=True) # 이상치 데이터 삭제하는 로직 추가 outlier_index = get_outlier(df=df_copy, column=&#39;V14&#39;, weight=1.5) df_copy.drop(outlier_index, axis=0, inplace=True) return df_copy X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) print(&#39;### 로지스틱 회귀 예측 성능 ###&#39;) get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) print(&#39;### LightGBM 예측 성능 ###&#39;) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . ### 로지스틱 회귀 예측 성능 ### 오차 행렬 [[85281 14] [ 48 98]] 정확도: 0.9993, 정밀도: 0.8750, 재현율: 0.6712, F1: 0.7597, AUC:0.9743 ### LightGBM 예측 성능 ### 오차 행렬 [[85290 5] [ 25 121]] 정확도: 0.9996, 정밀도: 0.9603, 재현율: 0.8288, F1: 0.8897, AUC:0.9780 . outlier 제거한 뒤 로지스틱 회귀와 LightGBM 모두 예측 성능이 크게 향상됐음을 알 수 있다. | . . SMOTE 기법으로 오버 샘플링을 적용한 뒤 로지스틱 회구와 LightGBM 모델의 예측 성능을 평가해보자 | SMOTE를 적용할 땐, 반드시 학습 데이터 세트만 오버 샘플링 해야함 | . from imblearn.over_sampling import SMOTE smote = SMOTE(random_state=0) X_train_over, y_train_over = smote.fit_sample(X_train, y_train) print(&#39;SMOTE 적용 전 학습용 피처/레이블 데이터 세트: &#39;, X_train.shape, y_train.shape) print(&#39;SMOTE 적용 후 학습용 피처/레이블 데이터 세트: &#39;, X_train_over.shape, y_train_over.shape) print(&#39;SMOTE 적용 후 레이블 값 분포: n&#39;, pd.Series(y_train_over).value_counts()) . SMOTE 적용 전 학습용 피처/레이블 데이터 세트: (199362, 29) (199362,) SMOTE 적용 후 학습용 피처/레이블 데이터 세트: (398040, 29) (398040,) SMOTE 적용 후 레이블 값 분포: 0 199020 1 199020 Name: Class, dtype: int64 . lr_clf = LogisticRegression() # ftr_train과 tgt_train 인자값이 SMOTE 증식된 X_train_over와 y_train_over로 변경됨에 유의 get_model_train_eval(lr_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test) . 오차 행렬 [[82937 2358] [ 11 135]] 정확도: 0.9723, 정밀도: 0.0542, 재현율: 0.9247, F1: 0.1023, AUC:0.9737 . 재현울은 크게 증가하나 정밀도가 급격히 저하 | 이 정도로 저조한 정밀도는 현실 업무에 적용할 수 없음 | . lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False) get_model_train_eval(lgbm_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test) . 오차 행렬 [[85283 12] [ 22 124]] 정확도: 0.9996, 정밀도: 0.9118, 재현율: 0.8493, F1: 0.8794, AUC:0.9814 . 재현율은 높아졌으나 정밀도는 좀 낮아짐. | SMOTE를 적용하면 재현율은 높아지나 정밀도는 낮아지는 것이 일반적임. | . &#49828;&#53468;&#53433; &#50521;&#49345;&#48660; . 스태킹은 개별적인 여러 알고리즘을 서로 결합해 예측 결과를 도출한다는 점에서 배깅,부스팅과 일맥상통. 허나 가장 큰 차이점은 개별 알고리즘으로 예측한 데이터를 기반으로 다시 마지막 특정 알고리즘으로 재예측시도함. 즉 개별 알고리즘의 예측 결과 데이터 세트를 최종적인 메타 데이터 세트로 만들어 별도의 ML 알고리즘으로 최종학습을 수행하고 테스트 데이터를 기반으로 다시 최종 예측을 수행하는 방식. 이렇게 개별 모델의 예측된 데이터 세트를 다시 기반으로 하여 학습하고 예측하는 방식을 메타 모델이라고 한다. 따라서 스태킹 모델은 두 종류의 모델 필요. 첫번째는 기반 모델, 두번째는 이 개별 기반 모델의 예측 데이터를 학습 데이터로 만들어서 학습하는 최종 메타 모델. 스태킹 모델의 핵심은 여러 개별 모델의 예측 데이터를 각각 스태킹 형태로 결합해, 즉 잘 쌓아서, 최종 메타 모델의 학습용 feature데이터 세트와 테스트용 feature 데이터 세트를 만드는 것이다. 이를 도식화 한 248p참고 . 기본 스태킹 모델을 위스콘신 암 데이터 세트에 적용해보자 | . import numpy as np from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score cancer_data = load_breast_cancer() X_data = cancer_data.data y_label = cancer_data.target X_train , X_test , y_train , y_test = train_test_split(X_data , y_label , test_size=0.2 , random_state=0) . 스태킹에 사용될 머신러닝 알고리즘 클래스를 생성하자. | 개별 모델은 KNN, 랜덤 포레스트, 결정 트리, 에이다 부스트이며 이들 모델의 예측 결과를 합한 데이터 세트로 학습,예측 수행하는 최종 모델은 로지스틱 회귀이다. | . knn_clf = KNeighborsClassifier(n_neighbors=4) rf_clf = RandomForestClassifier(n_estimators=100, random_state=0) dt_clf = DecisionTreeClassifier() ada_clf = AdaBoostClassifier(n_estimators=100) # 최종 Stacking 모델을 위한 Classifier생성. lr_final = LogisticRegression(C=10) . 개별 모델들을 학습시키자 | . knn_clf.fit(X_train, y_train) rf_clf.fit(X_train , y_train) dt_clf.fit(X_train , y_train) ada_clf.fit(X_train, y_train) . AdaBoostClassifier(n_estimators=100) . 개별 모델의 예측 데이터 세트를 반환하고 각 모델의 예측 정확도를 살펴보자 | . knn_pred = knn_clf.predict(X_test) rf_pred = rf_clf.predict(X_test) dt_pred = dt_clf.predict(X_test) ada_pred = ada_clf.predict(X_test) print(&#39;KNN 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, knn_pred))) print(&#39;랜덤 포레스트 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, rf_pred))) print(&#39;결정 트리 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, dt_pred))) print(&#39;에이다부스트 정확도: {0:.4f} :&#39;.format(accuracy_score(y_test, ada_pred))) . KNN 정확도: 0.9211 랜덤 포레스트 정확도: 0.9649 결정 트리 정확도: 0.9123 에이다부스트 정확도: 0.9561 : . 개별 알고리즘으로부터 예측된 예측값을 칼럼 레벨로 옆으로 붙여서 feature값으로 만들어, 최종 메타 모델인 로지스틱 회귀에서 학습 데이터로 다시 사용하자. | . pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred]) print(pred.shape) # transpose를 이용해 행과 열의 위치 교환. 컬럼 레벨로 각 알고리즘의 예측 결과를 피처로 만듦. pred = np.transpose(pred) print(pred.shape) . (4, 114) (114, 4) . lr_final.fit(pred, y_test) final = lr_final.predict(pred) print(&#39;최종 메타 모델의 예측 정확도: {0:.4f}&#39;.format(accuracy_score(y_test , final))) . 최종 메타 모델의 예측 정확도: 0.9737 . 개별 모델 정확도보다 스태킹으로 재구성해 최종 메타 모델에서 학습하고 예측한 결과가 더 높음. | . CV &#49464;&#53944; &#44592;&#48152;&#51032; &#49828;&#53468;&#53433; . 과적합을 개선하기 위해 최종 메타 모델을 위한 데이터 세트를 만들 때 교차 검증을 기반으로 예측된 결과 데이터 세트를 이용한다. 앞에서 마지막에 메타 모델인 로지스틱 회귀 모델을 기반으로 최종 학습할 때 레이블 데이터 세트로 학습데이터가 아닌 테스트용 레이블 데이터 세트를 기반으로 학습했기에 과적합 문제가 발생할 수 있다. CV세트 기반의 스태킹은 이에 대한 개선을 위해 개별 모델들이 각각 교차 검증으로 메타모델을 위한 학습용 스태킹 데이터 생성과 예측을 위한 테스트용 스태킹 데이터를 생성한 뒤 이를 기반으로 메타 모델이 학습과 예측을 수행한다. 자세한 건 284p참고하자 . from sklearn.model_selection import KFold from sklearn.metrics import mean_absolute_error # 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ): # 지정된 n_folds값으로 KFold 생성. kf = KFold(n_splits=n_folds, shuffle=False, random_state=0) #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 train_fold_pred = np.zeros((X_train_n.shape[0] ,1 )) test_pred = np.zeros((X_test_n.shape[0],n_folds)) print(model.__class__.__name__ , &#39; model 시작 &#39;) for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)): #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 print(&#39; t 폴드 세트: &#39;,folder_counter,&#39; 시작 &#39;) X_tr = X_train_n[train_index] y_tr = y_train_n[train_index] X_te = X_train_n[valid_index] #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행. model.fit(X_tr , y_tr) #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장. train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. test_pred[:, folder_counter] = model.predict(X_test_n) # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1) #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터 return train_fold_pred , test_pred_mean . knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7) rf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7) dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7) ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7) . KNeighborsClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 RandomForestClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 DecisionTreeClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 AdaBoostClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 . Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1) Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1) print(&#39;원본 학습 피처 데이터 Shape:&#39;,X_train.shape, &#39;원본 테스트 피처 Shape:&#39;,X_test.shape) print(&#39;스태킹 학습 피처 데이터 Shape:&#39;, Stack_final_X_train.shape, &#39;스태킹 테스트 피처 데이터 Shape:&#39;,Stack_final_X_test.shape) . 원본 학습 피처 데이터 Shape: (455, 30) 원본 테스트 피처 Shape: (114, 30) 스태킹 학습 피처 데이터 Shape: (455, 4) 스태킹 테스트 피처 데이터 Shape: (114, 4) . lr_final.fit(Stack_final_X_train, y_train) stack_final = lr_final.predict(Stack_final_X_test) print(&#39;최종 메타 모델의 예측 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, stack_final))) . 최종 메타 모델의 예측 정확도: 0.9737 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/09/intro.html",
            "relUrl": "/2022/01/09/intro.html",
            "date": " • Jan 9, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "2022/01/08/SAT",
            "content": "LightGBM . :XGBoost보다 학습에 걸리는 시간이 적다. 10,000건 이하의 적은 데이터 세트에 적용할 경우 과적합이 발생하기 쉽다. 리프 중심 트리(Leaf Wise) 분할 방식을 사용한다. 기존의 대부분 트리 기반 알고리즘은 트리의 깊이를 효과적으로 줄이기 위한 균형 트리 분할(Level wise) 방식을 사용. 즉 최대한 규형 잡힌 트리를 유지하면서 분할하기 때문에 트리의 깊이가 최소화될 수 있다. 하지만 LightGBM의 리프 중심 트리 분할 방식은 트리의 균형을 맞추지 않고 최대 손실 값(max delta loss)을 가지는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적인 규칙 트리가 생성된다. 하지만 이렇게 최대 손실값을 가지는 리프 노드를 지속적으로 분할해 생성된 규칙 트리는 학습을 반복할수록 결국은 균형 트리 방식보다 예측 오류 손실을 최소화할 수 있다는 것이 LightGBM의 구현 사상이다. Xgboost와 다르게 리프 노드가 계속 분할되면서 트리의 깊이가 깊어지므로 이러한 트리 특성에 맞는 하이퍼 파라미터 설정이 필요하다. (245p 균형 트리 분할과 리프 중심 트리 분할 도식화한 거 살펴보기) . learning_rate를 작게 하면서 n_estimators를 크게 하는 것이 부스팅 계열 튜닝에서 가장 기본적이 튜닝 방안이다. . LightGBM을 이용해 위스콘신 유방암 데이터 세트를 예측해보자. . from lightgbm import LGBMClassifier import pandas as pd import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split dataset = load_breast_cancer() ftr = dataset.data target = dataset.target # 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출 X_train, X_test, y_train, y_test=train_test_split(ftr, target, test_size=0.2, random_state=156 ) # 앞서 XGBoost와 동일하게 n_estimators는 400 설정. lgbm_wrapper = LGBMClassifier(n_estimators=400) # LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능. evals = [(X_test, y_test)] lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;logloss&quot;, eval_set=evals, verbose=True) preds = lgbm_wrapper.predict(X_test) pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1] . [1] valid_0&#39;s binary_logloss: 0.565079 [2] valid_0&#39;s binary_logloss: 0.507451 [3] valid_0&#39;s binary_logloss: 0.458489 [4] valid_0&#39;s binary_logloss: 0.417481 [5] valid_0&#39;s binary_logloss: 0.385507 [6] valid_0&#39;s binary_logloss: 0.355773 [7] valid_0&#39;s binary_logloss: 0.329587 [8] valid_0&#39;s binary_logloss: 0.308478 [9] valid_0&#39;s binary_logloss: 0.285395 [10] valid_0&#39;s binary_logloss: 0.267055 [11] valid_0&#39;s binary_logloss: 0.252013 [12] valid_0&#39;s binary_logloss: 0.237018 [13] valid_0&#39;s binary_logloss: 0.224756 [14] valid_0&#39;s binary_logloss: 0.213383 [15] valid_0&#39;s binary_logloss: 0.203058 [16] valid_0&#39;s binary_logloss: 0.194015 [17] valid_0&#39;s binary_logloss: 0.186412 [18] valid_0&#39;s binary_logloss: 0.179108 [19] valid_0&#39;s binary_logloss: 0.174004 [20] valid_0&#39;s binary_logloss: 0.167155 [21] valid_0&#39;s binary_logloss: 0.162494 [22] valid_0&#39;s binary_logloss: 0.156886 [23] valid_0&#39;s binary_logloss: 0.152855 [24] valid_0&#39;s binary_logloss: 0.151113 [25] valid_0&#39;s binary_logloss: 0.148395 [26] valid_0&#39;s binary_logloss: 0.145869 [27] valid_0&#39;s binary_logloss: 0.143036 [28] valid_0&#39;s binary_logloss: 0.14033 [29] valid_0&#39;s binary_logloss: 0.139609 [30] valid_0&#39;s binary_logloss: 0.136109 [31] valid_0&#39;s binary_logloss: 0.134867 [32] valid_0&#39;s binary_logloss: 0.134729 [33] valid_0&#39;s binary_logloss: 0.1311 [34] valid_0&#39;s binary_logloss: 0.131143 [35] valid_0&#39;s binary_logloss: 0.129435 [36] valid_0&#39;s binary_logloss: 0.128474 [37] valid_0&#39;s binary_logloss: 0.126683 [38] valid_0&#39;s binary_logloss: 0.126112 [39] valid_0&#39;s binary_logloss: 0.122831 [40] valid_0&#39;s binary_logloss: 0.123162 [41] valid_0&#39;s binary_logloss: 0.125592 [42] valid_0&#39;s binary_logloss: 0.128293 [43] valid_0&#39;s binary_logloss: 0.128123 [44] valid_0&#39;s binary_logloss: 0.12789 [45] valid_0&#39;s binary_logloss: 0.122818 [46] valid_0&#39;s binary_logloss: 0.12496 [47] valid_0&#39;s binary_logloss: 0.125578 [48] valid_0&#39;s binary_logloss: 0.127381 [49] valid_0&#39;s binary_logloss: 0.128349 [50] valid_0&#39;s binary_logloss: 0.127004 [51] valid_0&#39;s binary_logloss: 0.130288 [52] valid_0&#39;s binary_logloss: 0.131362 [53] valid_0&#39;s binary_logloss: 0.133363 [54] valid_0&#39;s binary_logloss: 0.1332 [55] valid_0&#39;s binary_logloss: 0.134543 [56] valid_0&#39;s binary_logloss: 0.130803 [57] valid_0&#39;s binary_logloss: 0.130306 [58] valid_0&#39;s binary_logloss: 0.132514 [59] valid_0&#39;s binary_logloss: 0.133278 [60] valid_0&#39;s binary_logloss: 0.134804 [61] valid_0&#39;s binary_logloss: 0.136888 [62] valid_0&#39;s binary_logloss: 0.138745 [63] valid_0&#39;s binary_logloss: 0.140497 [64] valid_0&#39;s binary_logloss: 0.141368 [65] valid_0&#39;s binary_logloss: 0.140764 [66] valid_0&#39;s binary_logloss: 0.14348 [67] valid_0&#39;s binary_logloss: 0.143418 [68] valid_0&#39;s binary_logloss: 0.143682 [69] valid_0&#39;s binary_logloss: 0.145076 [70] valid_0&#39;s binary_logloss: 0.14686 [71] valid_0&#39;s binary_logloss: 0.148051 [72] valid_0&#39;s binary_logloss: 0.147664 [73] valid_0&#39;s binary_logloss: 0.149478 [74] valid_0&#39;s binary_logloss: 0.14708 [75] valid_0&#39;s binary_logloss: 0.14545 [76] valid_0&#39;s binary_logloss: 0.148767 [77] valid_0&#39;s binary_logloss: 0.149959 [78] valid_0&#39;s binary_logloss: 0.146083 [79] valid_0&#39;s binary_logloss: 0.14638 [80] valid_0&#39;s binary_logloss: 0.148461 [81] valid_0&#39;s binary_logloss: 0.15091 [82] valid_0&#39;s binary_logloss: 0.153011 [83] valid_0&#39;s binary_logloss: 0.154807 [84] valid_0&#39;s binary_logloss: 0.156501 [85] valid_0&#39;s binary_logloss: 0.158586 [86] valid_0&#39;s binary_logloss: 0.159819 [87] valid_0&#39;s binary_logloss: 0.161745 [88] valid_0&#39;s binary_logloss: 0.162829 [89] valid_0&#39;s binary_logloss: 0.159142 [90] valid_0&#39;s binary_logloss: 0.156765 [91] valid_0&#39;s binary_logloss: 0.158625 [92] valid_0&#39;s binary_logloss: 0.156832 [93] valid_0&#39;s binary_logloss: 0.154616 [94] valid_0&#39;s binary_logloss: 0.154263 [95] valid_0&#39;s binary_logloss: 0.157156 [96] valid_0&#39;s binary_logloss: 0.158617 [97] valid_0&#39;s binary_logloss: 0.157495 [98] valid_0&#39;s binary_logloss: 0.159413 [99] valid_0&#39;s binary_logloss: 0.15847 [100] valid_0&#39;s binary_logloss: 0.160746 [101] valid_0&#39;s binary_logloss: 0.16217 [102] valid_0&#39;s binary_logloss: 0.165293 [103] valid_0&#39;s binary_logloss: 0.164749 [104] valid_0&#39;s binary_logloss: 0.167097 [105] valid_0&#39;s binary_logloss: 0.167697 [106] valid_0&#39;s binary_logloss: 0.169462 [107] valid_0&#39;s binary_logloss: 0.169947 [108] valid_0&#39;s binary_logloss: 0.171 [109] valid_0&#39;s binary_logloss: 0.16907 [110] valid_0&#39;s binary_logloss: 0.169521 [111] valid_0&#39;s binary_logloss: 0.167719 [112] valid_0&#39;s binary_logloss: 0.166648 [113] valid_0&#39;s binary_logloss: 0.169053 [114] valid_0&#39;s binary_logloss: 0.169613 [115] valid_0&#39;s binary_logloss: 0.170059 [116] valid_0&#39;s binary_logloss: 0.1723 [117] valid_0&#39;s binary_logloss: 0.174733 [118] valid_0&#39;s binary_logloss: 0.173526 [119] valid_0&#39;s binary_logloss: 0.1751 [120] valid_0&#39;s binary_logloss: 0.178254 [121] valid_0&#39;s binary_logloss: 0.182968 [122] valid_0&#39;s binary_logloss: 0.179017 [123] valid_0&#39;s binary_logloss: 0.178326 [124] valid_0&#39;s binary_logloss: 0.177149 [125] valid_0&#39;s binary_logloss: 0.179171 [126] valid_0&#39;s binary_logloss: 0.180948 [127] valid_0&#39;s binary_logloss: 0.183861 [128] valid_0&#39;s binary_logloss: 0.187579 [129] valid_0&#39;s binary_logloss: 0.188122 [130] valid_0&#39;s binary_logloss: 0.1857 [131] valid_0&#39;s binary_logloss: 0.187442 [132] valid_0&#39;s binary_logloss: 0.188578 [133] valid_0&#39;s binary_logloss: 0.189729 [134] valid_0&#39;s binary_logloss: 0.187313 [135] valid_0&#39;s binary_logloss: 0.189279 [136] valid_0&#39;s binary_logloss: 0.191068 [137] valid_0&#39;s binary_logloss: 0.192414 [138] valid_0&#39;s binary_logloss: 0.191255 [139] valid_0&#39;s binary_logloss: 0.193453 [140] valid_0&#39;s binary_logloss: 0.196969 [141] valid_0&#39;s binary_logloss: 0.196378 [142] valid_0&#39;s binary_logloss: 0.196367 [143] valid_0&#39;s binary_logloss: 0.19869 [144] valid_0&#39;s binary_logloss: 0.200352 [145] valid_0&#39;s binary_logloss: 0.19712 . C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:726: UserWarning: &#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;early_stopping()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:736: UserWarning: &#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;log_evaluation()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; . 조기 중단으로 145번 반복까지만 수행하고 학습을 종료. 이제 학습된 LightGBM 모델을 기반으로 예측 성능을 평가해보자 . from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.metrics import precision_score, recall_score from sklearn.metrics import f1_score, roc_auc_score # 수정된 get_clf_eval() 함수 def get_clf_eval(y_test, pred=None, pred_proba=None): confusion = confusion_matrix( y_test, pred) accuracy = accuracy_score(y_test , pred) precision = precision_score(y_test , pred) recall = recall_score(y_test , pred) f1 = f1_score(y_test,pred) # ROC-AUC 추가 roc_auc = roc_auc_score(y_test, pred_proba) print(&#39;오차 행렬&#39;) print(confusion) # ROC-AUC print 추가 print(&#39;정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}&#39;.format(accuracy, precision, recall, f1, roc_auc)) . get_clf_eval(y_test, preds, pred_proba) . 오차 행렬 [[33 4] [ 1 76]] 정확도: 0.9561, 정밀도: 0.9500, 재현율: 0.9870, F1: 0.9682, AUC:0.9905 . from lightgbm import plot_importance import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=(10, 12)) # 사이킷런 래퍼 클래스를 입력해도 무방. plot_importance(lgbm_wrapper, ax=ax) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}, xlabel=&#39;Feature importance&#39;, ylabel=&#39;Features&#39;&gt; . 이번에는 캐글의 산탄데르 고객 만족 데이터 세트에 대해 고객 만족 여부를 XGBoost와 LightGBM을 활용해 예측해보자 . 데이터 전처리 먼저! . import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib cust_df = pd.read_csv(&quot;./train_santander.csv&quot;,encoding=&#39;latin-1&#39;) print(&#39;dataset shape:&#39;, cust_df.shape) cust_df.head(3) . dataset shape: (76020, 371) . ID var3 var15 imp_ent_var16_ult1 imp_op_var39_comer_ult1 imp_op_var39_comer_ult3 imp_op_var40_comer_ult1 imp_op_var40_comer_ult3 imp_op_var40_efect_ult1 imp_op_var40_efect_ult3 ... saldo_medio_var33_hace2 saldo_medio_var33_hace3 saldo_medio_var33_ult1 saldo_medio_var33_ult3 saldo_medio_var44_hace2 saldo_medio_var44_hace3 saldo_medio_var44_ult1 saldo_medio_var44_ult3 var38 TARGET . 0 1 | 2 | 23 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 39205.17 | 0 | . 1 3 | 2 | 34 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 49278.03 | 0 | . 2 4 | 2 | 23 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 67333.77 | 0 | . 3 rows × 371 columns . 클래스 값 포함한 feature가 371개 | . cust_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 76020 entries, 0 to 76019 Columns: 371 entries, ID to TARGET dtypes: float64(111), int64(260) memory usage: 215.2 MB . null값 없음 $ to$ 그러나 null값이 없는 대신 극단적인 숫자로 대체해놨을수도 있음 . | 전체 데이터에서 만족과 불만족의 비율을 살펴보자 $ to$ 레이블인 Target 속성 값의 분포를 알아보자 . | . print(cust_df[&#39;TARGET&#39;].value_counts()) unsatisfied_cnt = cust_df[cust_df[&#39;TARGET&#39;] == 1][&#39;TARGET&#39;].count() total_cnt = cust_df[&#39;TARGET&#39;].count() print(&#39;unsatisfied 비율은 {0:.2f}&#39;.format((unsatisfied_cnt / total_cnt))) . 0 73012 1 3008 Name: TARGET, dtype: int64 unsatisfied 비율은 0.04 . 불만족 비율은 4%에 불과 | . cust_df.describe() . ID var3 var15 imp_ent_var16_ult1 imp_op_var39_comer_ult1 imp_op_var39_comer_ult3 imp_op_var40_comer_ult1 imp_op_var40_comer_ult3 imp_op_var40_efect_ult1 imp_op_var40_efect_ult3 ... saldo_medio_var33_hace2 saldo_medio_var33_hace3 saldo_medio_var33_ult1 saldo_medio_var33_ult3 saldo_medio_var44_hace2 saldo_medio_var44_hace3 saldo_medio_var44_ult1 saldo_medio_var44_ult3 var38 TARGET . count 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | ... | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 7.602000e+04 | 76020.000000 | . mean 75964.050723 | -1523.199277 | 33.212865 | 86.208265 | 72.363067 | 119.529632 | 3.559130 | 6.472698 | 0.412946 | 0.567352 | ... | 7.935824 | 1.365146 | 12.215580 | 8.784074 | 31.505324 | 1.858575 | 76.026165 | 56.614351 | 1.172358e+05 | 0.039569 | . std 43781.947379 | 39033.462364 | 12.956486 | 1614.757313 | 339.315831 | 546.266294 | 93.155749 | 153.737066 | 30.604864 | 36.513513 | ... | 455.887218 | 113.959637 | 783.207399 | 538.439211 | 2013.125393 | 147.786584 | 4040.337842 | 2852.579397 | 1.826646e+05 | 0.194945 | . min 1.000000 | -999999.000000 | 5.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 5.163750e+03 | 0.000000 | . 25% 38104.750000 | 2.000000 | 23.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 6.787061e+04 | 0.000000 | . 50% 76043.000000 | 2.000000 | 28.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.064092e+05 | 0.000000 | . 75% 113748.750000 | 2.000000 | 40.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.187563e+05 | 0.000000 | . max 151838.000000 | 238.000000 | 105.000000 | 210000.000000 | 12888.030000 | 21024.810000 | 8237.820000 | 11073.570000 | 6600.000000 | 6600.000000 | ... | 50003.880000 | 20385.720000 | 138831.630000 | 91778.730000 | 438329.220000 | 24650.010000 | 681462.900000 | 397884.300000 | 2.203474e+07 | 1.000000 | . 8 rows × 371 columns . var3 칼럼의 경우 min값이 -999999이다. NaN값이나 특정 예외 값을 -999999로 변환한 것으로 보임. | . print(cust_df.var3.value_counts()[:10]) . 2 74165 8 138 -999999 116 9 110 3 108 1 105 13 98 7 97 4 86 12 85 Name: var3, dtype: int64 . -999999값이 116개나 있음을 알 수 있다. . | var3은 숫자 형이고 다른 값에 비해 -999999은 편차가 너무 심하므로 -999999을 값이 가장 많은 2로 변환하자. . | ID feature는 단순 식별자에 불과하므로 feature를 drop하자. . | 그리고 클래스 데이터 세트와 faeture 데이터 세트를 분리해 별도의 데이터 세트로 별도로 저장하자 . | . cust_df[&#39;var3&#39;].replace(-999999, 2, inplace=True) cust_df.drop(&#39;ID&#39;,axis=1 , inplace=True) # 피처 세트와 레이블 세트분리. 레이블 컬럼은 DataFrame의 맨 마지막에 위치해 컬럼 위치 -1로 분리 X_features = cust_df.iloc[:, :-1] y_labels = cust_df.iloc[:, -1] print(&#39;피처 데이터 shape:{0}&#39;.format(X_features.shape)) . 피처 데이터 shape:(76020, 369) . 학습과 성능 평가를 위해서 원본 데이터 세트에서 학습 데이터 세트와 테스트 데이터 세트를 분리하자 . | 비대칭한 데이터 세트이므로 클래스인 Target 값 분포도가 학습 데이터와 테스트 데이터 세트에 모두 비슷하게 추출됐는지 확인하자. . | . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=0) train_cnt = y_train.count() test_cnt = y_test.count() print(&#39; n학습 세트 Shape:{0}, 테스트 세트 Shape:{1}&#39;.format(X_train.shape , X_test.shape)) print(&#39; n학습 세트 레이블 값 분포 비율&#39;) print(y_train.value_counts()/train_cnt) print(&#39; n 테스트 세트 레이블 값 분포 비율&#39;) print(y_test.value_counts()/test_cnt,&#39; n&#39;) . 학습 세트 Shape:(60816, 369), 테스트 세트 Shape:(15204, 369) 학습 세트 레이블 값 분포 비율 0 0.960964 1 0.039036 Name: TARGET, dtype: float64 테스트 세트 레이블 값 분포 비율 0 0.9583 1 0.0417 Name: TARGET, dtype: float64 . 학습과 테스트 데이터 세트 모두 Target의 값의 분포가 원본 데이터와 유사. | . XGBoost의 학습 모델을 생성하고 예측 결과를 ROC AUC로 평가해보자 | . from xgboost import XGBClassifier from sklearn.metrics import roc_auc_score # n_estimators는 500으로, random state는 예제 수행 시마다 동일 예측 결과를 위해 설정. xgb_clf = XGBClassifier(n_estimators=500, random_state=156) # 성능 평가 지표를 auc로, 조기 중단 파라미터는 100으로 설정하고 학습 수행. xgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;auc&quot;, eval_set=[(X_train, y_train), (X_test, y_test)]) xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(xgb_roc_score)) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [0] validation_0-auc:0.82005 validation_1-auc:0.81157 [1] validation_0-auc:0.83400 validation_1-auc:0.82452 [2] validation_0-auc:0.83870 validation_1-auc:0.82746 [3] validation_0-auc:0.84419 validation_1-auc:0.82922 [4] validation_0-auc:0.84783 validation_1-auc:0.83298 [5] validation_0-auc:0.85125 validation_1-auc:0.83500 [6] validation_0-auc:0.85501 validation_1-auc:0.83653 [7] validation_0-auc:0.85831 validation_1-auc:0.83782 [8] validation_0-auc:0.86143 validation_1-auc:0.83802 [9] validation_0-auc:0.86452 validation_1-auc:0.83914 [10] validation_0-auc:0.86717 validation_1-auc:0.83954 [11] validation_0-auc:0.87013 validation_1-auc:0.83983 [12] validation_0-auc:0.87369 validation_1-auc:0.84033 [13] validation_0-auc:0.87620 validation_1-auc:0.84054 [14] validation_0-auc:0.87799 validation_1-auc:0.84135 [15] validation_0-auc:0.88072 validation_1-auc:0.84117 [16] validation_0-auc:0.88238 validation_1-auc:0.84101 [17] validation_0-auc:0.88354 validation_1-auc:0.84071 [18] validation_0-auc:0.88458 validation_1-auc:0.84052 [19] validation_0-auc:0.88592 validation_1-auc:0.84023 [20] validation_0-auc:0.88790 validation_1-auc:0.84012 [21] validation_0-auc:0.88846 validation_1-auc:0.84022 [22] validation_0-auc:0.88980 validation_1-auc:0.84007 [23] validation_0-auc:0.89019 validation_1-auc:0.84009 [24] validation_0-auc:0.89195 validation_1-auc:0.83974 [25] validation_0-auc:0.89255 validation_1-auc:0.84015 [26] validation_0-auc:0.89332 validation_1-auc:0.84101 [27] validation_0-auc:0.89389 validation_1-auc:0.84088 [28] validation_0-auc:0.89420 validation_1-auc:0.84074 [29] validation_0-auc:0.89665 validation_1-auc:0.83999 [30] validation_0-auc:0.89741 validation_1-auc:0.83959 [31] validation_0-auc:0.89916 validation_1-auc:0.83952 [32] validation_0-auc:0.90106 validation_1-auc:0.83901 [33] validation_0-auc:0.90253 validation_1-auc:0.83885 [34] validation_0-auc:0.90278 validation_1-auc:0.83887 [35] validation_0-auc:0.90293 validation_1-auc:0.83864 [36] validation_0-auc:0.90463 validation_1-auc:0.83834 [37] validation_0-auc:0.90500 validation_1-auc:0.83810 [38] validation_0-auc:0.90519 validation_1-auc:0.83810 [39] validation_0-auc:0.90533 validation_1-auc:0.83813 [40] validation_0-auc:0.90575 validation_1-auc:0.83776 [41] validation_0-auc:0.90691 validation_1-auc:0.83720 [42] validation_0-auc:0.90716 validation_1-auc:0.83684 [43] validation_0-auc:0.90737 validation_1-auc:0.83672 [44] validation_0-auc:0.90759 validation_1-auc:0.83674 [45] validation_0-auc:0.90769 validation_1-auc:0.83693 [46] validation_0-auc:0.90779 validation_1-auc:0.83686 [47] validation_0-auc:0.90793 validation_1-auc:0.83678 [48] validation_0-auc:0.90831 validation_1-auc:0.83694 [49] validation_0-auc:0.90871 validation_1-auc:0.83676 [50] validation_0-auc:0.90892 validation_1-auc:0.83655 [51] validation_0-auc:0.91070 validation_1-auc:0.83669 [52] validation_0-auc:0.91240 validation_1-auc:0.83641 [53] validation_0-auc:0.91354 validation_1-auc:0.83690 [54] validation_0-auc:0.91389 validation_1-auc:0.83693 [55] validation_0-auc:0.91408 validation_1-auc:0.83681 [56] validation_0-auc:0.91548 validation_1-auc:0.83680 [57] validation_0-auc:0.91560 validation_1-auc:0.83667 [58] validation_0-auc:0.91631 validation_1-auc:0.83664 [59] validation_0-auc:0.91729 validation_1-auc:0.83591 [60] validation_0-auc:0.91765 validation_1-auc:0.83576 [61] validation_0-auc:0.91788 validation_1-auc:0.83534 [62] validation_0-auc:0.91876 validation_1-auc:0.83513 [63] validation_0-auc:0.91896 validation_1-auc:0.83510 [64] validation_0-auc:0.91900 validation_1-auc:0.83508 [65] validation_0-auc:0.91911 validation_1-auc:0.83518 [66] validation_0-auc:0.91975 validation_1-auc:0.83510 [67] validation_0-auc:0.91986 validation_1-auc:0.83523 [68] validation_0-auc:0.92012 validation_1-auc:0.83457 [69] validation_0-auc:0.92019 validation_1-auc:0.83460 [70] validation_0-auc:0.92029 validation_1-auc:0.83446 [71] validation_0-auc:0.92041 validation_1-auc:0.83462 [72] validation_0-auc:0.92093 validation_1-auc:0.83394 [73] validation_0-auc:0.92099 validation_1-auc:0.83410 [74] validation_0-auc:0.92140 validation_1-auc:0.83394 [75] validation_0-auc:0.92148 validation_1-auc:0.83368 [76] validation_0-auc:0.92330 validation_1-auc:0.83413 [77] validation_0-auc:0.92424 validation_1-auc:0.83359 [78] validation_0-auc:0.92512 validation_1-auc:0.83353 [79] validation_0-auc:0.92549 validation_1-auc:0.83293 [80] validation_0-auc:0.92586 validation_1-auc:0.83253 [81] validation_0-auc:0.92686 validation_1-auc:0.83187 [82] validation_0-auc:0.92714 validation_1-auc:0.83230 [83] validation_0-auc:0.92810 validation_1-auc:0.83216 [84] validation_0-auc:0.92832 validation_1-auc:0.83206 [85] validation_0-auc:0.92878 validation_1-auc:0.83196 [86] validation_0-auc:0.92883 validation_1-auc:0.83200 [87] validation_0-auc:0.92890 validation_1-auc:0.83208 [88] validation_0-auc:0.92928 validation_1-auc:0.83174 [89] validation_0-auc:0.92950 validation_1-auc:0.83160 [90] validation_0-auc:0.92958 validation_1-auc:0.83155 [91] validation_0-auc:0.92969 validation_1-auc:0.83165 [92] validation_0-auc:0.92974 validation_1-auc:0.83172 [93] validation_0-auc:0.93042 validation_1-auc:0.83160 [94] validation_0-auc:0.93043 validation_1-auc:0.83150 [95] validation_0-auc:0.93048 validation_1-auc:0.83132 [96] validation_0-auc:0.93094 validation_1-auc:0.83090 [97] validation_0-auc:0.93102 validation_1-auc:0.83091 [98] validation_0-auc:0.93179 validation_1-auc:0.83066 [99] validation_0-auc:0.93255 validation_1-auc:0.83058 [100] validation_0-auc:0.93296 validation_1-auc:0.83029 [101] validation_0-auc:0.93370 validation_1-auc:0.82955 [102] validation_0-auc:0.93369 validation_1-auc:0.82962 [103] validation_0-auc:0.93448 validation_1-auc:0.82893 [104] validation_0-auc:0.93460 validation_1-auc:0.82837 [105] validation_0-auc:0.93494 validation_1-auc:0.82815 [106] validation_0-auc:0.93594 validation_1-auc:0.82744 [107] validation_0-auc:0.93598 validation_1-auc:0.82728 [108] validation_0-auc:0.93625 validation_1-auc:0.82651 [109] validation_0-auc:0.93632 validation_1-auc:0.82650 [110] validation_0-auc:0.93673 validation_1-auc:0.82621 [111] validation_0-auc:0.93678 validation_1-auc:0.82620 [112] validation_0-auc:0.93726 validation_1-auc:0.82591 [113] validation_0-auc:0.93797 validation_1-auc:0.82498 ROC AUC: 0.8413 . from sklearn.model_selection import GridSearchCV # 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소 xgb_clf = XGBClassifier(n_estimators=100) params = {&#39;max_depth&#39;:[5, 7] , &#39;min_child_weight&#39;:[1,3] ,&#39;colsample_bytree&#39;:[0.5, 0.75] } # 하이퍼 파라미터 테스트의 수행속도를 향상 시키기 위해 cv 를 지정하지 않음. gridcv = GridSearchCV(xgb_clf, param_grid=params) gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=&quot;auc&quot;, eval_set=[(X_train, y_train), (X_test, y_test)]) print(&#39;GridSearchCV 최적 파라미터:&#39;,gridcv.best_params_) xgb_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:,1], average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(xgb_roc_score)) . 이전 예제의 ROC AUC보다 파라미터 조정후 성능이 좀 개선 됐음, 앞에서 구한 최적화 파라미터를 기반으로 다른 하이퍼 파라미터를 변경 또는 추가해 다시 최적화를 진행해보자 | . # n_estimators는 1000으로 증가시키고, learning_rate=0.02로 감소, reg_alpha=0.03으로 추가함. xgb_clf = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.02, max_depth=7, min_child_weight=1, colsample_bytree=0.75, reg_alpha=0.03) # evaluation metric을 auc로, early stopping은 200 으로 설정하고 학습 수행. xgb_clf.fit(X_train, y_train, early_stopping_rounds=200, eval_metric=&quot;auc&quot;,eval_set=[(X_train, y_train), (X_test, y_test)]) xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(xgb_roc_score)) . 이전 테스트보다 살짝 향상된 결과를 나타내고 있음. XGBoost가 GBM보다는 빠르지만 아무래도 GBM을 기반으로 하고 있기 때문에 수행시간이 상당히 요구된다. 이 때문에 하이퍼 파라미터를 다양하게 나열해 파라미터를 튜닝하는 것은 많은 시간이 소모. 앙상블 계열 알고리즘에서 하이퍼 파라미터 튜닝으로 성능 수치 개선이 급격하게 되긴 어렵다. . | 튜닝된 모델에서 각 feature의 중요도를 feature 중요도 그래프로 나타내보자 . | . from xgboost import plot_importance import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(1,1,figsize=(10,8)) plot_importance(xgb_clf, ax=ax , max_num_features=20,height=0.4) . LightGBM으로 학습을 수행하고 ROC-AUC를 측정해보자 | . from lightgbm import LGBMClassifier lgbm_clf = LGBMClassifier(n_estimators=500) evals = [(X_test, y_test)] lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;auc&quot;, eval_set=evals, verbose=True) lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(lgbm_roc_score)) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:726: UserWarning: &#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;early_stopping()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:736: UserWarning: &#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;log_evaluation()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; . [1] valid_0&#39;s auc: 0.817384 valid_0&#39;s binary_logloss: 0.165046 [2] valid_0&#39;s auc: 0.818903 valid_0&#39;s binary_logloss: 0.160006 [3] valid_0&#39;s auc: 0.827707 valid_0&#39;s binary_logloss: 0.156323 [4] valid_0&#39;s auc: 0.832155 valid_0&#39;s binary_logloss: 0.153463 [5] valid_0&#39;s auc: 0.834677 valid_0&#39;s binary_logloss: 0.151256 [6] valid_0&#39;s auc: 0.834093 valid_0&#39;s binary_logloss: 0.149427 [7] valid_0&#39;s auc: 0.837046 valid_0&#39;s binary_logloss: 0.147961 [8] valid_0&#39;s auc: 0.837838 valid_0&#39;s binary_logloss: 0.146591 [9] valid_0&#39;s auc: 0.839435 valid_0&#39;s binary_logloss: 0.145455 [10] valid_0&#39;s auc: 0.83973 valid_0&#39;s binary_logloss: 0.144486 [11] valid_0&#39;s auc: 0.839799 valid_0&#39;s binary_logloss: 0.143769 [12] valid_0&#39;s auc: 0.840034 valid_0&#39;s binary_logloss: 0.143146 [13] valid_0&#39;s auc: 0.840271 valid_0&#39;s binary_logloss: 0.142533 [14] valid_0&#39;s auc: 0.840342 valid_0&#39;s binary_logloss: 0.142036 [15] valid_0&#39;s auc: 0.840928 valid_0&#39;s binary_logloss: 0.14161 [16] valid_0&#39;s auc: 0.840337 valid_0&#39;s binary_logloss: 0.141307 [17] valid_0&#39;s auc: 0.839901 valid_0&#39;s binary_logloss: 0.141152 [18] valid_0&#39;s auc: 0.839742 valid_0&#39;s binary_logloss: 0.141018 [19] valid_0&#39;s auc: 0.839818 valid_0&#39;s binary_logloss: 0.14068 [20] valid_0&#39;s auc: 0.839307 valid_0&#39;s binary_logloss: 0.140562 [21] valid_0&#39;s auc: 0.839662 valid_0&#39;s binary_logloss: 0.140353 [22] valid_0&#39;s auc: 0.840411 valid_0&#39;s binary_logloss: 0.140144 [23] valid_0&#39;s auc: 0.840522 valid_0&#39;s binary_logloss: 0.139983 [24] valid_0&#39;s auc: 0.840208 valid_0&#39;s binary_logloss: 0.139943 [25] valid_0&#39;s auc: 0.839578 valid_0&#39;s binary_logloss: 0.139898 [26] valid_0&#39;s auc: 0.83975 valid_0&#39;s binary_logloss: 0.139814 [27] valid_0&#39;s auc: 0.83988 valid_0&#39;s binary_logloss: 0.139711 [28] valid_0&#39;s auc: 0.839704 valid_0&#39;s binary_logloss: 0.139681 [29] valid_0&#39;s auc: 0.839432 valid_0&#39;s binary_logloss: 0.139662 [30] valid_0&#39;s auc: 0.839196 valid_0&#39;s binary_logloss: 0.139641 [31] valid_0&#39;s auc: 0.838891 valid_0&#39;s binary_logloss: 0.139654 [32] valid_0&#39;s auc: 0.838943 valid_0&#39;s binary_logloss: 0.1396 [33] valid_0&#39;s auc: 0.838632 valid_0&#39;s binary_logloss: 0.139642 [34] valid_0&#39;s auc: 0.838314 valid_0&#39;s binary_logloss: 0.139687 [35] valid_0&#39;s auc: 0.83844 valid_0&#39;s binary_logloss: 0.139668 [36] valid_0&#39;s auc: 0.839074 valid_0&#39;s binary_logloss: 0.139562 [37] valid_0&#39;s auc: 0.838806 valid_0&#39;s binary_logloss: 0.139594 [38] valid_0&#39;s auc: 0.839041 valid_0&#39;s binary_logloss: 0.139574 [39] valid_0&#39;s auc: 0.839081 valid_0&#39;s binary_logloss: 0.139587 [40] valid_0&#39;s auc: 0.839276 valid_0&#39;s binary_logloss: 0.139504 [41] valid_0&#39;s auc: 0.83951 valid_0&#39;s binary_logloss: 0.139481 [42] valid_0&#39;s auc: 0.839544 valid_0&#39;s binary_logloss: 0.139487 [43] valid_0&#39;s auc: 0.839673 valid_0&#39;s binary_logloss: 0.139478 [44] valid_0&#39;s auc: 0.839677 valid_0&#39;s binary_logloss: 0.139453 [45] valid_0&#39;s auc: 0.839703 valid_0&#39;s binary_logloss: 0.139445 [46] valid_0&#39;s auc: 0.839601 valid_0&#39;s binary_logloss: 0.139468 [47] valid_0&#39;s auc: 0.839318 valid_0&#39;s binary_logloss: 0.139529 [48] valid_0&#39;s auc: 0.839462 valid_0&#39;s binary_logloss: 0.139486 [49] valid_0&#39;s auc: 0.839288 valid_0&#39;s binary_logloss: 0.139492 [50] valid_0&#39;s auc: 0.838987 valid_0&#39;s binary_logloss: 0.139572 [51] valid_0&#39;s auc: 0.838845 valid_0&#39;s binary_logloss: 0.139603 [52] valid_0&#39;s auc: 0.838655 valid_0&#39;s binary_logloss: 0.139623 [53] valid_0&#39;s auc: 0.838783 valid_0&#39;s binary_logloss: 0.139609 [54] valid_0&#39;s auc: 0.838695 valid_0&#39;s binary_logloss: 0.139638 [55] valid_0&#39;s auc: 0.838868 valid_0&#39;s binary_logloss: 0.139625 [56] valid_0&#39;s auc: 0.838653 valid_0&#39;s binary_logloss: 0.139645 [57] valid_0&#39;s auc: 0.83856 valid_0&#39;s binary_logloss: 0.139688 [58] valid_0&#39;s auc: 0.838475 valid_0&#39;s binary_logloss: 0.139694 [59] valid_0&#39;s auc: 0.8384 valid_0&#39;s binary_logloss: 0.139682 [60] valid_0&#39;s auc: 0.838319 valid_0&#39;s binary_logloss: 0.13969 [61] valid_0&#39;s auc: 0.838209 valid_0&#39;s binary_logloss: 0.13973 [62] valid_0&#39;s auc: 0.83806 valid_0&#39;s binary_logloss: 0.139765 [63] valid_0&#39;s auc: 0.838096 valid_0&#39;s binary_logloss: 0.139749 [64] valid_0&#39;s auc: 0.838163 valid_0&#39;s binary_logloss: 0.139746 [65] valid_0&#39;s auc: 0.838183 valid_0&#39;s binary_logloss: 0.139805 [66] valid_0&#39;s auc: 0.838215 valid_0&#39;s binary_logloss: 0.139815 [67] valid_0&#39;s auc: 0.838268 valid_0&#39;s binary_logloss: 0.139822 [68] valid_0&#39;s auc: 0.83836 valid_0&#39;s binary_logloss: 0.139816 [69] valid_0&#39;s auc: 0.838114 valid_0&#39;s binary_logloss: 0.139874 [70] valid_0&#39;s auc: 0.83832 valid_0&#39;s binary_logloss: 0.139816 [71] valid_0&#39;s auc: 0.838256 valid_0&#39;s binary_logloss: 0.139818 [72] valid_0&#39;s auc: 0.838231 valid_0&#39;s binary_logloss: 0.139845 [73] valid_0&#39;s auc: 0.838028 valid_0&#39;s binary_logloss: 0.139888 [74] valid_0&#39;s auc: 0.837912 valid_0&#39;s binary_logloss: 0.139905 [75] valid_0&#39;s auc: 0.83772 valid_0&#39;s binary_logloss: 0.13992 [76] valid_0&#39;s auc: 0.837606 valid_0&#39;s binary_logloss: 0.139899 [77] valid_0&#39;s auc: 0.837521 valid_0&#39;s binary_logloss: 0.139925 [78] valid_0&#39;s auc: 0.837462 valid_0&#39;s binary_logloss: 0.139957 [79] valid_0&#39;s auc: 0.837541 valid_0&#39;s binary_logloss: 0.139944 [80] valid_0&#39;s auc: 0.838013 valid_0&#39;s binary_logloss: 0.13983 [81] valid_0&#39;s auc: 0.83789 valid_0&#39;s binary_logloss: 0.139874 [82] valid_0&#39;s auc: 0.837671 valid_0&#39;s binary_logloss: 0.139975 [83] valid_0&#39;s auc: 0.837707 valid_0&#39;s binary_logloss: 0.139972 [84] valid_0&#39;s auc: 0.837631 valid_0&#39;s binary_logloss: 0.140011 [85] valid_0&#39;s auc: 0.837496 valid_0&#39;s binary_logloss: 0.140023 [86] valid_0&#39;s auc: 0.83757 valid_0&#39;s binary_logloss: 0.140021 [87] valid_0&#39;s auc: 0.837284 valid_0&#39;s binary_logloss: 0.140099 [88] valid_0&#39;s auc: 0.837228 valid_0&#39;s binary_logloss: 0.140115 [89] valid_0&#39;s auc: 0.836964 valid_0&#39;s binary_logloss: 0.140172 [90] valid_0&#39;s auc: 0.836752 valid_0&#39;s binary_logloss: 0.140225 [91] valid_0&#39;s auc: 0.836833 valid_0&#39;s binary_logloss: 0.140221 [92] valid_0&#39;s auc: 0.836648 valid_0&#39;s binary_logloss: 0.140277 [93] valid_0&#39;s auc: 0.836648 valid_0&#39;s binary_logloss: 0.140315 [94] valid_0&#39;s auc: 0.836677 valid_0&#39;s binary_logloss: 0.140321 [95] valid_0&#39;s auc: 0.836729 valid_0&#39;s binary_logloss: 0.140307 [96] valid_0&#39;s auc: 0.8368 valid_0&#39;s binary_logloss: 0.140313 [97] valid_0&#39;s auc: 0.836797 valid_0&#39;s binary_logloss: 0.140331 [98] valid_0&#39;s auc: 0.836675 valid_0&#39;s binary_logloss: 0.140361 [99] valid_0&#39;s auc: 0.83655 valid_0&#39;s binary_logloss: 0.14039 [100] valid_0&#39;s auc: 0.836518 valid_0&#39;s binary_logloss: 0.1404 [101] valid_0&#39;s auc: 0.836998 valid_0&#39;s binary_logloss: 0.140294 [102] valid_0&#39;s auc: 0.836778 valid_0&#39;s binary_logloss: 0.140366 [103] valid_0&#39;s auc: 0.83694 valid_0&#39;s binary_logloss: 0.140333 [104] valid_0&#39;s auc: 0.836749 valid_0&#39;s binary_logloss: 0.14039 [105] valid_0&#39;s auc: 0.836752 valid_0&#39;s binary_logloss: 0.140391 [106] valid_0&#39;s auc: 0.837197 valid_0&#39;s binary_logloss: 0.140305 [107] valid_0&#39;s auc: 0.837141 valid_0&#39;s binary_logloss: 0.140329 [108] valid_0&#39;s auc: 0.8371 valid_0&#39;s binary_logloss: 0.140344 [109] valid_0&#39;s auc: 0.837136 valid_0&#39;s binary_logloss: 0.14033 [110] valid_0&#39;s auc: 0.837102 valid_0&#39;s binary_logloss: 0.140388 [111] valid_0&#39;s auc: 0.836957 valid_0&#39;s binary_logloss: 0.140426 [112] valid_0&#39;s auc: 0.836779 valid_0&#39;s binary_logloss: 0.14051 [113] valid_0&#39;s auc: 0.836831 valid_0&#39;s binary_logloss: 0.140526 [114] valid_0&#39;s auc: 0.836783 valid_0&#39;s binary_logloss: 0.14055 [115] valid_0&#39;s auc: 0.836672 valid_0&#39;s binary_logloss: 0.140585 ROC AUC: 0.8409 . 수행시간이 더 단축됐음을 알 수 있다. . | GridSearchCV로 좀 더 다양한 하이퍼 파라미터에 대한 튜닝을 수행해보자 . | . from sklearn.model_selection import GridSearchCV # 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소 LGBM_clf = LGBMClassifier(n_estimators=200) params = {&#39;num_leaves&#39;: [32, 64 ], &#39;max_depth&#39;:[128, 160], &#39;min_child_samples&#39;:[60, 100], &#39;subsample&#39;:[0.8, 1]} # 하이퍼 파라미터 테스트의 수행속도를 향상 시키기 위해 cv 를 지정하지 않습니다. gridcv = GridSearchCV(lgbm_clf, param_grid=params) gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=&quot;auc&quot;, eval_set=[(X_train, y_train), (X_test, y_test)]) print(&#39;GridSearchCV 최적 파라미터:&#39;, gridcv.best_params_) lgbm_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:,1], average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(lgbm_roc_score)) . 해당 하이퍼 파라미터를 LightGBM에 적용하고 다시 학습해 ROC-AUC 측정 결과를 도출해보자 | . lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=32, sumbsample=0.8, min_child_samples=100, max_depth=128) evals = [(X_test, y_test)] lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;auc&quot;, eval_set=evals, verbose=True) lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(lgbm_roc_score)) . 캐글 신용카드 사기 검출 . 해당 데이터 세트의 레이블인 class 속성은 매우 불균형한 분포를 가지고 있다. 단 0.1%만이 사기 transaction이며 일반적으로 사기 검출(Fraud Detection)이나 이상 검출(Anomaly Detection)과 같은 데이터 세트는 극도로 불균형한 분포를 가질 수밖에 없음. 레이블이 불균형한 분포를 가진 데이터 세트를 학습시킬 때 예측 성능의 문제가 발생할 수 있는데 이는 이상 레이블을 가지는 데이터 건수가 정상 레이블을 가진 데이터 건수에 비해 너무 적기 때문에 발생. 즉 이상 레이블을 가지는 데이터 건수는 매우 적기 때문에 제대로 다양한 유형을 학습하지 못하는 반면 정상 레이블을 가지는 데이터 건수는 매우 많기 때문에 일방적으로 정상 레이블로 치우친 학습을 수행해 제대로 된 이상 데이터 검출이 어려워지기 쉽다. 지도 학습에서 극도로 불균형한 레이블 값 분포로 인한 문제점을 해결하기 위해서는 적절한 학습 데이터를 확보하는 방안이 필요한데 대표적으로 Oversampling과 Undersampling 방법이 있으며 Oversampling 방식이 예측성능상 더 유리한 경우가 많다. . Undersampling : 많은 데이터 세트를 적은 데이터 세트 수준으로 감소시키는 방식. 10,000 : 100 비율이라면 정상 데이터 건수를 100으로 감소시키는 것. 하지만 정상 레이블의 경우 오히려 제대로 된 학습을 수행할 수 없다는 단점이 있어 잘 적용 X . | Oversampling : 이상 데이터와 같이 적은 데이터 세트를 증식하여 학습을 위한 충분한 데이터를 확보하는 방법. 동일한 데이터를 단순 증식하는 것은 과적합(Overfitting) 되기 때문에 원본 데이터의 feature값들을 아주 약간만 변형하여 증식시킴. 대표적으로 SMOTE(Synthetic Minority Over-sampling Technique)이 있다. SMOTE는 적은 데이터 세트에 있는 개별 데이터들의 K 최근접 이웃을 찾아서 이 데이터와 K개 이웃들의 차이를 일정 값으로 만들어서 기존 데이터와 약간 차이가 나는 새로운 데이터들을 생성하는 방식이다. . | . . 데이터 세트를 로딩하고 신용카드 사기 검출 모델을 생성하자 | . import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&quot;ignore&quot;) %matplotlib inline card_df = pd.read_csv(&#39;./creditcard.csv&#39;) card_df.head(3) . Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class . 0 0.0 | -1.359807 | -0.072781 | 2.536347 | 1.378155 | -0.338321 | 0.462388 | 0.239599 | 0.098698 | 0.363787 | ... | -0.018307 | 0.277838 | -0.110474 | 0.066928 | 0.128539 | -0.189115 | 0.133558 | -0.021053 | 149.62 | 0 | . 1 0.0 | 1.191857 | 0.266151 | 0.166480 | 0.448154 | 0.060018 | -0.082361 | -0.078803 | 0.085102 | -0.255425 | ... | -0.225775 | -0.638672 | 0.101288 | -0.339846 | 0.167170 | 0.125895 | -0.008983 | 0.014724 | 2.69 | 0 | . 2 1.0 | -1.358354 | -1.340163 | 1.773209 | 0.379780 | -0.503198 | 1.800499 | 0.791461 | 0.247676 | -1.514654 | ... | 0.247998 | 0.771679 | 0.909412 | -0.689281 | -0.327642 | -0.139097 | -0.055353 | -0.059752 | 378.66 | 0 | . 3 rows × 31 columns . card_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 284807 entries, 0 to 284806 Data columns (total 31 columns): # Column Non-Null Count Dtype -- -- 0 Time 284807 non-null float64 1 V1 284807 non-null float64 2 V2 284807 non-null float64 3 V3 284807 non-null float64 4 V4 284807 non-null float64 5 V5 284807 non-null float64 6 V6 284807 non-null float64 7 V7 284807 non-null float64 8 V8 284807 non-null float64 9 V9 284807 non-null float64 10 V10 284807 non-null float64 11 V11 284807 non-null float64 12 V12 284807 non-null float64 13 V13 284807 non-null float64 14 V14 284807 non-null float64 15 V15 284807 non-null float64 16 V16 284807 non-null float64 17 V17 284807 non-null float64 18 V18 284807 non-null float64 19 V19 284807 non-null float64 20 V20 284807 non-null float64 21 V21 284807 non-null float64 22 V22 284807 non-null float64 23 V23 284807 non-null float64 24 V24 284807 non-null float64 25 V25 284807 non-null float64 26 V26 284807 non-null float64 27 V27 284807 non-null float64 28 V28 284807 non-null float64 29 Amount 284807 non-null float64 30 Class 284807 non-null int64 dtypes: float64(30), int64(1) memory usage: 67.4 MB . null count 없음. | . from sklearn.model_selection import train_test_split # 인자로 입력받은 DataFrame을 복사 한 뒤 Time 컬럼만 삭제하고 복사된 DataFrame 반환 def get_preprocessed_df(df=None): df_copy = df.copy() df_copy.drop(&#39;Time&#39;, axis=1, inplace=True) return df_copy # 사전 데이터 가공 후 학습과 테스트 데이터 세트를 반환하는 함수. def get_train_test_dataset(df=None): # 인자로 입력된 DataFrame의 사전 데이터 가공이 완료된 복사 DataFrame 반환 df_copy = get_preprocessed_df(df) # DataFrame의 맨 마지막 컬럼이 레이블, 나머지는 피처들 X_features = df_copy.iloc[:, :-1] y_target = df_copy.iloc[:, -1] # train_test_split( )으로 학습과 테스트 데이터 분할. stratify=y_target으로 Stratified 기반 분할 X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target) # 학습과 테스트 데이터 세트 반환 return X_train, X_test, y_train, y_test X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) . 생성한 학습 데이터 세트와 테스트 데이터 세트의 레이블 값 비율이 비슷하게 분할된 것을 알 수 있다. . print(&#39;학습 데이터 레이블 값 비율&#39;) print(y_train.value_counts()/y_train.shape[0] * 100) print(&#39;테스트 데이터 레이블 값 비율&#39;) print(y_test.value_counts()/y_test.shape[0] * 100) . 학습 데이터 레이블 값 비율 0 99.827451 1 0.172549 Name: Class, dtype: float64 테스트 데이터 레이블 값 비율 0 99.826785 1 0.173215 Name: Class, dtype: float64 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/08/intro.html",
            "relUrl": "/2022/01/08/intro.html",
            "date": " • Jan 8, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "2022/01/07/FRI",
            "content": "Random Forest . 배깅은 앞서 소개한 보팅과는 달리 같은 알고리즘으로 여러 개의 분류기를 만들어서 보팅으로 최종 결정하는 알고리즘이다. 그 중 랜덤 포레스트의 기반 알고리즘은 결정 트리이다. 개별 트리가 학습하는 데이터 세트는 전체 데이터 세트에서 일부가 중첩되게 샘플링된 데이터 세트이다. 이렇게 여러 개의 데이터 세트를 중첩되게 분리하는 것을 부트스트래핑 분할 방식이라 한다. 이때 서브 데이터 건수는 전체 데이터 건수와 동일(218p참고),사이킷런은 RandomForestClassifier 클래스를 통해 랜덤 포레스트 기반의 분류를 지원한다. . from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score import pandas as pd import warnings warnings.filterwarnings(&#39;ignore&#39;) def get_human_dataset( ): # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당. feature_name_df = pd.read_csv(&#39;./human_activity/features.txt&#39;,sep=&#39; s+&#39;, header=None,names=[&#39;column_index&#39;,&#39;column_name&#39;]) # 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. new_feature_name_df = get_new_feature_name_df(feature_name_df) # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환 feature_name = new_feature_name_df.iloc[:, 1].values.tolist() # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용 X_train = pd.read_csv(&#39;./human_activity/train/X_train.txt&#39;,sep=&#39; s+&#39;, names=feature_name ) X_test = pd.read_csv(&#39;./human_activity/test/X_test.txt&#39;,sep=&#39; s+&#39;, names=feature_name) # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여 y_train = pd.read_csv(&#39;./human_activity/train/y_train.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) y_test = pd.read_csv(&#39;./human_activity/test/y_test.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) # 로드된 학습/테스트용 DataFrame을 모두 반환 return X_train, X_test, y_train, y_test def get_new_feature_name_df(old_feature_name_df): #column_name으로 중복된 컬럼명에 대해서는 중복 차수 부여, col1, col1과 같이 2개의 중복 컬럼이 있을 경우 1, 2 feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(&#39;column_name&#39;).cumcount(), columns=[&#39;dup_cnt&#39;]) # feature_dup_df의 index인 column_name을 reset_index()를 이용하여 컬럼으로 변환. feature_dup_df = feature_dup_df.reset_index() # 인자로 받은 features_txt의 컬럼명 DataFrame과 feature_dup_df를 조인. new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=&#39;outer&#39;) # 새로운 컬럼명은 앞에 중복 차수를 접미어로 결합. new_feature_name_df[&#39;column_name&#39;] = new_feature_name_df[[&#39;column_name&#39;, &#39;dup_cnt&#39;]].apply(lambda x : x[0]+&#39;_&#39;+str(x[1]) if x[1] &gt;0 else x[0] , axis=1) new_feature_name_df = new_feature_name_df.drop([&#39;index&#39;], axis=1) return new_feature_name_df # 결정 트리에서 사용한 get_human_dataset( )을 이용해 학습/테스트용 DataFrame 반환 X_train, X_test, y_train, y_test = get_human_dataset() # 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가 rf_clf = RandomForestClassifier(random_state=0) rf_clf.fit(X_train , y_train) pred = rf_clf.predict(X_test) accuracy = accuracy_score(y_test , pred) print(&#39;랜덤 포레스트 정확도: {0:.4f}&#39;.format(accuracy)) . 랜덤 포레스트 정확도: 0.9253 . . GridSearchCV를 이용해 랜덤 포레스트의 하이퍼 파라미터를 튜닝해보자 . from sklearn.model_selection import GridSearchCV params = { &#39;n_estimators&#39;:[100], &#39;max_depth&#39; : [6, 8, 10, 12], &#39;min_samples_leaf&#39; : [8, 12, 18 ], &#39;min_samples_split&#39; : [8, 16, 20] } # RandomForestClassifier 객체 생성 후 GridSearchCV 수행 rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1) grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 ) grid_cv.fit(X_train , y_train) print(&#39;최적 하이퍼 파라미터: n&#39;, grid_cv.best_params_) print(&#39;최고 예측 정확도: {0:.4f}&#39;.format(grid_cv.best_score_)) . 최적 하이퍼 파라미터: {&#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 8, &#39;min_samples_split&#39;: 8, &#39;n_estimators&#39;: 100} 최고 예측 정확도: 0.9180 . n_estimators를 300으로 증가시키고, 최적화 하이퍼 파라미터로 다시 RandomForestClassifier를 학습시킨 뒤 이번에는 별도의 데이터 테스트 세트에서 예측을 수행해보자 | . rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8, min_samples_split=8, random_state=0) rf_clf1.fit(X_train , y_train) pred = rf_clf1.predict(X_test) print(&#39;예측 정확도: {0:.4f}&#39;.format(accuracy_score(y_test , pred))) . 예측 정확도: 0.9165 . feature_importances_를 이용해 알고리즘이 선택한 feature의 중요도를 알아보자 | . import matplotlib.pyplot as plt import seaborn as sns ftr_importances_values = rf_clf1.feature_importances_ ftr_importances = pd.Series(ftr_importances_values,index=X_train.columns ) ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] plt.figure(figsize=(8,6)) plt.title(&#39;Feature importances Top 20&#39;) sns.barplot(x=ftr_top20 , y = ftr_top20.index) plt.show() . GBM(Gradient Boosting Machine) . 부스팅 알고리즘은 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식이다. 부스팅의 대표적인 구현은 AdaBoost(Adaptive boosting)와 그래디언트 부스트가 있다. 에이다 부스트는 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 대표적인 알고리즘이다. (223p의 그림을 통해 에이다 부스트의 진행 과정에 대해 알아보자) 예를 들어 첫번째 약한 학습기는 가중치 0.3을 부여하며, 두번째 학습기는 가중치 0.5 이런 식으로 가중치를 늘려가며 학습을 진행한다. . GBM도 에이다 부스트와 유사하나 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용한다. 오류값은 실제 값-예측 값이며, 이 오류값을 최소화하는 방향성을 가지고 반복적으로 가중치 값을 업데이트 하는 것이 경사하강법이다. . 사이킷런은 GBM 기반의 분류를 위해서 GradientBoostingClassifier클래스를 제공. . from sklearn.ensemble import GradientBoostingClassifier import time import warnings warnings.filterwarnings(&#39;ignore&#39;) X_train, X_test, y_train, y_test = get_human_dataset() # GBM 수행 시간 측정을 위함. 시작 시간 설정. start_time = time.time() gb_clf = GradientBoostingClassifier(random_state=0) gb_clf.fit(X_train , y_train) gb_pred = gb_clf.predict(X_test) gb_accuracy = accuracy_score(y_test, gb_pred) print(&#39;GBM 정확도: {0:.4f}&#39;.format(gb_accuracy)) print(&quot;GBM 수행 시간: {0:.1f} 초 &quot;.format(time.time() - start_time)) . GBM 정확도: 0.9389 GBM 수행 시간: 508.8 초 . 8분이나 걸렸음 ; . (트리 기반 자체의 파라미터더 동일하게 적용되며, GBM에서 사용하는 하이퍼 파라미터 및 튜닝에 대해 225p 참고) . GridSearchCV를 이용해 하이퍼 파라미터를 최적화해보자. 다만 꽤 오랜 시간이 걸릴 것으로 예상되어 markdown처리 하겠다. . from sklearn.model_selection import GridSearchCV params = { &#39;n_estimators&#39;:[100, 500], &#39;learning_rate&#39; : [ 0.05, 0.1] } grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=2 ,verbose=1) grid_cv.fit(X_train , y_train) print(&#39;최적 하이퍼 파라미터: n&#39;, grid_cv.best_params_) print(&#39;최고 예측 정확도: {0:.4f}&#39;.format(grid_cv.best_score_)) . 이 설정을 그대로 테스트 데이터 세트에 적용해 예측 정확도를 확인해보자. 동일하게 markdown처리 하겠음 . # GridSearchCV를 이용하여 최적으로 학습된 estimator로 predict 수행. gb_pred = grid_cv.best_estimator_.predict(X_test) gb_accuracy = accuracy_score(y_test, gb_pred) print(&#39;GBM 정확도: {0:.4f}&#39;.format(gb_accuracy)) . 이처럼 GBM은 과적합에도 강한 뛰어난 예측성능을 가진 알고리즘이지만 수행 시간이 오래 걸린다. . 이제 머신러닝 세계에서 가장 각광 받고 있는 두 개의 그래디언트 부스팅 기반 패키지에 대해 알아보자 . . XGBoost . 트리 기반임. 분류에 있어서 일반적으로 다른 머신러닝보다 뛰어난 예측성능을 나타낸다. GBM에 기반하지만 더울 빠른 수행 시간 및 과적합 규제 부재 등의 문제를 해결해 각광받음. 병렬 CPU환경에서 병렬학습이 가능해 수행시간 UP,다만 일반적인 GBM에 비해 빠르다는 것이지 다른 머신러닝 알고리즘(예를 들어 랜덤 포레스트)에 비해 빠르다는 것은 아니다. . 파이썬 래퍼 XGBoost를 적용하여 위스콘신 유방암을 예측해보자 XGBoost의 파이썬 패키지인 xgboost는 자체적으로 교차 검증, 성능 평가, feature중요도등의 시각화 기능이 있음. 또한 조기 중단 기능이 있어 num_rounds로 지정한 부스팅 반복 횟수에 도달하지 않더라도 더 이상 예측 오류가 개선되지 않으면 반복을 끝까지 수행하지 않고 중지해 수행시간을 개선할 수 있음. . import xgboost as xgb from xgboost import plot_importance import pandas as pd import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split import warnings warnings.filterwarnings(&#39;ignore&#39;) dataset = load_breast_cancer() X_features= dataset.data y_label = dataset.target cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names) cancer_df[&#39;target&#39;]= y_label cancer_df.head(3) . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension ... worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension target . 0 17.99 | 10.38 | 122.8 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | 0.07871 | ... | 17.33 | 184.6 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | 0 | . 1 20.57 | 17.77 | 132.9 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | 0.05667 | ... | 23.41 | 158.8 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | 0 | . 2 19.69 | 21.25 | 130.0 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | 0.05999 | ... | 25.53 | 152.5 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | 0 | . 3 rows × 31 columns . 종양의 크기와 모양에 관한 많은 속성이 숫자형 값으로 돼 있음. 맨 마지막 column인 target label값의 종류는 악성인 &#39;malignant&#39;가 0값으로 양성인&#39;benign&#39;이 1값으로 돼있음. | . print(dataset.target_names) print(cancer_df[&#39;target&#39;].value_counts()) . [&#39;malignant&#39; &#39;benign&#39;] 1 357 0 212 Name: target, dtype: int64 . 전체 데이터 세트 중 80%를 학습용으로 20%를 테스트 용으로 분할해보자 | . X_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=156 ) print(X_train.shape , X_test.shape) . (455, 30) (114, 30) . 파이썬 래퍼 XGBoost는 사이킷런과 여러 차이가 있지만 눈에 띄는 차이는 학습용 테스트용 데이터 세트를 위해 별도의 객체인 DMatrix를 생성한다는 점이다. DMatrix는 주로 넘파이 입력 파라미터를 받아서 만들어지는 XGBoost만의 전용 데이터 세트이다. | . dtrain = xgb.DMatrix(data=X_train , label=y_train) dtest = xgb.DMatrix(data=X_test , label=y_test) . params = { &#39;max_depth&#39;:3, &#39;eta&#39;: 0.1, &#39;objective&#39;:&#39;binary:logistic&#39;, &#39;eval_metric&#39;:&#39;logloss&#39;, &#39;early_stoppings&#39;:100 } num_rounds = 400 . 이제 지정된 하이퍼 파라미터로 XGBoost 모델을 학습시켜보자 . # train 데이터 셋은 ‘train’ , evaluation(test) 데이터 셋은 ‘eval’ 로 명기합니다. wlist = [(dtrain,&#39;train&#39;),(dtest,&#39;eval&#39;) ] # 하이퍼 파라미터와 early stopping 파라미터를 train( ) 함수의 파라미터로 전달 xgb_model = xgb.train(params = params , dtrain=dtrain , num_boost_round=num_rounds , evals=wlist ) . train()으로 학습을 수행하면 반복 시 train-error와 eval_logloss가 지속적으로 감소함. xgboost를 이용해 모델의 학습이 완료됐으면 이를 이용해 테스트 데이터 세트에 예측을 수행. . pred_probs = xgb_model.predict(dtest) print(&#39;predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨&#39;) print(np.round(pred_probs[:10],3)) # 예측 확률이 0.5 보다 크면 1 , 그렇지 않으면 0 으로 예측값 결정하여 List 객체인 preds에 저장 preds = [ 1 if x &gt; 0.5 else 0 for x in pred_probs ] print(&#39;예측값 10개만 표시:&#39;,preds[:10]) . predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨 [0.95 0.003 0.9 0.086 0.993 1. 1. 0.999 0.998 0. ] 예측값 10개만 표시: [1, 0, 1, 0, 1, 1, 1, 1, 1, 0] . from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.metrics import precision_score, recall_score from sklearn.metrics import f1_score, roc_auc_score # 수정된 get_clf_eval() 함수 def get_clf_eval(y_test, pred=None, pred_proba=None): confusion = confusion_matrix( y_test, pred) accuracy = accuracy_score(y_test , pred) precision = precision_score(y_test , pred) recall = recall_score(y_test , pred) f1 = f1_score(y_test,pred) # ROC-AUC 추가 roc_auc = roc_auc_score(y_test, pred_proba) print(&#39;오차 행렬&#39;) print(confusion) # ROC-AUC print 추가 print(&#39;정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}&#39;.format(accuracy, precision, recall, f1, roc_auc)) . . xgboost 패키지에 내장된 시각화 기능을 수행해보자 . import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(figsize=(10, 12)) plot_importance(xgb_model, ax=ax) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}, xlabel=&#39;F score&#39;, ylabel=&#39;Features&#39;&gt; . 파이썬 래퍼 XGBoost는 사이킷런의 GridSearchCV와 유사하게 데이터 세트에 대한 교차 검증 수행 후 최적 파라미터를 구할 수 있는 방법을 cv() API로 제공. xgb.cv의 반환값은 DataFrame 형태임. . XGBoost를 위한 사이킷런 래퍼는 사이킷런과 호환돼 편리하게 사용할 수 있기 때문에 앞으로는 파이썬 래퍼XGBoost가 아닌 사이킷런 래퍼 XGBoost를 사용하겠음 . XGBoost는 크게 분류를 위한 XGBClassifier, 회귀를 위한 래퍼 클래스인 XGBRegressor가 있음 . from xgboost import XGBClassifier xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3) xgb_wrapper.fit(X_train , y_train) w_preds = xgb_wrapper.predict(X_test) w_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1] . [12:59:52] WARNING: .. src learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;multi:softprob&#39; was changed from &#39;merror&#39; to &#39;mlogloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. . . 앞 예제의 파이썬 래퍼 XGBoost와 동일한 평가 결과가 나옴을 알 수 있다. 사이킷런 래퍼 XGBoost에서도 조기 중단을 수행할 수 있다. 조기 중단 관련 파라미터를 fit()에 입력하면 된다. . from xgboost import XGBClassifier xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3) evals = [(X_test, y_test)] xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;logloss&quot;, eval_set=evals, verbose=True) ws100_preds = xgb_wrapper.predict(X_test) ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1] . n_estimator를 400으로 설정해도 400번 수행하지 않고 311번 반복한 후 학습을 완료함. (211번에서 311번까지 early_stopping_rounds=100으로 지정된 100번의 반복 동안 성능 평가 지수가 향상되지 않았기 때문에) . get_clf_eval(y_test , ws100_preds, ws100_pred_proba) . 조기 중단이 적용되지 않은 결과보다 약간 저조한 성능을 나타냈지만, 큰 차이는 아님 . 조기 중단값을 너무 급격하게 줄이면 예측 성능이 저하될 우려가 크다. 만일 early_stopping_rounds를 10으로 하면 아직 성능이 향상될 여지가 있음에도 불구하고 10번 반복하는 동안 성능 평가 지표가 향상디지 않으면 반복이 멈춰 버려서 충분한 학습이 되지 않아 예측 성능이 나빠질 수 있다. . feature의 중요도를 시각화하는 모듈인 plot_importance() API에 사이킷런 래퍼 클래스를 입력해도 앞에서 파이썬 래퍼 클래스를 입력한 결과와 똑같이 시각화 결과를 얻을 수 있음 | . from xgboost import plot_importance import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(figsize=(10,12)) # 사이킷런 래퍼 클래스를 입력해도 무방. plot_importance(xgb_wrapper, ax=ax) . 그림이 상당히 지저분해서 markdown처리 하겠음 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/07/FRI.html",
            "relUrl": "/2022/01/07/FRI.html",
            "date": " • Jan 7, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "2022/01/06/THU",
            "content": "결정 트리를 이용해 데이터 세트에 대한 예측 분류를 수행해보자 . import pandas as pd import matplotlib.pyplot as plt # features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드. feature_name_df = pd.read_csv(&#39;./human_activity/features.txt&#39;,sep=&#39; s+&#39;, header=None,names=[&#39;column_index&#39;,&#39;column_name&#39;]) # 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출 feature_name = feature_name_df.iloc[:, 1].values.tolist() print(&#39;전체 피처명에서 10개만 추출:&#39;, feature_name[:10]) feature_name_df.head(20) . 전체 피처명에서 10개만 추출: [&#39;tBodyAcc-mean()-X&#39;, &#39;tBodyAcc-mean()-Y&#39;, &#39;tBodyAcc-mean()-Z&#39;, &#39;tBodyAcc-std()-X&#39;, &#39;tBodyAcc-std()-Y&#39;, &#39;tBodyAcc-std()-Z&#39;, &#39;tBodyAcc-mad()-X&#39;, &#39;tBodyAcc-mad()-Y&#39;, &#39;tBodyAcc-mad()-Z&#39;, &#39;tBodyAcc-max()-X&#39;] . column_index column_name . 0 1 | tBodyAcc-mean()-X | . 1 2 | tBodyAcc-mean()-Y | . 2 3 | tBodyAcc-mean()-Z | . 3 4 | tBodyAcc-std()-X | . 4 5 | tBodyAcc-std()-Y | . 5 6 | tBodyAcc-std()-Z | . 6 7 | tBodyAcc-mad()-X | . 7 8 | tBodyAcc-mad()-Y | . 8 9 | tBodyAcc-mad()-Z | . 9 10 | tBodyAcc-max()-X | . 10 11 | tBodyAcc-max()-Y | . 11 12 | tBodyAcc-max()-Z | . 12 13 | tBodyAcc-min()-X | . 13 14 | tBodyAcc-min()-Y | . 14 15 | tBodyAcc-min()-Z | . 15 16 | tBodyAcc-sma() | . 16 17 | tBodyAcc-energy()-X | . 17 18 | tBodyAcc-energy()-Y | . 18 19 | tBodyAcc-energy()-Z | . 19 20 | tBodyAcc-iqr()-X | . feature명을 보면 인체의 움직임과 관련된 속성의 평균/표준편차가 X,Y,Z축 값으로 돼 있음을 유추할 수 있다. . 주의 : 위에서 feature명을 가지고 있는 feature.txt 파일은 중복된 feature명을 가지고 있음. 이 중복된 feature명들을 이용해 데이터 파일을 데이터 세트 DataFrame에 로드하면 오류가 발생. 따라서 중복된 feature명에 대해서는 원본 feature명에 _1 또는 _2를 부여해 변경한 뒤에 이를 이용해서 데이터를 DataFrame에 로드하자 . 먼저 중복된 feature명이 얼마나 있는지 확인해보자 . feature_dup_df = feature_name_df.groupby(&#39;column_name&#39;).count() print(feature_dup_df[feature_dup_df[&#39;column_index&#39;]&gt;1].count()) feature_dup_df[feature_dup_df[&#39;column_index&#39;]&gt;1].head() . column_index 42 dtype: int64 . column_index . column_name . fBodyAcc-bandsEnergy()-1,16 3 | . fBodyAcc-bandsEnergy()-1,24 3 | . fBodyAcc-bandsEnergy()-1,8 3 | . fBodyAcc-bandsEnergy()-17,24 3 | . fBodyAcc-bandsEnergy()-17,32 3 | . 총 42개의 feature명이 중복돼 있다. . 이 중복된 feature명에 대해서 원본 feature명에 _1또는 _2를 추가 부여해 새로운 feature명을 가지는 DataFrame을 반환하는 함수를 정의하자 . def get_new_feature_name_df(old_feature_name_df): #column_name으로 중복된 컬럼명에 대해서는 중복 차수 부여, col1, col1과 같이 2개의 중복 컬럼이 있을 경우 1, 2 feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(&#39;column_name&#39;).cumcount(), columns=[&#39;dup_cnt&#39;]) # feature_dup_df의 index인 column_name을 reset_index()를 이용하여 컬럼으로 변환. feature_dup_df = feature_dup_df.reset_index() # 인자로 받은 features_txt의 컬럼명 DataFrame과 feature_dup_df를 조인. new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=&#39;outer&#39;) # 새로운 컬럼명은 앞에 중복 차수를 접미어로 결합. new_feature_name_df[&#39;column_name&#39;] = new_feature_name_df[[&#39;column_name&#39;, &#39;dup_cnt&#39;]].apply(lambda x : x[0]+&#39;_&#39;+str(x[1]) if x[1] &gt;0 else x[0] , axis=1) new_feature_name_df = new_feature_name_df.drop([&#39;index&#39;], axis=1) return new_feature_name_df . import pandas as pd def get_human_dataset( ): # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당. feature_name_df = pd.read_csv(&#39;./human_activity/features.txt&#39;,sep=&#39; s+&#39;, header=None,names=[&#39;column_index&#39;,&#39;column_name&#39;]) # 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. new_feature_name_df = get_new_feature_name_df(feature_name_df) # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환 feature_name = new_feature_name_df.iloc[:, 1].values.tolist() # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용 X_train = pd.read_csv(&#39;./human_activity/train/X_train.txt&#39;,sep=&#39; s+&#39;, names=feature_name ) X_test = pd.read_csv(&#39;./human_activity/test/X_test.txt&#39;,sep=&#39; s+&#39;, names=feature_name) # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여 y_train = pd.read_csv(&#39;./human_activity/train/y_train.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) y_test = pd.read_csv(&#39;./human_activity/test/y_test.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) # 로드된 학습/테스트용 DataFrame을 모두 반환 return X_train, X_test, y_train, y_test X_train, X_test, y_train, y_test = get_human_dataset() . load한 학습용 feature 데이터 세트를 간략히 살펴보자 . print(&#39; n## 학습 피처 데이터셋 info() n&#39;) print(X_train.info()) . ## 학습 피처 데이터셋 info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7352 entries, 0 to 7351 Columns: 561 entries, tBodyAcc-mean()-X to angle(Z,gravityMean) dtypes: float64(561) memory usage: 31.5 MB None . 학습 데이터 세트는 7352개의 레코드로 561개의 feature를 가지고 있다. 또한 feature가 전부 float 형의 숫자 형이므로 별도의 카테고리 인코딩은 수행할 필요가 없음. . print(y_train[&#39;action&#39;].value_counts()) . 6 1407 5 1374 4 1286 1 1226 2 1073 3 986 Name: action, dtype: int64 . 레이블 값은 1,2,3,4,5,6 즉, 6개 값이고 꽤 고르게 분포돼 있음 . 이제 사이킷런의 DecisionTreeClassifier를 이용해 동작 예측 분류를 수행하보자. 먼저 DecisionTreeClassifier의 하이퍼 파라미터는 모두 default값으로 설정해 수행하고, 이때의 하이퍼 파라미터 값을 모두 추출해보자 . from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정 dt_clf = DecisionTreeClassifier(random_state=156) dt_clf.fit(X_train , y_train) pred = dt_clf.predict(X_test) accuracy = accuracy_score(y_test , pred) print(&#39;결정 트리 예측 정확도: {0:.4f}&#39;.format(accuracy)) # DecisionTreeClassifier의 하이퍼 파라미터 추출 print(&#39;DecisionTreeClassifier 기본 하이퍼 파라미터: n&#39;, dt_clf.get_params()) . 결정 트리 예측 정확도: 0.8548 DecisionTreeClassifier 기본 하이퍼 파라미터: {&#39;ccp_alpha&#39;: 0.0, &#39;class_weight&#39;: None, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: None, &#39;max_leaf_nodes&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;random_state&#39;: 156, &#39;splitter&#39;: &#39;best&#39;} . . 이번에는 결정 트리의 트리 깊이(Tree Depth)가 예측 정확도에 주는 영향을 살펴보자. 결정 트리의 경우 분류를 위해 리프 노드(클래스 결정 노드)가 될 수 있는 적합한 수준이 될 때까지 지속해서 트리의 분할을 수행하면서 깊이가 깊어진다고 했음. 다음은 GridSearchCV를 이용해 사이킷런 결정 트리의 깊이를 조절할 수 있는 하이퍼 파라미터인 max_depth 값을 변화시키면서 예측 성능을 확인해보자. max_depth를 6~24까지 계속 늘리면서 예측 성능을 측정할 것이며, 교차 검증은 4개 세트이다. . from sklearn.model_selection import GridSearchCV params = { &#39;max_depth&#39; : [ 6, 8 ,10, 12, 16 ,20, 24] } grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=&#39;accuracy&#39;, cv=5, verbose=1 ) grid_cv.fit(X_train , y_train) print(&#39;GridSearchCV 최고 평균 정확도 수치:{0:.4f}&#39;.format(grid_cv.best_score_)) print(&#39;GridSearchCV 최적 하이퍼 파라미터:&#39;, grid_cv.best_params_) . Fitting 5 folds for each of 7 candidates, totalling 35 fits GridSearchCV 최고 평균 정확도 수치:0.8513 GridSearchCV 최적 하이퍼 파라미터: {&#39;max_depth&#39;: 16} . max_depth가 16일 때 5개의 폴드 세트의 최고 평균 정확도 결과가 약 85.13%로 도출됐다. max_depth 값 증가에 따라 예측 성능이 어떻게 변하는지 확인해보자 . cv_results_df = pd.DataFrame(grid_cv.cv_results_) # max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출 cv_results_df[[&#39;param_max_depth&#39;, &#39;mean_test_score&#39;]] . param_max_depth mean_test_score . 0 6 | 0.850791 | . 1 8 | 0.851069 | . 2 10 | 0.851209 | . 3 12 | 0.844135 | . 4 16 | 0.851344 | . 5 20 | 0.850800 | . 6 24 | 0.849440 | . mean_test_score는 5개 CV 세트에서 검증용 데이터 세트의 정확도 평균 수치이다. . 깊어진 트리는 학습 데이터 세트에는 올바른 예측 결과를 가져올지 모르지만 검증 데이터 세트에서는 오히려 과적합으로 인한 성능 저하를 유발하게 됨. . 별도의 테스트 데이터 세트에서 max_depth의 변화에 따른 값을 측정해보자 . max_depths = [ 6, 8 ,10, 12, 16 ,20, 24] # max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정 for depth in max_depths: dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156) dt_clf.fit(X_train , y_train) pred = dt_clf.predict(X_test) accuracy = accuracy_score(y_test , pred) print(&#39;max_depth = {0} 정확도: {1:.4f}&#39;.format(depth , accuracy)) . max_depth = 6 정확도: 0.8558 max_depth = 8 정확도: 0.8707 max_depth = 10 정확도: 0.8673 max_depth = 12 정확도: 0.8646 max_depth = 16 정확도: 0.8575 max_depth = 20 정확도: 0.8548 max_depth = 24 정확도: 0.8548 . max_depth = 8 일 때 가장 높은 정확도를 가짐. 8이후부터 정확도 계속 감소중. 즉 트리 깊이가 깊어질수록 테스트 데이터 세트의 정확도는 더 떨어진다. 이처럼 결정 트리는 깊이가 깊어질수록 과적합의 영향력이 커지므로 하이퍼 파라미터를 이용해 깊이를 제어해야 한다. . max_depth와 min_sample_split을 같이 변경하면서 정확도 성능을 튜닝해보자 . params = { &#39;max_depth&#39; : [ 8 , 12, 16 ,20], &#39;min_samples_split&#39; : [16,24], } grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=&#39;accuracy&#39;, cv=5, verbose=1 ) grid_cv.fit(X_train , y_train) print(&#39;GridSearchCV 최고 평균 정확도 수치: {0:.4f}&#39;.format(grid_cv.best_score_)) print(&#39;GridSearchCV 최적 하이퍼 파라미터:&#39;, grid_cv.best_params_) . Fitting 5 folds for each of 8 candidates, totalling 40 fits GridSearchCV 최고 평균 정확도 수치: 0.8549 GridSearchCV 최적 하이퍼 파라미터: {&#39;max_depth&#39;: 8, &#39;min_samples_split&#39;: 16} . max_depth가 8일 때, min_samples_split이 16일 때 가장 최고의 정확도. . 별도 분리된 테스트 데이터 세트에 해당 하이퍼 파라미터를 적용해보자 . (앞 예제의 GridSearchCV객체인 grid_cv의 속성인 best_estimator_는 최적 하이퍼 파라미터인 max_depth와 min_samples_split이 각각 8과 16으로 학습이 완료된 Estimator 객체이다) . best_df_clf = grid_cv.best_estimator_ pred1 = best_df_clf.predict(X_test) accuracy = accuracy_score(y_test , pred1) print(&#39;결정 트리 예측 정확도:{0:.4f}&#39;.format(accuracy)) . 결정 트리 예측 정확도:0.8717 . 마지막으로 결정 트리에서 각 feature의 중요도를 feature_importances_속성을 이용해 알아보자 . import seaborn as sns ftr_importances_values = best_df_clf.feature_importances_ # Top 중요도로 정렬을 쉽게 하고, 시본(Seaborn)의 막대그래프로 쉽게 표현하기 위해 Series변환 ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns ) # 중요도값 순으로 Series를 정렬 ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] plt.figure(figsize=(8,6)) plt.title(&#39;Feature importances Top 20&#39;) sns.barplot(x=ftr_top20 , y = ftr_top20.index) plt.show() . 막대 그래프상에서 확인해보면 이 중 가장 높은 중요도를 가진 Top5의 feature들이 매우 중요하게 규칙 생성에 영향을 미치고 있음을 알 수 있다. . . &#50521;&#49345;&#48660; &#54617;&#49845; . 앙상블 학습(Ensemble Learning)을 통한 분류는 여러 개의 분류기(Classifier)를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법. 단일 분류기(Classifier)보다 신뢰성이 높은 예측값을 얻을 수 있다. . 대부분의 정형 데이터 분류 시에는 앙상블이 뛰어난 성능을 나타내고 있다. . | 앙상블 학습의 유형은 전통적으로 보팅(Voting),배깅(Bagging), 부스팅(Boosting)의 세 가지로 나눌 수 있으며, 이외에도 스태킹(Stacking)을 포함한 다양한 앙상블 방법이 있음. . | . 보팅과 배깅은 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식. . . 보팅의 경우 일반적으로 서로 다른 알고리즘을 가진 분류기를 결합. 배깅은 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 보팅을 수행하는 것. | . 대표적인 배깅 방식이 랜덤 포레스트 알고리즘이다. . | 배깅은 학습하는 데이터 세트가 보팅 방식과는 다르다. 개별 분류기에 할당된 학습 데이터는 원본 학습 데이터를 샘플링해 추출하는데 이렇게 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식을 부트스트래핑(Bootstrapping) 분할 방식이라고 부름 . | 교차 검증이 데이터 세트간 중첩을 허용하지 않는 것과 다르게 배깅 방식은 중첩을 허용. . | 앙상블 학습의 유형 중 부스팅은 여러개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측한 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에게는 가중치(weight)를 부여하면서 학습과 예측을 진행. . | 대표적 부스틴 모듈로 그래디언트 부스트,XGBoost, LightGBM이 있다. . | 앙상블 학습의 유형 중 스태킹은 여러 가지 다른 모델의 예측 결과값을 다시 학습 데이터로 만들어서 다른 모델(메타 모델)로 재학습시켜 결과를 예측하는 방법 . | . . &#54616;&#46300; &#48372;&#54021;&#44284; &#49548;&#54532;&#53944; &#48372;&#54021; . 하드 보팅 : 다수결, 예측한 결과값들중 다수의 분류기가 결정한 예측값을 최종 보팅 결과값으로 선정 . 소프트 보팅 : 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결과값으로 선정.(일반적으로 이 보팅방식이 보팅 방법으로 적용됨) . 사이킷런은 보팅 방식의 앙상블을 구현한 VotingClassifier 클래스를 제공 | . import pandas as pd from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score cancer = load_breast_cancer() data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names) data_df.head(3) . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension ... worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension . 0 17.99 | 10.38 | 122.8 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | 0.07871 | ... | 25.38 | 17.33 | 184.6 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | . 1 20.57 | 17.77 | 132.9 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | 0.05667 | ... | 24.99 | 23.41 | 158.8 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | . 2 19.69 | 21.25 | 130.0 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | 0.05999 | ... | 23.57 | 25.53 | 152.5 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | . 3 rows × 30 columns . 로지스틱 회귀와 KNN을 기반으로 하여 소프트 보팅 방식으로 새롭게 보팅 분류기를 만들어보자 . VotingClassifier 클래스를 이용해 보팅 분류기를 만들 수 있으며, VotingClassifier 클래스는 주요 생성 인자로 estimators와 voting값을 입력받음. estimators는 리스트 값으로 보팅에 사용될 여러 개의 Classifier 객체들을 튜플 형식으로 입력 받으며 voting은 hard,soft선택. default는 hard . lr_clf = LogisticRegression() knn_clf = KNeighborsClassifier(n_neighbors=8) # 개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기 vo_clf = VotingClassifier( estimators=[(&#39;LR&#39;,lr_clf),(&#39;KNN&#39;,knn_clf)] , voting=&#39;soft&#39; ) X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2 , random_state= 156) # VotingClassifier 학습/예측/평가. vo_clf.fit(X_train , y_train) pred = vo_clf.predict(X_test) print(&#39;Voting 분류기 정확도: {0:.4f}&#39;.format(accuracy_score(y_test , pred))) import warnings warnings.filterwarnings(&#39;ignore&#39;) # 개별 모델의 학습/예측/평가. classifiers = [lr_clf, knn_clf] for classifier in classifiers: classifier.fit(X_train , y_train) pred = classifier.predict(X_test) class_name= classifier.__class__.__name__ print(&#39;{0} 정확도: {1:.4f}&#39;.format(class_name, accuracy_score(y_test , pred))) . Voting 분류기 정확도: 0.9474 LogisticRegression 정확도: 0.9386 KNeighborsClassifier 정확도: 0.9386 . C: Users ehfus Anaconda3 envs dv2021 lib site-packages sklearn linear_model _logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . 보팅 분류기의 정확도가 살짝 높음, 보팅으로 여러 개의 분류기를 결합한다고 해서 무조건 기반이 되는 분류기들 보다 예측 성능이 향상되진 않음. 데이터의 특성과 분포, 다양한 요건에 따라 오히려 기반 분류기 중 가장 좋은 분류기의 성능이 보팅했을 때보다 나을 수 있다. . 머신 러닝 모델의 성능은 이렇게 다양한 테스트 데이터에 의해 검증되므로 어떻게 높은 유연성을 가지고 현실에 대처할 수 있는가가 중요한 ML모델의 평가요소가 된다. 이런 관점에서 편향-분산 트레이드 오프는 ML이 극복해야할 중요 과제이다. .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/06/intro.html",
            "relUrl": "/2022/01/06/intro.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "2022/01/05/WED",
            "content": "지도학습은 레이블, 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식이다. 대표적인 유형으로는 분류(Classification)가 있으며 분류는 학습 데이터로 주어진 데이터의 feature와 레이블값(결정 값, 클래스 값)을 머신 러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것이다. 즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것이다. | . 분류는 다양한 머신러닝 알고리즘으로 구현할 수 있다. 베이즈 통계와 생성 모델에 기반한 나이브 베이즈 | 독립 변수와 종속 변수의 선형 관계성에 기반한 로지스틱 회귀 | 데이터 균일도에 따른 규칙 기반의 결정 트리 | 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신 | 근접 거리를 기준으로 하는 최소 근접 알고리즘 | 심층 연결 기반의 신경망 | 서로 다른 (또는 같은) 머신 러닝 알고리즘을 결합한 앙상블 | . | . 그 중 앙상블에 대해 배워보자 . | 앙상블에는 서로 다른 또는 서로 동일한 알고리즘을 단순히 결합한 형태도 있으나, 일반적으로는 배깅(Bagging)과 부스팅(Boosting) 방식으로 나뉜다. . | 앙상블은 서로 다른 또는 서로 동일한 알고리즘을 결합한다고 했는데 대부분은 동일한 알고리즘을 결합한다. . | . &#44208;&#51221;&#53944;&#47532; . :데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 것이다. : 스무고개 게임과 유사하며 룰 기반의 프로그램에 적용되는 if/else를 자동으로 찾아내 예측을 위한 규칙을 만드는 알고리즘을 이해하면 된다. : 따라서 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우한다. : 규칙 노드로 표시된 노드는 규칙 조건이 되는 것이며, 리프 노드로 표시된 노드는 결절된 라벨 값 즉 클래스 값을 의미한다. : 새러운 규칙 조건 마다 서브 트리가 생성된다. : 데이터 세트에 featrue가 있고 이러한 feature가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어진다. : 하지만 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 얘기이며 이는 곧 과적합으로 이어지기 쉽다. : 즉 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높다. : 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙 필요 . 위에서 분류의 다양한 머신 러닝 알고리즘 중 결정 트리는 데이터 균일도에 따른 규칙 기반이라고 했다. 그렇다면 데이터 균일도는 무엇일까? 균일도라 하면 여러 자료가 균등하게 분포되어 있을수록 균일도가 높다고 착각할 수 있는데 그렇지 않다. 예를 들어 검은 공과 흰공이 있을 때 섞여있을 수록 균일도는 낮은 것이며 오로지 검은공으로만 이루어질 수록 균일도가 높다고 할 수 있다. | . | . 만약 눈을 감고 세 주머니에서 공을 뽑을 때 오로지 검은 공으로만 이루어진 주머니에서 공을 뽑을 때 우리는 검은 공을 쉽게 예측할 수 있다. 만약 검은공과 흰공이 섞여있는 혼잡도가 높고 균일도가 낮은 주머니에서 공을 뽑을 때 검은 공을 예측하려면 더 많은 정보가 필요로 할 것이다. | . 결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만든다. 예를 들어보자, 박스 안에 서른 개의 레고 블록이 있는데 각 레고 블록은 형태 속성으로 동그라미, 네모, 세모 색깔 속성으로 노랑, 빨강, 파랑이 있다. 이 중 노랑색 블록의 경우 모두 동그라미로 구성되고 빨강과 파랑의 경우 동그라미, 네모, 세모가 골고루 섞여 있다고 한다면 각 레고 블록을 형태와 색깔 속성으로 분류하고자 할 떄 가장 첫 번째로 만들어져야 하는 규칙 조건은 if 색깔==&#39;노란색&#39;이 될것이다. 왜냐하면 노란색 블록이면 모두 노란 동그라미 블록으로 가장 쉽게 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도 조건을 찾아 분류하는것이 가장 효율적인 분류 방식이기 때문이다. | . | . 이러한 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 정보 이득($Information$ $Gain$)지수와 지니 계수가 있다. . | 정보 이득 지수 : 1 - 엔트로피 지수(주어진 데이터 집합의 혼잡도) $ to$ 결정 트리는 이 정보 이득 지수로 분할 기준을 정한다. 정보 이득이 높은 속성을 기준으로 분할한다. . | 지니 계수 : 낮을수록 데이터 균일도가 높은 것으로 해석햐 지니 계수가 낮은 속성을 기준으로 분할한다. . | . 결정트리의 일반적인 알고리즘은 데이터 세트를 분할하는 데 가장 좋은 조건, 즉 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정한다. . - 결정 트리의 가장 큰 단점은 과적합으로 적합도가 떨어진다. 트리의 깊이가 너무 깊어지면 깊어질수록 예측도는 낮아지기에 사전에 트리의 크기를 제한하는 것이 오히려 성능 튜닝에 더 도움이 된다. . 사이킷런은 결정 트리 알고리즘을 구현한 DecisionTreeClassifier(분류를 위한 클래스)와 DecisionTreeRegressor(회귀를 위한 클래스) 클래스를 제공한다. . 두 클래스 모두 동일한 파라미터를 사용하며 파라미터에 대한 설명은 188~189p 참고 . from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import warnings warnings.filterwarnings(&#39;ignore&#39;) # DecisionTree Classifier 생성 dt_clf = DecisionTreeClassifier(random_state=156) # 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리 iris_data = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=.2, random_state=11) # DecisionTreeClassifier 학습 dt_clf.fit(X_train, y_train) from sklearn.tree import export_graphviz # export_graphviz()의 호출결과로 out_file로 지정된 tree.dot 파일을 생성함 export_graphviz(dt_clf, out_file=&quot;tree.dot&quot;, class_names=iris_data.target_names, feature_names= iris_data.feature_names, impurity=True,filled=True) import graphviz # 위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북상에서 시각화 with open(&quot;tree.dot&quot;) as f: dot_graph = f.read() graphviz.Source(dot_graph) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 petal length (cm) &lt;= 2.45 gini = 0.667 samples = 120 value = [41, 40, 39] class = setosa 1 gini = 0.0 samples = 41 value = [41, 0, 0] class = setosa 0&#45;&gt;1 True 2 petal width (cm) &lt;= 1.55 gini = 0.5 samples = 79 value = [0, 40, 39] class = versicolor 0&#45;&gt;2 False 3 petal length (cm) &lt;= 5.25 gini = 0.051 samples = 38 value = [0, 37, 1] class = versicolor 2&#45;&gt;3 6 petal width (cm) &lt;= 1.75 gini = 0.136 samples = 41 value = [0, 3, 38] class = virginica 2&#45;&gt;6 4 gini = 0.0 samples = 37 value = [0, 37, 0] class = versicolor 3&#45;&gt;4 5 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 3&#45;&gt;5 7 sepal length (cm) &lt;= 5.45 gini = 0.5 samples = 4 value = [0, 2, 2] class = versicolor 6&#45;&gt;7 12 petal length (cm) &lt;= 4.85 gini = 0.053 samples = 37 value = [0, 1, 36] class = virginica 6&#45;&gt;12 8 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 7&#45;&gt;8 9 petal length (cm) &lt;= 5.45 gini = 0.444 samples = 3 value = [0, 2, 1] class = versicolor 7&#45;&gt;9 10 gini = 0.0 samples = 2 value = [0, 2, 0] class = versicolor 9&#45;&gt;10 11 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 9&#45;&gt;11 13 sepal length (cm) &lt;= 5.95 gini = 0.444 samples = 3 value = [0, 1, 2] class = virginica 12&#45;&gt;13 16 gini = 0.0 samples = 34 value = [0, 0, 34] class = virginica 12&#45;&gt;16 14 gini = 0.0 samples = 1 value = [0, 1, 0] class = versicolor 13&#45;&gt;14 15 gini = 0.0 samples = 2 value = [0, 0, 2] class = virginica 13&#45;&gt;15 더 이상 자식 노드가 없는 노드는 리프 노드이다. 리프 노드는 최종 클래스(레이블) 값이 결정되는 노드이다. . | 리프 노드가 되려면 오직 하나의 클래스 값으로 최종 데이터가 구성되거나 리프 노드가 될 수 있는 하이퍼 파라미터 조건을 충족하면 된다. . | 자식 노드가 있는 노드는 브랜치 노드이며 자식 노드를 만들기 위한 분할 규칙 조건을 가지고 있다. . | . 루트 노드인 가장 상위 1번 노드를 설명해보자면, smaples :전체 데이터가 120개라는 의미, value의 값 각각은 레이블 0,1,2값이 가지는 데이터 수를 의미함, gini는 지니 계수, class=setosa는 하위 노드를 가질 경우에 setosa의 개수가 41개로 제일 많다는 의미임 . 이처럼 결정 트리는 규칙 생성 로직을 미리 제어하지 않으면 완벽하게 클래스 값을 구별해내기 위해 트리 노드를 계속해서 만들어간다. 이로 인해 결국 매우 복잡한 규칙 트리가 만들어져 모델이 쉽게 과적합되는 문제점을 갖게 됨, 이러한 이유로 결정 트리는 과적합이 상당히 높은 ML알고리즘이다. 이때문에 결정트리 알고리즘을 제어하는 대부분 하이퍼 파라미터는 복잡한 트리가 생성되는 것을 막기 위한 용도이다. . 따라서 결정 트리의 max_depth 하이퍼 파라미터를 제한 없음에서 3개로 설정하면 더 간단한 결정 트리가 된다. . $+$ 결정트리의 또 다른 하이퍼 파라미터 요소인 min_samples_split을 4로 설정하면 서로 다른 클래스가 혼재해 있더라도 데이터 개수가 4보다 낮아지면 더이상 Split하지 않는다. . $+$ 마지막으로 min_samples_leaf 하이퍼 파라미터 변경에 따른 결정 트리의 변화를 살펴보자, 더 이상 자식 노드가 없는 리프 노드는 클래스 결정 값이 되는데, min_samples_leaf는 이 리프 노드가 될 수 있는 샘플 데이터 건수의 최솟값을 지정함 . . 결정 트리는 균일도에 기반해 어떠한 속성을 규칙 조건으로 선택하느냐가 중요한 요건이다 . 사이킷런은 결정 트리 알고리즘이 학습을 통해 규칙을 정하는 데 있어 feature의 중요한 역할 지표를 DecisionTreeClassifier 객체의 featureimportances 속성으로 제공한다. . | 해보자 . | . import seaborn as sns import numpy as np # feature importance 추출 print(&#39;Feature importance: n{}&#39;.format(np.round(dt_clf.feature_importances_,3))) # feature 별 importance 매핑 for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_): print(&#39;{}:{:.3f}&#39;.format(name,value)) # feature importance를 column 별로 시각화 해보자 sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names) . Feature importance: [0.025 0. 0.555 0.42 ] sepal length (cm):0.025 sepal width (cm):0.000 petal length (cm):0.555 petal width (cm):0.420 . &lt;AxesSubplot:&gt; . 이들 중 petal length가 가장 feature importance가 높음을 알 수 있다. . &#44208;&#51221; &#53944;&#47532; &#44284;&#51201;&#54633; (Overfitiing) . 결정 트리가 어떻게 학습 데이터를 분할해 예측을 수행하는지와 이로 인한 과적합 문제를 시각화해보자 . # 이 메서드를 이용해 2개의 feature가 3가지 유형의 클래스 값을 가지는 데이터 세트를 만들고 이를 그래프 형태로 시각화하자. from sklearn.datasets import make_classification import matplotlib.pyplot as plt plt.title(&#39;3 Class values with 2 Features Sample data creation&#39;) # 2차원 시각화를 위해서 feature는 2개, 클래스는 3가지 유형의 분류 샘플 데이터 생성 X_features,y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=0) # 그래프 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스 값은 다른 색 plt.scatter(X_features[:,0],X_features[:,1],marker=&#39;o&#39;,c=y_labels,s=25,edgecolors=&#39;k&#39;) . &lt;matplotlib.collections.PathCollection at 0x165b900c700&gt; . x축 y축은 두 개의 feature가 각 나열된 것이며, 3개의 클래스 값 구분은 색으로 하였음 . 이제 결정 트리를 기반으로 학습해보자 . from sklearn.tree import DecisionTreeClassifier # 특정한 트리 생성 제약 없는 결정 트리의 학습과 결정 경계 시각화 dt_clf=DecisionTreeClassifier().fit(X_features, y_labels) # Classifier의 Decision Boundary를 시각화 하는 함수- 이건 몰라도 될듯 def visualize_boundary(model, X, y): fig,ax = plt.subplots() # 학습 데이타 scatter plot으로 나타내기 ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap=&#39;rainbow&#39;, edgecolor=&#39;k&#39;, clim=(y.min(), y.max()), zorder=3) ax.axis(&#39;tight&#39;) ax.axis(&#39;off&#39;) xlim_start , xlim_end = ax.get_xlim() ylim_start , ylim_end = ax.get_ylim() # 호출 파라미터로 들어온 training 데이타로 model 학습 . model.fit(X, y) # meshgrid 형태인 모든 좌표값으로 예측 수행. xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) # contourf() 를 이용하여 class boundary 를 visualization 수행. n_classes = len(np.unique(y)) contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap=&#39;rainbow&#39;, clim=(y.min(), y.max()), zorder=1) visualize_boundary(dt_clf, X_features,y_labels) . 이번에는 min_samples_leaf=6을 설정해 6개 이하의 데이터는 리프 노드를 생성할 수 잇도록 리프 노트 생성 규칙을 완화한 뒤 하이퍼 파라미터를 변경해 어떻게 결정 기준 경계가 변하는지 살펴보자 . dt_clf = DecisionTreeClassifier(min_samples_leaf=6).fit(X_features, y_labels) visualize_boundary(dt_clf, X_features, y_labels) . 이상치에 크게 반응하지 않으면서 좀 더 일반화 된 분류 규칙에 따라 분류됐음을 알 수 있다. .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/05/intro.html",
            "relUrl": "/2022/01/05/intro.html",
            "date": " • Jan 5, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "2022/01/04/TUE",
            "content": "F1 &#49828;&#53076;&#50612; . - 정밀도와 재현율이 어느 한 쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가진다. . $F1 = 2 * frac{precision * recall}{precision + recall}$ . API : f1_score() . (168~169p 코드 참고) . . ROC &#44257;&#49440;&#44284; AUC &#49828;&#53076;&#50612; . - 이진 분류의 예측 성능 측정에서 중요하게 사용됨 . ROC 곡선은 FPR을 0부터 1까지 변경하면서 TPR의 변화 값을 구함. 분류 결정 임계값을 변경하면 FPR을 0부터 1까지 변경할 수 있음. FPR을 0으로 만드려면 임계값을 1로 지정하면 된다. . API : roc_curve() . 171~172,174p 참고 . 일반적으로 ROC 곡선 자체는 FPR과 TPR의 변화값을 보는 데 이용하며 분류의 성능 지표로 사용되는 것을 ROC 곡선 면적에 기반한 AUC 값으로 결정한다. AUC가 1에 가까울수록 좋은 수치이다. AUC 수치가 커지려면 FPR이 작은 상태에서 얼마나 더 큰 TPR을 얻을 수 있느냐가 관건이다. | . . &#54588;&#47560; &#51064;&#46356;&#50616; &#45817;&#45544;&#48337; &#50696;&#52769; . import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression diabetes_data = pd.read_csv(&#39;diabetes.csv&#39;) print(diabetes_data[&#39;Outcome&#39;].value_counts()) diabetes_data.head(3) . 0 500 1 268 Name: Outcome, dtype: int64 . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . Negative : 500, Positive : 268 | . diabetes_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Pregnancies 768 non-null int64 1 Glucose 768 non-null int64 2 BloodPressure 768 non-null int64 3 SkinThickness 768 non-null int64 4 Insulin 768 non-null int64 5 BMI 768 non-null float64 6 DiabetesPedigreeFunction 768 non-null float64 7 Age 768 non-null int64 8 Outcome 768 non-null int64 dtypes: float64(2), int64(7) memory usage: 54.1 KB . null값 없고, feature타입은 모두 숫자형 | 따라서 별도의 feature incoding은 불필요 | . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#47484; &#51060;&#50857;&#54644; &#50696;&#52769; &#47784;&#45944;&#51012; &#49373;&#49457;&#54644;&#48372;&#51088; . # feature 데이터 세트 X, 레이블 데이터 세트 y를 추출 # 맨 끝이 Outcome 칼럼으로서 레이블 값임. 따라서 그 칼럼의 위치인 -1을 이용해 빼줌 X=diabetes_data.iloc[:,:-1] # feature 데이터 세트 y=diabetes_data.iloc[:,-1] # 레이블 데이터 세트 X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state = 156, stratify=y) # 로지스틱 회귀로 학습, 예측 및 평가 lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) pred = lr_clf.predict(X_test) pred_proba=lr_clf.predict_proba(X_test)[:,1] get_clf_eval(y_test, pred, pred_proba) . 재현율 : 59.26%, 전체 데이터의 65%가Negative이므로 정확도보다는 재현율 성능에 조금 더 초점을 맞추기 위해 임곗값별 정밀도와 재현율 값의 변화를 확인해보자 . pred_proba_c1 = lr_clf.predict_proba(X_test)[:,1] precision_recall_curve_plot(y_test,pred_proba_c1) . 임계값 0.42 정도에서 정밀도와 재현율이 어느 정도 균형을 맞춤. 그렇지만 두 지표 모두 0.7이 안 되는 수치로서 아직도 낮은 지표 값임. 임계값을 임의적으로 조정하기 전에 데이터값을 다시 보자 . diabetes_data.describe() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . count 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | . mean 3.845052 | 120.894531 | 69.105469 | 20.536458 | 79.799479 | 31.992578 | 0.471876 | 33.240885 | 0.348958 | . std 3.369578 | 31.972618 | 19.355807 | 15.952218 | 115.244002 | 7.884160 | 0.331329 | 11.760232 | 0.476951 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.078000 | 21.000000 | 0.000000 | . 25% 1.000000 | 99.000000 | 62.000000 | 0.000000 | 0.000000 | 27.300000 | 0.243750 | 24.000000 | 0.000000 | . 50% 3.000000 | 117.000000 | 72.000000 | 23.000000 | 30.500000 | 32.000000 | 0.372500 | 29.000000 | 0.000000 | . 75% 6.000000 | 140.250000 | 80.000000 | 32.000000 | 127.250000 | 36.600000 | 0.626250 | 41.000000 | 1.000000 | . max 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 | 67.100000 | 2.420000 | 81.000000 | 1.000000 | . min행이 0이 굉장히 많음. 예를 들어 Glucose는 포도당 수치인데 0인 것은 말이 안 됨. . plt.hist(diabetes_data[&#39;Glucose&#39;],bins=10) . (array([ 5., 0., 4., 32., 156., 211., 163., 95., 56., 46.]), array([ 0. , 19.9, 39.8, 59.7, 79.6, 99.5, 119.4, 139.3, 159.2, 179.1, 199. ]), &lt;BarContainer object of 10 artists&gt;) . 0 값이 일정 수준 존재함을 알 수 있다. . min() 값이 0으로 돼 있는 feature에 대해 0 값의 건수 및 전체 데이터 건수 대비 및 몇 퍼센트의 비율로 존재하는지 확인해보자. . zero_features=[&#39;Glucose&#39;,&#39;BloodPressure&#39;,&#39;SkinThickness&#39;,&#39;Insulin&#39;,&#39;BMI&#39;] # 전체 데이터 건수 total_count=diabetes_data[&#39;Glucose&#39;].count() #feature별로 반복하면서 데이터 값이 0인 데이터 건수를 추출하고, 퍼센트 계산해보자 for feature in zero_features: zero_count = diabetes_data[diabetes_data[feature]==0][feature].count() print(&#39;{} 0건수는 {}, 퍼센트는 {:.2f}%&#39;.format(feature, zero_count, 100*zero_count/total_count)) . Glucose 0건수는 5, 퍼센트는 0.65% BloodPressure 0건수는 35, 퍼센트는 4.56% SkinThickness 0건수는 227, 퍼센트는 29.56% Insulin 0건수는 374, 퍼센트는 48.70% BMI 0건수는 11, 퍼센트는 1.43% . SkimThickness와 Insulin의 0 값은 각각 전체의 약 30,50%로 대단히 많다. 전체 데이터 건수가 약 800개 이므로 많지 않음, 따라서 0 데이터를 일괄적으로 삭제할 경우에는 학습을 효과적으로 수행하기 어렵다. 따라서 위 feature의 0값을 평균으로 대체해보자 | . mean_zero_features = diabetes_data[zero_features].mean() diabetes_data[zero_features]=diabetes_data[zero_features].replace(0,mean_zero_features) . 로지스틱 회귀의 경우 일반적으로 숫자 데이터에 스케일링을 적용하는 것이 좋음. 따라서 0값을 평균값으로 대체한 데이터 세트에 feature 스케일링을 적용해 변환해보자 . X=diabetes_data.iloc[:,:-1] y=diabetes_data.iloc[:,-1] # StandardScaler 클래스를 이용해 feature 데이터 세트에 일괄적으로 스케일링 적용 scaler = StandardScaler() X_scaled = scaler.fit_transform(X) X_train,X_test,y_train,y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=156,stratify=y) # 로지스틱 회귀로 학습, 예측 및 평가 수행 lr_clf=LogisticRegression() lr_clf.fit(X_train,y_train) pred = lr_clf.predict(X_test) pred_proba=lr_clf.predict_proba(X_test)[:,1] get_clf_eval(y_test,pred,pred_proba) . 데이터 변환과 스케일링을 통해 성능 수치가 일정 수준 개선됐지만 재현율 수치는 아직 개선이 더 필요, 분류 결정 임계값을 0.3에서 0.5까지 0.03씩 변화시키면서 재현율과 다른 평가 지표의 값 변화를 출력해보자 . thresholds = [0.3,0.33,0.36,0.39,0.42,0.45,0.5] pred_proba = lr_clf.predict_proba(X_test) get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1),thresholds) . 값을 반환했을 때 정확도와 정밀도를 어느 정도 희생하고 재현율을 높이는 데 가장 좋은 임계값은 0.33으로 재현율 값이 0.7963이다. 하지만 정밀도가 매우 저조해져서 극단적 선택임. 임계값 0.48이 전체적인 성능 평가 지표를 유지하면서 재현율을 약간 향상시키는 좋은 임계값으로 보임. . 181p 코드 참고 . &#51648;&#44552;&#44620;&#51648; &#48516;&#47448;&#50640; &#49324;&#50857;&#46104;&#45716; &#51221;&#54869;&#46020;, &#50724;&#52264; &#54665;&#47148;, &#51221;&#48128;&#46020;, &#51116;&#54788;&#50984;, F1 &#49828;&#53076;&#50612;, ROC-AUC&#50752; &#44057;&#51008; &#49457;&#45733; &#54217;&#44032; &#51648;&#54364;&#44032; &#51080;&#50632;&#51020;. .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/04/intro.html",
            "relUrl": "/2022/01/04/intro.html",
            "date": " • Jan 4, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "2022/01/03/MON",
            "content": "평가 process에 대해 알아보자 . - &#51221;&#54869;&#46020; ? = &#50696;&#52769; &#44208;&#44284;&#44032; &#46041;&#51068;&#54620; &#45936;&#51060;&#53552; &#44148;&#49688; / &#51204;&#52404; &#50696;&#52769; &#45936;&#51060;&#53552; &#44148;&#49688; . - 이진 부류의 경우 데이터 구성에 따라 ML 모델의 성능을 왜곡할 수 있기 때문에 정확도 수치 하나만 가지고 성능을 평가하는 건 위험함 - 그 예를 살펴보자 . from sklearn.base import BaseEstimator import numpy as np class MyDummyClassifier(BaseEstimator): # fit 메서드는 아무것도 학습하지 않음 def fit(self, X, y=None): pass # predict() 메서드는 단순히 Sex feature 1이면 0 그렇지 않으면 1로 예측함 def predict(self, X): pred = np.zeros((X.shape[0],1)) for i in range(X.shape[0]) : if X[&#39;Sex&#39;].iloc[i]==1 : pred[i]=0 else : pred[i]=1 return pred . import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # 원본 데이터를 재로딩, 데이터 가공, 학습 데이터/테스트 데이터 분할 titanic_df= pd.read_csv(&#39;C:/Users/ehfus/Downloads/titanic/train.csv&#39;) y_titanic_df=titanic_df[&#39;Survived&#39;] X_titanic_df=titanic_df.drop(&#39;Survived&#39;,axis=1) X_titanic_df=transform_features(X_titanic_df) # transform_features 함수 정의 안 했기 때문에 에러 발생(140p) x_train,X_test,y_train,y_test = train_test_split(X_titanic_df, y_titanic_df,test_size=.2,random_state=0) # 위에서 생성한 Dummy Classifier를 이용해 학습/예측/평가 수행 myclf=MydummyClassifier() myclf.fit(X_train,y_train) mypredictions=myclf.predict(X_train,y_train) mypredictions = myclf.predict(X_test) print(&#39;Dummy Classifier의 정확도는: {0:.4f}&#39;.format(accuracy_score(y_test,mypredictions))) . Dummy Classifier의 정확도는 0.7877정도 나온다 . 즉 이렇게 단순한 알고리즘으로 예측을 하더라도 데이터의 구성에 따라 정확도의 결과는 약 78%정도로 높은 수치가 나올 수 있음. 따라서 정확도를 지표로 사용할 때는 매우 신중해야 함. . 특히 불균형한 레이블 값 분포에서 ML 모델의 성능을 판단할 경우 적합한 평가 지표가 아님, 예를 들어보자 . from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.base import BaseEstimator from sklearn.metrics import accuracy_score import numpy as np import pandas as pd class MyFakeClassifier(BaseEstimator): def fit(self, X, y): pass # 입력값으로 들어오는 X 데이터 세트의 크기만큼 모두 0값으로 만들어서 반환 def predict(self, X): return np.zeros((len(X),1),dtype=bool) # 사이킷런의 내장 데이터 세트인 load_digits()를 이용해 MNIST 데이터 로딩 digits = load_digits() # digits 번호가 7번이면 True이고 이를 astype(int)로 1로 변환, 7번이 아니면 Fasle이고 0으로 변환. y=(digits.target==7).astype(int) X_train,X_test,y_train,y_test= train_test_split(digits.data,y,random_state=11) # 불균형한 레이블 데이터 분포도 확인 print(&#39;레이블 테스트 세트 크기: &#39;,y_test.shape) print(&#39;테스트 세트 레이블 0과 1의 분포도&#39;) print(pd.Series(y_test).value_counts()) # Dummy Classifier로 학습/예측/정확도 평가 fakeclf=MyFakeClassifier() fakeclf.fit(X_train,y_train) fakepred=fakeclf.predict(X_test) print(&#39;모든 예측을 0으로 하여도 정확도는:{:.3f}&#39;.format(accuracy_score(y_test,fakepred))) . 레이블 테스트 세트 크기: (450,) 테스트 세트 레이블 0과 1의 분포도 0 405 1 45 dtype: int64 모든 예측을 0으로 하여도 정확도는:0.900 . 이처럼 정확도 평가 지표는 불균형한 레이블 데이터 세트에서는 성능 수치로 사용 되어서는 안 된다. 여러가지 분류 지표와 함께 이용하자 | . - &#50724;&#52264;&#54665;&#47148; &#46608;&#45716; &#54844;&#46041;&#54665;&#47148; ? . - 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리고 있는지도 함께 보여주는 지표 - TN,FP,FN,TP로 나뉘며, 앞문자는 예측값과 실제값이 &#39;같은가/ 틀린가&#39;를 의미, 뒤 문자는 예측 결과 값이 부정(0)/긍정(1)을 의미 - 이 값을 조합해 정확도, 정밀도, 재현율 값을 알 수 있음 . from sklearn.metrics import confusion_matrix confusion_matrix(y_test, fakepred) . array([[405, 0], [ 45, 0]], dtype=int64) . TN은 405개, FN은 45개이다. | . 정확도 = 예측 결과와 실제 값이 동일한 건수/전체 데이터 수 = (TN + TP) / (TN + FP + FN + TP) | 정밀도 = TP / (FP + TP) | 재현율 = TP = (FN + TP) | . (158p 참고) . 분류하려는 업무의 특성상 정밀도 또는 재현율이 특별히 강조되어야 할 경우 분류의 결정 임계값(Threshold)을 조정해 정밀도 또는 재현율의 수치를 높일 수 있다. . 개별 데이터 별로 예측 확률을 반환하는 메서드 = predict_proba() predict_proba() 메서드는 학습이 완료된 사이킷런 Classifier 객체에서 호출이 가능하며 테스트 feature 데이터 세트를 파라미터로 입력해주면 테스트 feature 레코드의 개별 클래스 예측 확률을 반환함. predict() 메서드와 유사하지만 단지 반환 결과가 예측 결과 클래스값이 아닌 예측 확률 결과이다. . (160p 참고) . threshold 값을 조정해보자 | . from sklearn.preprocessing import Binarizer X=[[1,-1,2], [2,0,0], [0,1.1,1.2]] # X의 개별 원소들이 threshold 값보다 같거나 작으면 0을 크면 1을 반환 binarizer = Binarizer(threshold=1.1) print(binarizer.fit_transform(X)) . [[0. 0. 1.] [1. 0. 0.] [0. 0. 1.]] . 이제 이 Binarizer를 이용해 사이킷런 predict()의 의사(pseudo)코드를 만들어보자 . from sklearn.preprocessing import Binarizer # Binarizer의 threshold 설정값, 즉 분류 결정 임계값임. custom_threshold=0.5 # predict_proba() 반환값의 두 번째 칼럼, 즉 Positive 클래스 칼럼 하나만 추출해 Binirizer를 적용 pred_proba_1=pred_proba[:,1].reshape(-1,1) binarizer = Binarizer(threshold=cistom_threshold).fit(pred_proba_1) custom_predict = binarizer.transform(pred_proba_1) get_clf_eval(y_test, custom_predict) . 이때 threshold 설정값을 0.4로 설정, 즉 분류 결정 임계값을 낮추면 True값이 많아질 것이고 재현율 값이 올라가고 정밀도는 떨어질 것이다. . 임계값을 0.4에서부터 0.6까지 0.05씩 증가시키며 평가 지표를 조사해보자, 이를 위해 get_eval_by_threshold() 함수를 만듦 . # 테스트를 수행할 모든 임계값을 리스트 객체로 저장 thresholds = [0.4,0.45,0.5,0.55,0.6] def get_eval_by_threshold(y_test,pred_proba_c1,thresholds): # thresholds list 객체 내의 값을 차례로 iteration하면서 Evaluation 수행. for custom_threshold in thresholds: binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) custom_threshold = binarizer.transform(pres_proba_c1) print(&#39;임계값:&#39;,custom_threshold) get_clf_eval(y_test,custom_predict) get_eval_by_threshold(y_test,pred_proba[:,1].reshapep(-1,1), thresholds) . 임계값의 변경은 업무 환경에 맞게 두 개의 수치를 상호 보완할 수 있는 수준에서 적용돼야 한다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/03/intro.html",
            "relUrl": "/2022/01/03/intro.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "2022/01/02/SUN",
            "content": "&#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 데이터 인코딩 - 레이블 인코딩 . | . from sklearn.preprocessing import LabelEncoder items=[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;] # LabelEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행 encoder = LabelEncoder() # 객체 생성 encoder.fit(items) labels=encoder.transform(items) print(&#39;인코딩 변환값: &#39;, labels) print(&#39;-&#39;) print(&#39;인코딩 클래스: &#39;, encoder.classes_) print(&#39;차례대로 0부터 5까지 부여됨&#39;) print(&#39;-&#39;) print(&#39;디코딩 원본값: &#39;,encoder.inverse_transform([0,4,5,5,1,1,2,0])) print(&#39;이렇게 원하는 인코딩 값의 리스트를 통해 디코딩 할 수 있다&#39;) . 인코딩 변환값: [0 1 4 5 3 3 2 2] - 인코딩 클래스: [&#39;TV&#39; &#39;냉장고&#39; &#39;믹서&#39; &#39;선풍기&#39; &#39;전자레인지&#39; &#39;컴퓨터&#39;] 차례대로 0부터 5까지 부여됨 - 디코딩 원본값: [&#39;TV&#39; &#39;전자레인지&#39; &#39;컴퓨터&#39; &#39;컴퓨터&#39; &#39;냉장고&#39; &#39;냉장고&#39; &#39;믹서&#39; &#39;TV&#39;] 이렇게 원하는 인코딩 값의 리스트를 통해 디코딩 할 수 있다 . ***주의*** . 레이블 인코딩이 1,2일때 특정 ML알고리즘에서 가중치가 더 부여되거나 더 중요하게 인식할 가능성이 발생함. 하지만 단순 인코딩 숫자이기에 이러한 현상은 피해야함. 따라서 이러한 레이블 인코딩은 선형 회귀와 같은 ML 알고리즘에는 적용 X, 트리 계열의 알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 레이블 인코딩도 별 문제 X, 원-핫 인코딩은 레이블 인코딩의 이러한 문제점을 해결하기 위한 인코딩 방식임 . . 데이터 인코딩 - 원-핫 인코딩 . | . from sklearn.preprocessing import OneHotEncoder import numpy as np items=[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;] # 먼저 숫자 값으로 변환하기 위해 LabelEncoder로 변환해야함 encoder=LabelEncoder() # 객체 생성 encoder.fit(items) labels=encoder.transform(items) # 꼭 2차원 데이터로 변경해야 함 labels=labels.reshape(-1,1) # 이제 원-핫 인코딩 oh_encoder= OneHotEncoder() # 객체 생성 oh_encoder.fit(labels) oh_labels=oh_encoder.transform(labels) print(&#39;원-핫 인코딩 데이터&#39;) print(oh_labels.toarray()) # array 형태로 print(&#39;원-핫 인코딩 데이터 차원&#39;) print(oh_labels.shape) # 8행 6열의 행렬 형태 . 원-핫 인코딩 데이터 [[1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0.] [0. 0. 0. 1. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0.]] 원-핫 인코딩 데이터 차원 (8, 6) . 이러한 과정을 pandas를 통해 한 번에? = get_dummies . import pandas as pd df=pd.DataFrame({&#39;item&#39;:[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;]}) pd.get_dummies(df) . item_TV item_냉장고 item_믹서 item_선풍기 item_전자레인지 item_컴퓨터 . 0 1 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 1 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 1 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 1 | . 4 0 | 0 | 0 | 1 | 0 | 0 | . 5 0 | 0 | 0 | 1 | 0 | 0 | . 6 0 | 0 | 1 | 0 | 0 | 0 | . 7 0 | 0 | 1 | 0 | 0 | 0 | . get_dummies()를 이용하면 숫자형 값으로 변환 없이도 바로 변환이 가능함 | . . feature &#49828;&#52992;&#51068;&#47553;&#44284; &#51221;&#44508;&#54868; . feature scaling에는 표준화와 정규화가 있으며, 표준화는 feature각각이 평균0, 분산1인 가우시안 정규 분포를 가진 값으로 변환하는 것을 의미 . $x_i _new$ = $ frac{x_i-mean(x)}{stdev(x)}$ 이며 $stdev(x)$는 표준편차를 의미함. . 일반적으로 정규화는 서로 다른 feature의 크기를 통일하기 위해 크기는 변환해주는 개념. 0~1값으로 변환한다. 즉 개별 데이터의 크기를 모두 똑같은 단위로 변경하는 것. . $x_i _new$ = $ frac{x_i-min(x)}{max(x)-min(x)}$ . 주의 . 사이킷런의 전처리에서 제공하는 Normalizer 모듈과 일반적인 정규화는 약간의 차이가 있음. 사이킷런의 Normalizer 모듈은 선형 대수에서의 정규화 개념이 적용 됐으며, 개별 벡터의 크기를 맞추기 위해 변환하는 것을 의미함. . $x_i _new$ = $ frac{x_i}{ sqrt(x_i^2+y_i^2+z_i^2)}$ . 이를 벡터 정규화로 지칭하자 . . StandardScaler :표준화 . from sklearn.datasets import load_iris iris=load_iris() iris_data=iris.data iris_df=pd.DataFrame(data=iris_data,columns=iris.feature_names) print(&#39;feature들의 평균값&#39;) print(iris_df.mean()) print(&#39; nfeature들의 분산값&#39;) print(iris_df.var()) . feature들의 평균값 sepal length (cm) 5.843333 sepal width (cm) 3.057333 petal length (cm) 3.758000 petal width (cm) 1.199333 dtype: float64 feature들의 분산값 sepal length (cm) 0.685694 sepal width (cm) 0.189979 petal length (cm) 3.116278 petal width (cm) 0.581006 dtype: float64 . from sklearn.preprocessing import StandardScaler scaler=StandardScaler() # 객체 생성 scaler.fit(iris_df) iris_scaled=scaler.transform(iris_df) # transform()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DataFrame으로 변환 iris_df_scaled = pd.DataFrame(data=iris_scaled,columns=iris.feature_names) print(&#39;feature들의 평균값&#39;) print(iris_df_scaled.mean()) print(&#39; nfeature들의 분산값&#39;) print(iris_df_scaled.var()) . feature들의 평균값 sepal length (cm) -1.690315e-15 sepal width (cm) -1.842970e-15 petal length (cm) -1.698641e-15 petal width (cm) -1.409243e-15 dtype: float64 feature들의 분산값 sepal length (cm) 1.006711 sepal width (cm) 1.006711 petal length (cm) 1.006711 petal width (cm) 1.006711 dtype: float64 . 두 셀을 비교해보면 위 셀 $ to$ 아래 셀, 모든 칼럼 값의 평균이 0에 아주 가까운 값으로, 그리고 분산은 1에 아주 가까운 값으로 변환됐음을 알 수 있다. | . MinMaxScaler :정규화 . 데이터 값을 0과 1사이의 범위 값으로 변환한다. (음수 값이 있으면 -1에서 1값으로 변환) | 데이터의 분포가 가우시안 분포가 아닐 경우에 적용 가능 | . from sklearn.preprocessing import MinMaxScaler scaler=MinMaxScaler() scaler.fit(iris_df) iris_scaled=scaler.transform(iris_df) # transform()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DF으로 변환 iris_df_scaled = pd.DataFrame(data=iris_scaled,columns=iris.feature_names) print(&#39;feature들의 최솟값&#39;) print(iris_df_scaled.min()) print(&#39; nfeature들의 최댓값&#39;) print(iris_df_scaled.max()) . feature들의 최솟값 sepal length (cm) 0.0 sepal width (cm) 0.0 petal length (cm) 0.0 petal width (cm) 0.0 dtype: float64 feature들의 최댓값 sepal length (cm) 1.0 sepal width (cm) 1.0 petal length (cm) 1.0 petal width (cm) 1.0 dtype: float64 . 모든 feature가 0에서 1사이의 값으로 변환되는 스케일링이 적용됐음을 알 수 있다. | . . 유의점 128p 참고 . 학습 데이터로 fit()이 적용된 스케일링 기준 정보를 그대로 테스트 데이터에 적용해야 하며, 그렇지 않으면 학습 데이터와 테스트 데이터의 스케일링 기준 정보가 서로 달라지기 때문에 올바른 예측 결과를 도출하지 못할 수 있다. . test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform()만으로 변환해야한다. . 즉 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리하던가 이것이 여의치 않다면 테스트 데이터 변환 시에는 fit()이나 fit_transform()을 적용하지 않고 학습 데이터로 이미 fit()된 scaler 객체를 이용해 transform()으로 변환 | . . 131p 사이킷런으로 수행하는 타이타닉 생존자 예측, 꼭 한 번 실습해보기 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/02/intro.html",
            "relUrl": "/2022/01/02/intro.html",
            "date": " • Jan 2, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "2022/01/01/SAT(HappyNewYear)",
            "content": "datail-review 해보자 . feature_names = 높이,가로 길이 이런 것들, data = 각 featuredml 값들, target = 0,1,2...예를 들면 붓꽃의 이름을 대용한 것, target_names = 각 target이 가리키는 이름이 무엇인지? | . . model_selection 모듈은 학습 데이터와 테스트 데이터 세트를 분리하거나 교차 검증 분할 및 평가, 그리고 Estimator의 하이퍼 파라미터를 튜닝하기 위한 다양한 함수와 클래스를 제공, 전체 데이터를 학습 데이터와 테스트 데이터 세트로 분리해주는 train_test_split()부터 살펴보자 . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score iris=load_iris() # 붓꽃 데이터 세트 로딩 dt_clf=DecisionTreeClassifier() train_data=iris.data # 데이터 세트에서 feature만으로 구성된 데이터가 ndarray train_label=iris.target # 데이터 세트에서 label 데이터 dt_clf.fit(train_data, train_label) # 학습 수행중 pred=dt_clf.predict(train_data) # 예측 수행중 // 그런데 학습때 사용했던 train_data를 사용했음 -&gt; 예측도 1 나올 것 print(&#39;예측도: &#39;,accuracy_score(train_label,pred)) . 예측도: 1.0 . 정확도가 100% 나왔음 $ to$ 이미 학습한 학습 데이터 세트를 기반으로 예측했기 때문. 답을 알고 있는데 같은 문제를 낸 것이나 마찬가지 | 따라서 예측을 수행하는 데이터 세트는 학습을 수행한 학습용 데이터 세트가 아닌 전용의 테스트 데이터 세트여야 함. | . from sklearn.model_selection import train_test_split . dt_clf=DecisionTreeClassifier() iris=load_iris() # train_test_split()의 반환값은 튜플 형태이다. 순차적으로 네가지 요소들을 반환한다 X_train,X_test,y_train,y_test=train_test_split(iris.data, iris.target,test_size=0.3,random_state=121) dt_clf.fit(X_train,y_train) pred = dt_clf.predict(X_test) print(&#39;예측 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 예측 정확도: 0.9556 . . 지금까지의 방법은 모델이 학습 데이터에만 과도하게 최적화되어, 실제 예측을 다른 데이터로 수행할 경우에는 예측 성능이 과도하게 떨어지는 과적합이 발생할 수 있다. 즉 해당 테스트 데이터에만 과적합되는 학습 모델이 만들어져 다른 테스트용 데이터가들어올 경우에는 성능이 저하된다. $ to$ 개선하기 위해 교차검증을 이용해 다양한 학습과 평가를 수행해야 한다. . 교차검증? . : 본고사 치르기 전, 여러 모의고사를 치르는 것. 즉 본고사가 테스트 데이터 세트에 대해 평가하는 것이라면 모의고사는 교차 검증에서 많은 학습과 검증 세트에서 알고리즘 학습과 평가를 수행하는 것. . : 학습 데이터 세트를 검증 데이터 세트와 학습 데이터 세트로 분할하여 수행한 뒤, 모든 학습/검증 과정이 완료된 후 최종적으로 성능을 평가하기 위해 테스트 데이터 세트를 마련함. . K fold 교차 검증? . : K개의 데이터 폴드 세트를 만들어서 K번만큼 각 폴드 세트에 학습과 검증, 평가를 반복적으로 수행 / 개괄적 과정은 교재 104 참고 . 실습해보자 | . import numpy as np . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import KFold # 위에서는 trian_test_split을 import했었음 iris=load_iris() # 붓꽃 데이터 세트 로딩 features=iris.data label=iris.target dt_clf=DecisionTreeClassifier(random_state=156) kfold=KFold(n_splits=5) # KFold 객체 생성 cv_accuracy=[] # fold set별 정확도를 담을 리스트 객체 생성 print(&#39;붓꽃 데이터 세트 크기:&#39;,features.shape[0]) . 붓꽃 데이터 세트 크기: 150 . . kfold=KFold(n_splits=5) . 로 KFold객체를 생성했으니 객체의 split()을 호출해 전체 붓꽃 데이터를 5개의 fold 데이터 세트로 분리하자. 붓꽃 데이터 세트 크기가 150개니 120개는 학습용, 30개는 검증 테스트 데이터 세트이다. . n_iter=0 for train_index,test_index in kfold.split(features): # kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출 X_train, X_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] # 학습 및 예측 dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) n_iter+=1 # 반복 시마다 정확도 측정 accuracy = np.round(accuracy_score(y_test,pred),4) train_size = X_train.shape[0] test_size = X_test.shape[0] print(&#39; n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기 :{2}, 검증 데이터 크기 :{3}&#39;.format(n_iter,accuracy,train_size,test_size)) print(&#39;#{0} 검증 세트 인덱스:{1}&#39;.format(n_iter, test_index)) cv_accuracy.append(accuracy) # 개별 iteration별 정확도를 합하여 평균 정확도 계산 print(&#39; n *Conclusion* 평균 검증 정확도:&#39;, np.mean(cv_accuracy)) . #1 교차 검증 정확도 :1.0, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #1 검증 세트 인덱스:[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29] #2 교차 검증 정확도 :0.9667, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #2 검증 세트 인덱스:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59] #3 교차 검증 정확도 :0.8667, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #3 검증 세트 인덱스:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89] #4 교차 검증 정확도 :0.9333, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #4 검증 세트 인덱스:[ 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119] #5 교차 검증 정확도 :0.7333, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #5 검증 세트 인덱스:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] *Conclusion* 평균 검증 정확도: 0.9 . . 교차 검증시마다 검증 세트의 인덱스가 달라짐을 알 수 있다. . | 검증세트 인덱스를 살펴보면 104p에서 설명한 그림의 설명과 유사함 . | . . Stratified K 폴드 . : 불균형한 분포도를가진 레이블(결정 클래스) 데이터 집합을 위한 K 폴드 방식이다. 불균형한 분포도를 가진 레이블 데이터 집합은 특정 레이블 값이 특이하게 많거나 또는 적어서 분포가 한쪽으로 치우치는 것을 말함 . 가령 대출 사기 데이터를 예측한다고 가정해보자, 이 데이터 세트는 1억건이고 수십개의 feature와 대출 사기 여부를 뜻하는 label(정상 대출0, 대출사기 : 1)로 구성돼 있다. K폴드로 랜덤하게 학습 및 테스트 세트의 인덱스를 고르더라도 레이블 값인 0과1의 비율을 제대로 반영하지 못하게 됨. 따라서 원본 데이터와 유사한 대출 사기 레이블 값의 분포를 학습/테스트 세트에도 유지하는 게 매우 중요 . Stratified K 폴드는 이처럼 K폴드가 레이블 데이터 집합이 원본 데이터 집합의 레이블 분포를 학습 및 테스트 세트에 제대로 분배하지 못하는 경우의 문제를 해결해줌 | . 붓꽃 데이터 세트를 DataFrame으로 생성하고 레이블 값의 분포도를 먼저 확인해보자 . import pandas as pd iris=load_iris() iris_df=pd.DataFrame(data=iris.data,columns=iris.feature_names) iris_df[&#39;label&#39;]=iris.target print(iris_df[&#39;label&#39;].value_counts(),&#39; n&#39;) . 0 50 1 50 2 50 Name: label, dtype: int64 . label값은 모두 50개로 분배되어 있음 | . kfold=KFold(n_splits=3) n_iter=0 for train_index, test_index in kfold.split(iris_df): n_iter+=1 label_train = iris_df[&#39;label&#39;].iloc[train_index] label_test=iris_df[&#39;label&#39;].iloc[test_index] print(&#39;## 교차 검증: {}&#39;.format(n_iter)) print(&#39;학습 레이블 데이터 분포: n&#39;, label_train.value_counts()) print(&#39;검증 레이블 데이터 분포: n&#39;, label_test.value_counts()) print(&#39;&#39;) . ## 교차 검증: 1 학습 레이블 데이터 분포: 1 50 2 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 50 Name: label, dtype: int64 ## 교차 검증: 2 학습 레이블 데이터 분포: 0 50 2 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 1 50 Name: label, dtype: int64 ## 교차 검증: 3 학습 레이블 데이터 분포: 0 50 1 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 2 50 Name: label, dtype: int64 . 교차 검증 시마다 3개의 폴드 세트로 만들어지는 학습 레이블과 검증 레이블이 완전히 다른 값으로 추출되었다. 예를 들어 첫번째 교차 검증에서는 학습 레이블의 1,2값이 각각 50개가 추출되었고 검증 레이블의 0값이 50개 추출되었음, 즉 학습레이블은 1,2 밖에 없으므로 0의 경우는 전혀 학습하지 못함. 반대로 검증 레이블은 0밖에 없으므로 학습 모델은 절대 0을 예측하지 못함. 이런 유형으로 교차 검증 데이터 세트를 분할하면 검증 예측 정확도는 0이 될 수밖에 없다. | . StratifiedKFold는 이렇게 KFold로 분할된 레이블 데이터 세트가 전체 레이블 값의 분포도를 반영하지 못하는 문제를 해결함. | . . 실습해보자 . from sklearn.model_selection import StratifiedKFold skf=StratifiedKFold(n_splits=3) n_iter=0 # split 메소드에 인자로 feature데이터 세트뿐만 아니라 레이블 데이터 세트도 반드시 넣어줘야함 for train_index,test_index in skf.split(iris_df,iris_df[&#39;label&#39;]): n_iter+=1 label_train=iris_df[&#39;label&#39;].iloc[train_index] label_test=iris_df[&#39;label&#39;].iloc[test_index] print(&#39;## 교차검증: {}&#39;.format(n_iter)) print(&#39;학습 레이블 데이터 분포: n&#39;, label_train.value_counts()) print(&#39;검증 레이블 데이터 분포: n&#39;, label_test.value_counts()) print(&#39;--&#39;) . ## 교차검증: 1 학습 레이블 데이터 분포: 2 34 0 33 1 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 17 1 17 2 16 Name: label, dtype: int64 -- ## 교차검증: 2 학습 레이블 데이터 분포: 1 34 0 33 2 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 17 2 17 1 16 Name: label, dtype: int64 -- ## 교차검증: 3 학습 레이블 데이터 분포: 0 34 1 33 2 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 1 17 2 17 0 16 Name: label, dtype: int64 -- . 학습 레이블과 검증 레이블 데이터 값의 분포도가 동일하게 할당됐음을 알 수 있다. 이렇게 분할이 되어야 레이블 값 0,1,2를 모두 학습할 수 있고 이에 기반해 검증을 수행할 수 있다. | . 이제 StratifiedKFold를 이용해 붓꽃 데이터를 교차 검증해보자 | . df_clf=DecisionTreeClassifier(random_state=156) skfold=StratifiedKFold(n_splits=3) n_iter=3 cv_accuracy=[] # StratifiedKFol의 split() 호출시 반드시 레이블 데이터 세트도 추가 입력 필요 for train_index, test_ondex in skfold.split(features, label): # split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출 X_train,X_test=features[train_index],features[test_index] y_train,y_test=label[train_index], label[test_index] # 학습 및 예측 df_clf.fit(X_train,y_train) pred=dt_clf.predict(X_test) # 반복시마다 정확도 측정 n_iter+=1 accuracy=np.around(accuracy_score(y_test,pred),4) train_size=X_train.shape[0] test_size = X_test.shape[0] print(&#39; n#{} 교차 검증 정확도 : {}, 학습 데이터 크기 : {}, 검증 데이터 크기 : {}&#39;.format(n_iter,accuracy,train_size,test_size)) print(&#39;#{} 검증 세트 인덱스: {}&#39;.format(n_iter, test_index)) cv_accuracy.append(accuracy) # 교차 검증별 정확도 및 평균 정확도 계산 print(&#39; n## 교차 검증별 정확도:&#39;, np.around(cv_accuracy,4)) print(&#39;## 평균 검증 정확도:&#39;,np.mean(cv_accuracy)) . #4 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #4 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92] ## 평균 검증 정확도: 0.92 #5 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #5 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92 0.92] ## 평균 검증 정확도: 0.92 #6 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #6 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92 0.92 0.92] ## 평균 검증 정확도: 0.92 . . &#44368;&#52264; &#44160;&#51613;&#51012; &#48372;&#45796; &#44036;&#54200;&#54616;&#44172; - cross_val_score() . from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score,cross_validate from sklearn.datasets import load_iris iris_data=load_iris() dt_clf = DecisionTreeClassifier(random_state=156) data= iris_data.data label=iris_data.target # 성능 지표는 정확도 (accuracy), 교차 검증 세트는 3개 scores = cross_val_score(dt_clf, data, label, scoring=&#39;accuracy&#39;, cv=3) print(&#39;교차 검증별 정확도: &#39;,np.round(scores,4)) print(&#39;평균 검증 정확도: &#39;,np.round(np.mean(scores),4)) . 교차 검증별 정확도: [0.98 0.94 0.98] 평균 검증 정확도: 0.9667 . cv로 지정된 횟수만큼 scoring 파라미터로 지정된 평가지표로 평가 결과값을 배열로 반환 | . . GridSearchCV - &#44368;&#52264; &#44160;&#51613;&#44284; &#52572;&#51201; &#54616;&#51060;&#54140; &#54028;&#46972;&#48120;&#53552; &#53916;&#45789;&#51012; &#46041;&#49884;&#50640; . 하이퍼 파라미터? 머신러닝 알고리즘을 구성하는 주요 구성 요소이며, 이 값을 조정해 알고리즘의 예측 성능을 개선할 수 있음 | . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import GridSearchCV # 데이터를 로딩하고 학습 데이터와 테스트 데이터 분리 iris_data = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data,iris_data.target, test_size=0.2, random_state=121) dtree= DecisionTreeClassifier() # 파라미터를 딕셔너리 형태로 설정 parameters = {&#39;max_depth&#39; : [1,2,3], &#39;min_samples_split&#39; : [2,3]} import pandas as pd # param_grid의 하이퍼 파라미터를 3개의 train, test set fold로 나누어 테스트 수행 설정 # rifit=True가 default이며, 이때 가장 젛은 파라미터 설정으로 재학습시킴 grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True) # 붓꽃 학습 데이터로 param_grid의 하이퍼 파라미터를 순차적으로 학습/평가 grid_dtree.fit(X_train,y_train) #GridSearchCV 결과를 추출해 DataFrame으로 변환 scores_df = pd.DataFrame(grid_dtree.cv_results_) scores_df[[&#39;params&#39;,&#39;mean_test_score&#39;,&#39;rank_test_score&#39;,&#39;split0_test_score&#39;,&#39;split1_test_score&#39;,&#39;split2_test_score&#39;]] . params mean_test_score rank_test_score split0_test_score split1_test_score split2_test_score . 0 {&#39;max_depth&#39;: 1, &#39;min_samples_split&#39;: 2} | 0.700000 | 5 | 0.700 | 0.7 | 0.70 | . 1 {&#39;max_depth&#39;: 1, &#39;min_samples_split&#39;: 3} | 0.700000 | 5 | 0.700 | 0.7 | 0.70 | . 2 {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 2} | 0.958333 | 3 | 0.925 | 1.0 | 0.95 | . 3 {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 3} | 0.958333 | 3 | 0.925 | 1.0 | 0.95 | . 4 {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2} | 0.975000 | 1 | 0.975 | 1.0 | 0.95 | . 5 {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 3} | 0.975000 | 1 | 0.975 | 1.0 | 0.95 | . print(&#39;GridSearchCV 최적 파라미터:&#39;, grid_dtree.best_params_) print(&#39;GridSearchCV 최고 정확도:{:4f}&#39;.format(grid_dtree.best_score_)) . GridSearchCV 최적 파라미터: {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2} GridSearchCV 최고 정확도:0.975000 . 인덱스 4,5rk rank_test_score가 1인 것으로 보아 공동 1위이며 예측 성능 1등을 의미함. | 열 4,5,6은 cv=3 이라서 열2는 그 세개의 평균을 의미 | . estimator = grid_dtree.best_estimator_ # GridSearchCV의 best_estimator_는 이미 최적 학습이 됐으므로 별도 학습이 필요없음 pred = estimator.predict(X_test) print(&#39;테스트 데이터 세트 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 테스트 데이터 세트 정확도: 0.9667 . 일반적으로 학습 데이터를 GridSearchCV를 이용해 최적 하이퍼 파라미터 튜닝을 수행한 뒤에 별도의 테스트 세트에서 이를 평가하는 것이 일반적인 머신 러닝 모델 적용 방법이다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/01/intro.html",
            "relUrl": "/2022/01/01/intro.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "2021/12/31/FRI",
            "content": "Scikit-Learn :파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리 $ to$ 머신러닝을 위한 다양한 알고리즘과 편리한 프레임 워크, API를 제공 . import sklearn . &#48531;&#44867; &#54408;&#51333; &#50696;&#52769;&#54616;&#44592; . 붓꽃 데이터 세트로 붓꽃의 품종을 분류 . | 분류(Classification)는 대표적인 지도학습(Supervised Learning) 방법 중 하나 . | 지도학습은 학습을 위한 다양한 feature와 분류 결정값인 레이블 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터 세트에서 미지의 레이블을 예측 . | 학습을 위해 주어진 데이터 세트를 학습 데이터 세트, 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세트를 테스트 데이터 세트라 함 . | . . sklearn.datasets내의 모듈은 사이킷런에서 자체적으로 제공하는 데이터 세트를 생성하는 모듈의 모임 . | sklearn.tree내의 모듈은 트리 기반 ML 알고리즘을 구현한 클래스의 모임 . | sklearn.model_selection은 학습 데이터와 검증 데이터, 예측 데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가하기 위한 다양한 모듈의 모임 . | 하이퍼 파라미터 : 머신 러닝 알고리즘별로 최적의 학습을 위해 직접 입력하는 파라미터를 통칭, 하이퍼 파라미터를 통해 머신 러닝 알고리즘의 성능을 튜닝할 수 있다. . | . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split import pandas as pd . iris = load_iris() # 붓꽃 데이터 세트 로딩 . iris.keys() # 따라서 iris.키이름 또는 iris[&#39;키이름&#39;]을 통해 해당 값들을 확인할 수 있다. . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;, &#39;data_module&#39;]) . iris_data = iris.data # iris 데이터 세트에서 feature만으로 구성된 데이터를 numpy로 로딩 . iris_label = iris.target # 데이터 세트에서 레이블(결정값) 데이터를 numpy로 로딩 . iris.target_names . array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) . 붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환 | . iris.feature_names . [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] . iris_df = pd.DataFrame(data=iris_data,columns=iris.feature_names) . iris_df . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . ... ... | ... | ... | ... | . 145 6.7 | 3.0 | 5.2 | 2.3 | . 146 6.3 | 2.5 | 5.0 | 1.9 | . 147 6.5 | 3.0 | 5.2 | 2.0 | . 148 6.2 | 3.4 | 5.4 | 2.3 | . 149 5.9 | 3.0 | 5.1 | 1.8 | . 150 rows × 4 columns . iris_df[&#39;label&#39;]=iris_label . iris_df.head(3) . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) label . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . feature : columns를 의미함 | label(결정값) : 0,1,2 세 가지 값응로 돼 있으며 순서대로 Setosa,versicolor,virginica 품종을 의미 | . . 학습용 데이터와 테스트용 데이터를 분리해보자 | . 학습용 데이터와 테스트용 데이터는 반드시 분리해야 함, 이를 위해 Scikit-learn에선 train_test_split() API를 제공, 해당 API를 이용하면 학습 데이터와 테스트 데이터를 test_size 파라미터 입력 값의 비율로 쉽게 분할함 | . 예를 들어보자, teat_size=0.2로 입력 파라미터를 설정하면 전체 데이터 중 테스트 데이터가 20%, 학습 데이터가 80%로 데이터를 분할함 | . X_train,X_test,y_train,y_test=train_test_split(iris_data,iris_label,test_size=0.2, random_state=11) . train_test_split의 첫 번째 파라미터인 iris_data는 feature 데이터 세트이며, 두 번째 파라미터인 iris_label은 Label 데이터 세트. random_state=11은 호출할 때마다 같은 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 발생 값. . | train_test_split은 호출 시 무작위로 데이터를 분리하므로 random_state를 지정하지 않으면 수행할 때마다 다른 학습/테스트 용 데이터를 만듦. . | X_train,X_test,y_train,y_test = 학습용 feature데이터 세트, 테스트용 feature데이터 세트, 학습용 레이블 데이터 세트, 테스트용 레이블 데이터 세트를 의미 . | . . 이제 이 데이터를 기반으로 머신 러닝 분류 알고리즘의 하나인 의사 결정 트리를 이용해 학습과 예측을 수행 . dt_clf=DecisionTreeClassifier(random_state=11) . dt_clf.fit(X_train,y_train) # 학습용 feature 데이터 세트와 학습용 레이블 데이터 세트를 입력해 학습 수행중 . DecisionTreeClassifier(random_state=11) . 학습 완료/ 예측을 수행해야하는데 학습 데이터가 아닌 다른 데이터를 이용해야 하며, 일반적으로 테스트 데이터 세트를 이용함 | . pred=dt_clf.predict(X_test) # 예측 수행 중 # 예측 label data set . 예측 성능 평가, 여러 평가 방법 중 정확도를 측정해보자. 정확도는 예측 결과가 실제 레이블 값과 얼마나 정확하게 맞는지를 평가하는 지표 | . from sklearn.metrics import accuracy_score print(&#39;예측 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 예측 정확도: 0.9333 . 학습한 의사 결정 트리의 알고리즘 예측 정확도가 약 93.33% | . . Conclusion . 1) 데이터 세트 분리 : 데이터를 학습 데이터와 테스트 데이터로 분리 2) 모델 학습 : 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습 3) 예측 수행 : 학습된 ML 모델을 이용해 테스트 데이터의 분류(즉, 붓꽃 종류)를 예측 4) 평가 : 이렇게 예측된 결과값과 테스트 데이터의 실제 결과값을 비교해 ML 모델 성능을 평가 . . 간단한 실습을 해보았으니 전체적 틀을 review해보자 . ML 모델 학습을 위해서 fit(), 학습된 모델의 예측을 위해 predict()를 사용 | Scikit Learn에서는 분류 알고리즘을 구현한 클래스를 Classifier로, 그리고 회귀 알고리즘을 구현한 클래스를 Regressor로 지칭 | Classifier와 Regressor를 합쳐서 Estimator 클래스라고 부름. 즉, 지도학습의 모든 알고리즘을 구현한 클래스를 통칭해서 Estimator라고 부름 | . Scikit-Learn의 다양한 모듈은 교재 94p,95p 참고 . 머신러닝 모델을 구축하는 주요 프로세스 = feature의 가공, 변경, 추출을 수행하는 feature processing, ML 알고리즙 학습/예측 수행, 그리고 모델 평가 단계를 반복적으로 수행하는 것 | . 사이킷런에 내장되어 있는 데이터 세트는 일반적으로 dict형태 | 이때 key는 data(feature의 데이터 세트),target(레이블 값, 숫자 결과값 데이터 세트),target_names(개별 레이블의 이름), feature_names(feature의 이름), DESCR 데이터 세트에 대한 설명과 각 feature의 설명 . | 앞에 두 key는 ndarray, 다음 두개는 ndarray 또는 list 그 다음은 str . | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2021/12/31/intro.html",
            "relUrl": "/2021/12/31/intro.html",
            "date": " • Dec 31, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "2021/12/30/THU",
            "content": "&#45936;&#51060;&#53552;&#50640; &#47784;&#45944;&#51012; &#47582;&#52632;&#45796; . 애플리케이션을 수정하지 않고도 데이터를 기반으로 패턴을 학습하고 결과를 예측하는 알고리즘 기법을 통칭 | . 데이터 기반으로 통계적인 신뢰도를 강화하고 예측 오류를 최소화하기 위한 다양한 수학적 기법을 적용해 데이터 내의 패턴을 스스로 인지하고 신뢰도 있는 예측 결과를 도출 | . 데이터를 관통하는 패턴을 학습, 이에 기반한 예측 수행 | . 모델을 맞춘다? 모델을 학습시키는 기법들에는 딥러닝, 나이브베이즈, 디시전트리 등이 있음, 모델을 맞추는 행위를 하지 않으면 이 모델은 어떤 문제도 해결할 수가 없음, 따라서 데이터를 모델에 맞추는 행위가 필요함 | . 따라서 이 데이터에 문제를 최대한 많이 맞출 수 있도록 모델을 최적화하여야 하는데 이렇게 최적화가 된 모델이 그 데이터에 해당된 문제를 해결할 수 있게 됨 | . 분류 :지도 학습(Supervised Learning), 비지도 학습(Un-supervised Learning), 강화 학습(Reinforcement Learning) 지도학습 $ to$ 분류(Classification)와 회귀(Regression)로 나눌 수 있음 . . data? Garbage In $ to$ Garbage Out : data도 질이 중요하다 | . 데이터를 이해하고 효율적으로 가공,처리,추출하여 최적의 데이터를 기반으로 알고리즘을 구동할 수 있도록 준비하는 능력 필요 | . . 만약 온도, 습도, 풍속을 정리해놓은 데이터가 있을 때 눈이 오는 여부를 다양한 기법으로 해결할 수 있음 . 1) 조건을 정해서 해결한다.(decision tree 등) . 2) 수식(가중치)으로 해결한다.(선형 회귀, 딥러닝 등) . 이러한 접근 방식을 통해서 가지고 있는 데이터를 50%정도만 해결했다면 이 접근 방식들은 썩 좋지 않은 방식일 것임 . 따라서 머신러닝을 학습한다는 것은 이 정답을 최대한 맞출 수 있도록 모델을 최대한 최적화한다는 의미임. 이렇게 가장 좋은 성능이 나올 수 있는 식과 조건을 찾아나가는 것을 기계가 스스로 하는 것을 머신러닝이라고 생각할 수도 있겠음. . . 딥러닝 등 머신러닝 기법들 전반적으로 공부할 필요가 있다 . 딥러닝 : 자연어와 이미지 처리에 강하다. 그렇지만 다른 과업처리에 있어서도 항상 우수한 결과를 도출해내는 것은 아니다. 대표적으로 KAGGLE에 있는 TITANIC자료에서 실제로 산 승객과 죽은 승객을 처리해내는 과업을 수행할 땐 딥러닝보다 머신러닝의 모델이 더 좋은 결과를 도출해냈음 | . &#44208;&#44397; &#47785;&#51201;&#51008; &#45936;&#51060;&#53552;&#47484; &#47784;&#45944;&#50640; &#52572;&#51201;&#54868; &#49884;&#53412;&#45716; &#44163;&#51060;&#45796; . 머신러닝 논문에 머신러닝 기법들간에 성능을 비교한 표도 있음. 즉, 머신러닝 기법들은 다양한 기법들이 있기 때문에 그 것들간의 차이점과, 각각의 알고리즘이 무엇을 최적화하려는 것인지의 관점에서 이해해보아야함 . . . &#54028;&#51060;&#50028; &#47672;&#49888;&#47084;&#45789; &#49373;&#53468;&#44228;&#47484; &#44396;&#49457;&#54616;&#45716; &#51452;&#50836; &#54056;&#53412;&#51648; . 머신러닝 패키지 : Scikit-Learn . | 행렬/ 선형대수/ 통계 패키지 : Numpy, SciPy . | 데이터 핸들링 : Pandas(Numpy는 행렬 기반의 데이터 처리에 특화) $ to$ 2차원 데이터 처리에 특화,Matplotlib . | 시각화 : matplotlib(너무 세분화 되어 있어서 익히기 어려움, 시각적인 면에서도 투박),Seaborn(matplotlib의 대안이 될 것) . | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2021/12/28/intro.html",
            "relUrl": "/2021/12/28/intro.html",
            "date": " • Dec 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rhkrehtjd.github.io/INTROml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rhkrehtjd.github.io/INTROml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}