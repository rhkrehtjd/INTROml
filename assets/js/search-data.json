{
  
    
        "post0": {
            "title": "2022/01/05/WED",
            "content": "지도학습은 레이블, 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식이다. 대표적인 유형으로는 분류(Classification)가 있으며 분류는 학습 데이터로 주어진 데이터의 feature와 레이블값(결정 값, 클래스 값)을 머신 러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것이다. 즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것이다. | . 분류는 다양한 머신러닝 알고리즘으로 구현할 수 있다. 베이즈 통계와 생성 모델에 기반한 나이브 베이즈 | 독립 변수와 종속 변수의 선형 관계성에 기반한 로지스틱 회귀 | 데이터 균일도에 따른 규칙 기반의 결정 트리 | 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신 | 근접 거리를 기준으로 하는 최소 근접 알고리즘 | 심층 연결 기반의 신경망 | 서로 다른 (또는 같은) 머신 러닝 알고리즘을 결합한 앙상블 | . | . 그 중 앙상블에 대해 배워보자 . | 앙상블에는 서로 다른 또는 서로 동일한 알고리즘을 단순히 결합한 형태도 있으나, 일반적으로는 배깅(Bagging)과 부스팅(Boosting) 방식으로 나뉜다. . | 앙상블은 서로 다른 또는 서로 동일한 알고리즘을 결합한다고 했는데 대부분은 동일한 알고리즘을 결합한다. . | . &#44208;&#51221;&#53944;&#47532; . :데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 것이다. : 스무고개 게임과 유사하며 룰 기반의 프로그램에 적용되는 if/else를 자동으로 찾아내 예측을 위한 규칙을 만드는 알고리즘을 이해하면 된다. : 따라서 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우한다. : 규칙 노드로 표시된 노드는 규칙 조건이 되는 것이며, 리프 노드로 표시된 노드는 결절된 라벨 값 즉 클래스 값을 의미한다. : 새러운 규칙 조건 마다 서브 트리가 생성된다. : 데이터 세트에 featrue가 있고 이러한 feature가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어진다. : 하지만 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 얘기이며 이는 곧 과적합으로 이어지기 쉽다. : 즉 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높다. : 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙 필요 . 위에서 분류의 다양한 머신 러닝 알고리즘 중 결정 트리는 데이터 균일도에 따른 규칙 기반이라고 했다. 그렇다면 데이터 균일도는 무엇일까? 균일도라 하면 여러 자료가 균등하게 분포되어 있을수록 균일도가 높다고 착각할 수 있는데 그렇지 않다. 예를 들어 검은 공과 흰공이 있을 때 섞여있을 수록 균일도는 낮은 것이며 오로지 검은공으로만 이루어질 수록 균일도가 높다고 할 수 있다. | . | . 만약 눈을 감고 세 주머니에서 공을 뽑을 때 오로지 검은 공으로만 이루어진 주머니에서 공을 뽑을 때 우리는 검은 공을 쉽게 예측할 수 있다. 만약 검은공과 흰공이 섞여있는 혼잡도가 높고 균일도가 낮은 주머니에서 공을 뽑을 때 검은 공을 예측하려면 더 많은 정보가 필요로 할 것이다. | . 결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만든다. 예를 들어보자, 박스 안에 서른 개의 레고 블록이 있는데 각 레고 블록은 형태 속성으로 동그라미, 네모, 세모 색깔 속성으로 노랑, 빨강, 파랑이 있다. 이 중 노랑색 블록의 경우 모두 동그라미로 구성되고 빨강과 파랑의 경우 동그라미, 네모, 세모가 골고루 섞여 있다고 한다면 각 레고 블록을 형태와 색깔 속성으로 분류하고자 할 떄 가장 첫 번째로 만들어져야 하는 규칙 조건은 if 색깔==&#39;노란색&#39;이 될것이다. 왜냐하면 노란색 블록이면 모두 노란 동그라미 블록으로 가장 쉽게 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도 조건을 찾아 분류하는것이 가장 효율적인 분류 방식이기 때문이다. | . | . 이러한 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 정보 이득($Information$ $Gain$)지수와 지니 계수가 있다. . | 정보 이득 지수 : 1 - 엔트로피 지수(주어진 데이터 집합의 혼잡도) $ to$ 결정 트리는 이 정보 이득 지수로 분할 기준을 정한다. 정보 이득이 높은 속성을 기준으로 분할한다. . | 지니 계수 : 낮을수록 데이터 균일도가 높은 것으로 해석햐 지니 계수가 낮은 속성을 기준으로 분할한다. . | . 결정트리의 일반적인 알고리즘은 데이터 세트를 분할하는 데 가장 좋은 조건, 즉 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정한다. . - 결정 트리의 가장 큰 단점은 과적합으로 적합도가 떨어진다. 트리의 깊이가 너무 깊어지면 깊어질수록 예측도는 낮아지기에 사전에 트리의 크기를 제한하는 것이 오히려 성능 튜닝에 더 도움이 된다. . 사이킷런은 결정 트리 알고리즘을 구현한 DecisionTreeClassifier(분류를 위한 클래스)와 DecisionTreeRegressor(회귀를 위한 클래스) 클래스를 제공한다. . 두 클래스 모두 동일한 파라미터를 사용하며 파라미터에 대한 설명은 188~189p 참고 . from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import warnings warnings.filterwarnings(&#39;ignore&#39;) # DecisionTree Classifier 생성 dt_clf = DecisionTreeClassifier(random_state=156) # 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리 iris_data = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=.2, random_state=11) # DecisionTreeClassifier 학습 dt_clf.fit(X_train, y_train) from sklearn.tree import export_graphviz # export_graphviz()의 호출결과로 out_file로 지정된 tree.dot 파일을 생성함 export_graphviz(dt_clf, out_file=&quot;tree.dot&quot;, class_names=iris_data.target_names, feature_names= iris_data.feature_names, impurity=True,filled=True) import graphviz # 위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북상에서 시각화 with open(&quot;tree.dot&quot;) as f: dot_graph = f.read() graphviz.Source(dot_graph) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 petal length (cm) &lt;= 2.45 gini = 0.667 samples = 120 value = [41, 40, 39] class = setosa 1 gini = 0.0 samples = 41 value = [41, 0, 0] class = setosa 0&#45;&gt;1 True 2 petal width (cm) &lt;= 1.55 gini = 0.5 samples = 79 value = [0, 40, 39] class = versicolor 0&#45;&gt;2 False 3 petal length (cm) &lt;= 5.25 gini = 0.051 samples = 38 value = [0, 37, 1] class = versicolor 2&#45;&gt;3 6 petal width (cm) &lt;= 1.75 gini = 0.136 samples = 41 value = [0, 3, 38] class = virginica 2&#45;&gt;6 4 gini = 0.0 samples = 37 value = [0, 37, 0] class = versicolor 3&#45;&gt;4 5 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 3&#45;&gt;5 7 sepal length (cm) &lt;= 5.45 gini = 0.5 samples = 4 value = [0, 2, 2] class = versicolor 6&#45;&gt;7 12 petal length (cm) &lt;= 4.85 gini = 0.053 samples = 37 value = [0, 1, 36] class = virginica 6&#45;&gt;12 8 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 7&#45;&gt;8 9 petal length (cm) &lt;= 5.45 gini = 0.444 samples = 3 value = [0, 2, 1] class = versicolor 7&#45;&gt;9 10 gini = 0.0 samples = 2 value = [0, 2, 0] class = versicolor 9&#45;&gt;10 11 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 9&#45;&gt;11 13 sepal length (cm) &lt;= 5.95 gini = 0.444 samples = 3 value = [0, 1, 2] class = virginica 12&#45;&gt;13 16 gini = 0.0 samples = 34 value = [0, 0, 34] class = virginica 12&#45;&gt;16 14 gini = 0.0 samples = 1 value = [0, 1, 0] class = versicolor 13&#45;&gt;14 15 gini = 0.0 samples = 2 value = [0, 0, 2] class = virginica 13&#45;&gt;15 더 이상 자식 노드가 없는 노드는 리프 노드이다. 리프 노드는 최종 클래스(레이블) 값이 결정되는 노드이다. . | 리프 노드가 되려면 오직 하나의 클래스 값으로 최종 데이터가 구성되거나 리프 노드가 될 수 있는 하이퍼 파라미터 조건을 충족하면 된다. . | 자식 노드가 있는 노드는 브랜치 노드이며 자식 노드를 만들기 위한 분할 규칙 조건을 가지고 있다. . | . 루트 노드인 가장 상위 1번 노드를 설명해보자면, smaples :전체 데이터가 120개라는 의미, value의 값 각각은 레이블 0,1,2값이 가지는 데이터 수를 의미함, gini는 지니 계수, class=setosa는 하위 노드를 가질 경우에 setosa의 개수가 41개로 제일 많다는 의미임 . 이처럼 결정 트리는 규칙 생성 로직을 미리 제어하지 않으면 완벽하게 클래스 값을 구별해내기 위해 트리 노드를 계속해서 만들어간다. 이로 인해 결국 매우 복잡한 규칙 트리가 만들어져 모델이 쉽게 과적합되는 문제점을 갖게 됨, 이러한 이유로 결정 트리는 과적합이 상당히 높은 ML알고리즘이다. 이때문에 결정트리 알고리즘을 제어하는 대부분 하이퍼 파라미터는 복잡한 트리가 생성되는 것을 막기 위한 용도이다. . 따라서 결정 트리의 max_depth 하이퍼 파라미터를 제한 없음에서 3개로 설정하면 더 간단한 결정 트리가 된다. . $+$ 결정트리의 또 다른 하이퍼 파라미터 요소인 min_samples_split을 4로 설정하면 서로 다른 클래스가 혼재해 있더라도 데이터 개수가 4보다 낮아지면 더이상 Split하지 않는다. . $+$ 마지막으로 min_samples_leaf 하이퍼 파라미터 변경에 따른 결정 트리의 변화를 살펴보자, 더 이상 자식 노드가 없는 리프 노드는 클래스 결정 값이 되는데, min_samples_leaf는 이 리프 노드가 될 수 있는 샘플 데이터 건수의 최솟값을 지정함 . . 결정 트리는 균일도에 기반해 어떠한 속성을 규칙 조건으로 선택하느냐가 중요한 요건이다 . 사이킷런은 결정 트리 알고리즘이 학습을 통해 규칙을 정하는 데 있어 feature의 중요한 역할 지표를 DecisionTreeClassifier 객체의 featureimportances 속성으로 제공한다. . | 해보자 . | . import seaborn as sns import numpy as np # feature importance 추출 print(&#39;Feature importance: n{}&#39;.format(np.round(dt_clf.feature_importances_,3))) # feature 별 importance 매핑 for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_): print(&#39;{}:{:.3f}&#39;.format(name,value)) # feature importance를 column 별로 시각화 해보자 sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names) . Feature importance: [0.025 0. 0.555 0.42 ] sepal length (cm):0.025 sepal width (cm):0.000 petal length (cm):0.555 petal width (cm):0.420 . &lt;AxesSubplot:&gt; . 이들 중 petal length가 가장 feature importance가 높음을 알 수 있다. . &#44208;&#51221; &#53944;&#47532; &#44284;&#51201;&#54633; (Overfitiing) . 결정 트리가 어떻게 학습 데이터를 분할해 예측을 수행하는지와 이로 인한 과적합 문제를 시각화해보자 . # 이 메서드를 이용해 2개의 feature가 3가지 유형의 클래스 값을 가지는 데이터 세트를 만들고 이를 그래프 형태로 시각화하자. from sklearn.datasets import make_classification import matplotlib.pyplot as plt plt.title(&#39;3 Class values with 2 Features Sample data creation&#39;) # 2차원 시각화를 위해서 feature는 2개, 클래스는 3가지 유형의 분류 샘플 데이터 생성 X_features,y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=0) # 그래프 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스 값은 다른 색 plt.scatter(X_features[:,0],X_features[:,1],marker=&#39;o&#39;,c=y_labels,s=25,edgecolors=&#39;k&#39;) . &lt;matplotlib.collections.PathCollection at 0x165b900c700&gt; . x축 y축은 두 개의 feature가 각 나열된 것이며, 3개의 클래스 값 구분은 색으로 하였음 . 이제 결정 트리를 기반으로 학습해보자 . from sklearn.tree import DecisionTreeClassifier # 특정한 트리 생성 제약 없는 결정 트리의 학습과 결정 경계 시각화 dt_clf=DecisionTreeClassifier().fit(X_features, y_labels) # Classifier의 Decision Boundary를 시각화 하는 함수- 이건 몰라도 될듯 def visualize_boundary(model, X, y): fig,ax = plt.subplots() # 학습 데이타 scatter plot으로 나타내기 ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap=&#39;rainbow&#39;, edgecolor=&#39;k&#39;, clim=(y.min(), y.max()), zorder=3) ax.axis(&#39;tight&#39;) ax.axis(&#39;off&#39;) xlim_start , xlim_end = ax.get_xlim() ylim_start , ylim_end = ax.get_ylim() # 호출 파라미터로 들어온 training 데이타로 model 학습 . model.fit(X, y) # meshgrid 형태인 모든 좌표값으로 예측 수행. xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) # contourf() 를 이용하여 class boundary 를 visualization 수행. n_classes = len(np.unique(y)) contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap=&#39;rainbow&#39;, clim=(y.min(), y.max()), zorder=1) visualize_boundary(dt_clf, X_features,y_labels) . 이번에는 min_samples_leaf=6을 설정해 6개 이하의 데이터는 리프 노드를 생성할 수 잇도록 리프 노트 생성 규칙을 완화한 뒤 하이퍼 파라미터를 변경해 어떻게 결정 기준 경계가 변하는지 살펴보자 . dt_clf = DecisionTreeClassifier(min_samples_leaf=6).fit(X_features, y_labels) visualize_boundary(dt_clf, X_features, y_labels) . 이상치에 크게 반응하지 않으면서 좀 더 일반화 된 분류 규칙에 따라 분류됐음을 알 수 있다. .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/05/intro.html",
            "relUrl": "/2022/01/05/intro.html",
            "date": " • Jan 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "2022/01/04/TUE",
            "content": "F1 &#49828;&#53076;&#50612; . - 정밀도와 재현율이 어느 한 쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가진다. . $F1 = 2 * frac{precision * recall}{precision + recall}$ . API : f1_score() . (168~169p 코드 참고) . . ROC &#44257;&#49440;&#44284; AUC &#49828;&#53076;&#50612; . - 이진 분류의 예측 성능 측정에서 중요하게 사용됨 . ROC 곡선은 FPR을 0부터 1까지 변경하면서 TPR의 변화 값을 구함. 분류 결정 임계값을 변경하면 FPR을 0부터 1까지 변경할 수 있음. FPR을 0으로 만드려면 임계값을 1로 지정하면 된다. . API : roc_curve() . 171~172,174p 참고 . 일반적으로 ROC 곡선 자체는 FPR과 TPR의 변화값을 보는 데 이용하며 분류의 성능 지표로 사용되는 것을 ROC 곡선 면적에 기반한 AUC 값으로 결정한다. AUC가 1에 가까울수록 좋은 수치이다. AUC 수치가 커지려면 FPR이 작은 상태에서 얼마나 더 큰 TPR을 얻을 수 있느냐가 관건이다. | . . &#54588;&#47560; &#51064;&#46356;&#50616; &#45817;&#45544;&#48337; &#50696;&#52769; . import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression diabetes_data = pd.read_csv(&#39;diabetes.csv&#39;) print(diabetes_data[&#39;Outcome&#39;].value_counts()) diabetes_data.head(3) . 0 500 1 268 Name: Outcome, dtype: int64 . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . Negative : 500, Positive : 268 | . diabetes_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Pregnancies 768 non-null int64 1 Glucose 768 non-null int64 2 BloodPressure 768 non-null int64 3 SkinThickness 768 non-null int64 4 Insulin 768 non-null int64 5 BMI 768 non-null float64 6 DiabetesPedigreeFunction 768 non-null float64 7 Age 768 non-null int64 8 Outcome 768 non-null int64 dtypes: float64(2), int64(7) memory usage: 54.1 KB . null값 없고, feature타입은 모두 숫자형 | 따라서 별도의 feature incoding은 불필요 | . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#47484; &#51060;&#50857;&#54644; &#50696;&#52769; &#47784;&#45944;&#51012; &#49373;&#49457;&#54644;&#48372;&#51088; . # feature 데이터 세트 X, 레이블 데이터 세트 y를 추출 # 맨 끝이 Outcome 칼럼으로서 레이블 값임. 따라서 그 칼럼의 위치인 -1을 이용해 빼줌 X=diabetes_data.iloc[:,:-1] # feature 데이터 세트 y=diabetes_data.iloc[:,-1] # 레이블 데이터 세트 X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state = 156, stratify=y) # 로지스틱 회귀로 학습, 예측 및 평가 lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) pred = lr_clf.predict(X_test) pred_proba=lr_clf.predict_proba(X_test)[:,1] get_clf_eval(y_test, pred, pred_proba) . 재현율 : 59.26%, 전체 데이터의 65%가Negative이므로 정확도보다는 재현율 성능에 조금 더 초점을 맞추기 위해 임곗값별 정밀도와 재현율 값의 변화를 확인해보자 . pred_proba_c1 = lr_clf.predict_proba(X_test)[:,1] precision_recall_curve_plot(y_test,pred_proba_c1) . 임계값 0.42 정도에서 정밀도와 재현율이 어느 정도 균형을 맞춤. 그렇지만 두 지표 모두 0.7이 안 되는 수치로서 아직도 낮은 지표 값임. 임계값을 임의적으로 조정하기 전에 데이터값을 다시 보자 . diabetes_data.describe() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . count 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | . mean 3.845052 | 120.894531 | 69.105469 | 20.536458 | 79.799479 | 31.992578 | 0.471876 | 33.240885 | 0.348958 | . std 3.369578 | 31.972618 | 19.355807 | 15.952218 | 115.244002 | 7.884160 | 0.331329 | 11.760232 | 0.476951 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.078000 | 21.000000 | 0.000000 | . 25% 1.000000 | 99.000000 | 62.000000 | 0.000000 | 0.000000 | 27.300000 | 0.243750 | 24.000000 | 0.000000 | . 50% 3.000000 | 117.000000 | 72.000000 | 23.000000 | 30.500000 | 32.000000 | 0.372500 | 29.000000 | 0.000000 | . 75% 6.000000 | 140.250000 | 80.000000 | 32.000000 | 127.250000 | 36.600000 | 0.626250 | 41.000000 | 1.000000 | . max 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 | 67.100000 | 2.420000 | 81.000000 | 1.000000 | . min행이 0이 굉장히 많음. 예를 들어 Glucose는 포도당 수치인데 0인 것은 말이 안 됨. . plt.hist(diabetes_data[&#39;Glucose&#39;],bins=10) . (array([ 5., 0., 4., 32., 156., 211., 163., 95., 56., 46.]), array([ 0. , 19.9, 39.8, 59.7, 79.6, 99.5, 119.4, 139.3, 159.2, 179.1, 199. ]), &lt;BarContainer object of 10 artists&gt;) . 0 값이 일정 수준 존재함을 알 수 있다. . min() 값이 0으로 돼 있는 feature에 대해 0 값의 건수 및 전체 데이터 건수 대비 및 몇 퍼센트의 비율로 존재하는지 확인해보자. . zero_features=[&#39;Glucose&#39;,&#39;BloodPressure&#39;,&#39;SkinThickness&#39;,&#39;Insulin&#39;,&#39;BMI&#39;] # 전체 데이터 건수 total_count=diabetes_data[&#39;Glucose&#39;].count() #feature별로 반복하면서 데이터 값이 0인 데이터 건수를 추출하고, 퍼센트 계산해보자 for feature in zero_features: zero_count = diabetes_data[diabetes_data[feature]==0][feature].count() print(&#39;{} 0건수는 {}, 퍼센트는 {:.2f}%&#39;.format(feature, zero_count, 100*zero_count/total_count)) . Glucose 0건수는 5, 퍼센트는 0.65% BloodPressure 0건수는 35, 퍼센트는 4.56% SkinThickness 0건수는 227, 퍼센트는 29.56% Insulin 0건수는 374, 퍼센트는 48.70% BMI 0건수는 11, 퍼센트는 1.43% . SkimThickness와 Insulin의 0 값은 각각 전체의 약 30,50%로 대단히 많다. 전체 데이터 건수가 약 800개 이므로 많지 않음, 따라서 0 데이터를 일괄적으로 삭제할 경우에는 학습을 효과적으로 수행하기 어렵다. 따라서 위 feature의 0값을 평균으로 대체해보자 | . mean_zero_features = diabetes_data[zero_features].mean() diabetes_data[zero_features]=diabetes_data[zero_features].replace(0,mean_zero_features) . 로지스틱 회귀의 경우 일반적으로 숫자 데이터에 스케일링을 적용하는 것이 좋음. 따라서 0값을 평균값으로 대체한 데이터 세트에 feature 스케일링을 적용해 변환해보자 . X=diabetes_data.iloc[:,:-1] y=diabetes_data.iloc[:,-1] # StandardScaler 클래스를 이용해 feature 데이터 세트에 일괄적으로 스케일링 적용 scaler = StandardScaler() X_scaled = scaler.fit_transform(X) X_train,X_test,y_train,y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=156,stratify=y) # 로지스틱 회귀로 학습, 예측 및 평가 수행 lr_clf=LogisticRegression() lr_clf.fit(X_train,y_train) pred = lr_clf.predict(X_test) pred_proba=lr_clf.predict_proba(X_test)[:,1] get_clf_eval(y_test,pred,pred_proba) . 데이터 변환과 스케일링을 통해 성능 수치가 일정 수준 개선됐지만 재현율 수치는 아직 개선이 더 필요, 분류 결정 임계값을 0.3에서 0.5까지 0.03씩 변화시키면서 재현율과 다른 평가 지표의 값 변화를 출력해보자 . thresholds = [0.3,0.33,0.36,0.39,0.42,0.45,0.5] pred_proba = lr_clf.predict_proba(X_test) get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1),thresholds) . 값을 반환했을 때 정확도와 정밀도를 어느 정도 희생하고 재현율을 높이는 데 가장 좋은 임계값은 0.33으로 재현율 값이 0.7963이다. 하지만 정밀도가 매우 저조해져서 극단적 선택임. 임계값 0.48이 전체적인 성능 평가 지표를 유지하면서 재현율을 약간 향상시키는 좋은 임계값으로 보임. . 181p 코드 참고 . &#51648;&#44552;&#44620;&#51648; &#48516;&#47448;&#50640; &#49324;&#50857;&#46104;&#45716; &#51221;&#54869;&#46020;, &#50724;&#52264; &#54665;&#47148;, &#51221;&#48128;&#46020;, &#51116;&#54788;&#50984;, F1 &#49828;&#53076;&#50612;, ROC-AUC&#50752; &#44057;&#51008; &#49457;&#45733; &#54217;&#44032; &#51648;&#54364;&#44032; &#51080;&#50632;&#51020;. .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/04/intro.html",
            "relUrl": "/2022/01/04/intro.html",
            "date": " • Jan 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "2022/01/03/MON",
            "content": "평가 process에 대해 알아보자 . - &#51221;&#54869;&#46020; ? = &#50696;&#52769; &#44208;&#44284;&#44032; &#46041;&#51068;&#54620; &#45936;&#51060;&#53552; &#44148;&#49688; / &#51204;&#52404; &#50696;&#52769; &#45936;&#51060;&#53552; &#44148;&#49688; . - 이진 부류의 경우 데이터 구성에 따라 ML 모델의 성능을 왜곡할 수 있기 때문에 정확도 수치 하나만 가지고 성능을 평가하는 건 위험함 - 그 예를 살펴보자 . from sklearn.base import BaseEstimator import numpy as np class MyDummyClassifier(BaseEstimator): # fit 메서드는 아무것도 학습하지 않음 def fit(self, X, y=None): pass # predict() 메서드는 단순히 Sex feature 1이면 0 그렇지 않으면 1로 예측함 def predict(self, X): pred = np.zeros((X.shape[0],1)) for i in range(X.shape[0]) : if X[&#39;Sex&#39;].iloc[i]==1 : pred[i]=0 else : pred[i]=1 return pred . import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # 원본 데이터를 재로딩, 데이터 가공, 학습 데이터/테스트 데이터 분할 titanic_df= pd.read_csv(&#39;C:/Users/ehfus/Downloads/titanic/train.csv&#39;) y_titanic_df=titanic_df[&#39;Survived&#39;] X_titanic_df=titanic_df.drop(&#39;Survived&#39;,axis=1) X_titanic_df=transform_features(X_titanic_df) # transform_features 함수 정의 안 했기 때문에 에러 발생(140p) x_train,X_test,y_train,y_test = train_test_split(X_titanic_df, y_titanic_df,test_size=.2,random_state=0) # 위에서 생성한 Dummy Classifier를 이용해 학습/예측/평가 수행 myclf=MydummyClassifier() myclf.fit(X_train,y_train) mypredictions=myclf.predict(X_train,y_train) mypredictions = myclf.predict(X_test) print(&#39;Dummy Classifier의 정확도는: {0:.4f}&#39;.format(accuracy_score(y_test,mypredictions))) . Dummy Classifier의 정확도는 0.7877정도 나온다 . 즉 이렇게 단순한 알고리즘으로 예측을 하더라도 데이터의 구성에 따라 정확도의 결과는 약 78%정도로 높은 수치가 나올 수 있음. 따라서 정확도를 지표로 사용할 때는 매우 신중해야 함. . 특히 불균형한 레이블 값 분포에서 ML 모델의 성능을 판단할 경우 적합한 평가 지표가 아님, 예를 들어보자 . from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.base import BaseEstimator from sklearn.metrics import accuracy_score import numpy as np import pandas as pd class MyFakeClassifier(BaseEstimator): def fit(self, X, y): pass # 입력값으로 들어오는 X 데이터 세트의 크기만큼 모두 0값으로 만들어서 반환 def predict(self, X): return np.zeros((len(X),1),dtype=bool) # 사이킷런의 내장 데이터 세트인 load_digits()를 이용해 MNIST 데이터 로딩 digits = load_digits() # digits 번호가 7번이면 True이고 이를 astype(int)로 1로 변환, 7번이 아니면 Fasle이고 0으로 변환. y=(digits.target==7).astype(int) X_train,X_test,y_train,y_test= train_test_split(digits.data,y,random_state=11) # 불균형한 레이블 데이터 분포도 확인 print(&#39;레이블 테스트 세트 크기: &#39;,y_test.shape) print(&#39;테스트 세트 레이블 0과 1의 분포도&#39;) print(pd.Series(y_test).value_counts()) # Dummy Classifier로 학습/예측/정확도 평가 fakeclf=MyFakeClassifier() fakeclf.fit(X_train,y_train) fakepred=fakeclf.predict(X_test) print(&#39;모든 예측을 0으로 하여도 정확도는:{:.3f}&#39;.format(accuracy_score(y_test,fakepred))) . 레이블 테스트 세트 크기: (450,) 테스트 세트 레이블 0과 1의 분포도 0 405 1 45 dtype: int64 모든 예측을 0으로 하여도 정확도는:0.900 . 이처럼 정확도 평가 지표는 불균형한 레이블 데이터 세트에서는 성능 수치로 사용 되어서는 안 된다. 여러가지 분류 지표와 함께 이용하자 | . - &#50724;&#52264;&#54665;&#47148; &#46608;&#45716; &#54844;&#46041;&#54665;&#47148; ? . - 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리고 있는지도 함께 보여주는 지표 - TN,FP,FN,TP로 나뉘며, 앞문자는 예측값과 실제값이 &#39;같은가/ 틀린가&#39;를 의미, 뒤 문자는 예측 결과 값이 부정(0)/긍정(1)을 의미 - 이 값을 조합해 정확도, 정밀도, 재현율 값을 알 수 있음 . from sklearn.metrics import confusion_matrix confusion_matrix(y_test, fakepred) . array([[405, 0], [ 45, 0]], dtype=int64) . TN은 405개, FN은 45개이다. | . 정확도 = 예측 결과와 실제 값이 동일한 건수/전체 데이터 수 = (TN + TP) / (TN + FP + FN + TP) | 정밀도 = TP / (FP + TP) | 재현율 = TP = (FN + TP) | . (158p 참고) . 분류하려는 업무의 특성상 정밀도 또는 재현율이 특별히 강조되어야 할 경우 분류의 결정 임계값(Threshold)을 조정해 정밀도 또는 재현율의 수치를 높일 수 있다. . 개별 데이터 별로 예측 확률을 반환하는 메서드 = predict_proba() predict_proba() 메서드는 학습이 완료된 사이킷런 Classifier 객체에서 호출이 가능하며 테스트 feature 데이터 세트를 파라미터로 입력해주면 테스트 feature 레코드의 개별 클래스 예측 확률을 반환함. predict() 메서드와 유사하지만 단지 반환 결과가 예측 결과 클래스값이 아닌 예측 확률 결과이다. . (160p 참고) . threshold 값을 조정해보자 | . from sklearn.preprocessing import Binarizer X=[[1,-1,2], [2,0,0], [0,1.1,1.2]] # X의 개별 원소들이 threshold 값보다 같거나 작으면 0을 크면 1을 반환 binarizer = Binarizer(threshold=1.1) print(binarizer.fit_transform(X)) . [[0. 0. 1.] [1. 0. 0.] [0. 0. 1.]] . 이제 이 Binarizer를 이용해 사이킷런 predict()의 의사(pseudo)코드를 만들어보자 . from sklearn.preprocessing import Binarizer # Binarizer의 threshold 설정값, 즉 분류 결정 임계값임. custom_threshold=0.5 # predict_proba() 반환값의 두 번째 칼럼, 즉 Positive 클래스 칼럼 하나만 추출해 Binirizer를 적용 pred_proba_1=pred_proba[:,1].reshape(-1,1) binarizer = Binarizer(threshold=cistom_threshold).fit(pred_proba_1) custom_predict = binarizer.transform(pred_proba_1) get_clf_eval(y_test, custom_predict) . 이때 threshold 설정값을 0.4로 설정, 즉 분류 결정 임계값을 낮추면 True값이 많아질 것이고 재현율 값이 올라가고 정밀도는 떨어질 것이다. . 임계값을 0.4에서부터 0.6까지 0.05씩 증가시키며 평가 지표를 조사해보자, 이를 위해 get_eval_by_threshold() 함수를 만듦 . # 테스트를 수행할 모든 임계값을 리스트 객체로 저장 thresholds = [0.4,0.45,0.5,0.55,0.6] def get_eval_by_threshold(y_test,pred_proba_c1,thresholds): # thresholds list 객체 내의 값을 차례로 iteration하면서 Evaluation 수행. for custom_threshold in thresholds: binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) custom_threshold = binarizer.transform(pres_proba_c1) print(&#39;임계값:&#39;,custom_threshold) get_clf_eval(y_test,custom_predict) get_eval_by_threshold(y_test,pred_proba[:,1].reshapep(-1,1), thresholds) . 임계값의 변경은 업무 환경에 맞게 두 개의 수치를 상호 보완할 수 있는 수준에서 적용돼야 한다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/03/intro.html",
            "relUrl": "/2022/01/03/intro.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "2022/01/02/SUN",
            "content": "&#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 데이터 인코딩 - 레이블 인코딩 . | . from sklearn.preprocessing import LabelEncoder items=[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;] # LabelEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행 encoder = LabelEncoder() # 객체 생성 encoder.fit(items) labels=encoder.transform(items) print(&#39;인코딩 변환값: &#39;, labels) print(&#39;-&#39;) print(&#39;인코딩 클래스: &#39;, encoder.classes_) print(&#39;차례대로 0부터 5까지 부여됨&#39;) print(&#39;-&#39;) print(&#39;디코딩 원본값: &#39;,encoder.inverse_transform([0,4,5,5,1,1,2,0])) print(&#39;이렇게 원하는 인코딩 값의 리스트를 통해 디코딩 할 수 있다&#39;) . 인코딩 변환값: [0 1 4 5 3 3 2 2] - 인코딩 클래스: [&#39;TV&#39; &#39;냉장고&#39; &#39;믹서&#39; &#39;선풍기&#39; &#39;전자레인지&#39; &#39;컴퓨터&#39;] 차례대로 0부터 5까지 부여됨 - 디코딩 원본값: [&#39;TV&#39; &#39;전자레인지&#39; &#39;컴퓨터&#39; &#39;컴퓨터&#39; &#39;냉장고&#39; &#39;냉장고&#39; &#39;믹서&#39; &#39;TV&#39;] 이렇게 원하는 인코딩 값의 리스트를 통해 디코딩 할 수 있다 . ***주의*** . 레이블 인코딩이 1,2일때 특정 ML알고리즘에서 가중치가 더 부여되거나 더 중요하게 인식할 가능성이 발생함. 하지만 단순 인코딩 숫자이기에 이러한 현상은 피해야함. 따라서 이러한 레이블 인코딩은 선형 회귀와 같은 ML 알고리즘에는 적용 X, 트리 계열의 알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 레이블 인코딩도 별 문제 X, 원-핫 인코딩은 레이블 인코딩의 이러한 문제점을 해결하기 위한 인코딩 방식임 . . 데이터 인코딩 - 원-핫 인코딩 . | . from sklearn.preprocessing import OneHotEncoder import numpy as np items=[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;] # 먼저 숫자 값으로 변환하기 위해 LabelEncoder로 변환해야함 encoder=LabelEncoder() # 객체 생성 encoder.fit(items) labels=encoder.transform(items) # 꼭 2차원 데이터로 변경해야 함 labels=labels.reshape(-1,1) # 이제 원-핫 인코딩 oh_encoder= OneHotEncoder() # 객체 생성 oh_encoder.fit(labels) oh_labels=oh_encoder.transform(labels) print(&#39;원-핫 인코딩 데이터&#39;) print(oh_labels.toarray()) # array 형태로 print(&#39;원-핫 인코딩 데이터 차원&#39;) print(oh_labels.shape) # 8행 6열의 행렬 형태 . 원-핫 인코딩 데이터 [[1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0.] [0. 0. 0. 1. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0.]] 원-핫 인코딩 데이터 차원 (8, 6) . 이러한 과정을 pandas를 통해 한 번에? = get_dummies . import pandas as pd df=pd.DataFrame({&#39;item&#39;:[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;]}) pd.get_dummies(df) . item_TV item_냉장고 item_믹서 item_선풍기 item_전자레인지 item_컴퓨터 . 0 1 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 1 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 1 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 1 | . 4 0 | 0 | 0 | 1 | 0 | 0 | . 5 0 | 0 | 0 | 1 | 0 | 0 | . 6 0 | 0 | 1 | 0 | 0 | 0 | . 7 0 | 0 | 1 | 0 | 0 | 0 | . get_dummies()를 이용하면 숫자형 값으로 변환 없이도 바로 변환이 가능함 | . . feature &#49828;&#52992;&#51068;&#47553;&#44284; &#51221;&#44508;&#54868; . feature scaling에는 표준화와 정규화가 있으며, 표준화는 feature각각이 평균0, 분산1인 가우시안 정규 분포를 가진 값으로 변환하는 것을 의미 . $x_i _new$ = $ frac{x_i-mean(x)}{stdev(x)}$ 이며 $stdev(x)$는 표준편차를 의미함. . 일반적으로 정규화는 서로 다른 feature의 크기를 통일하기 위해 크기는 변환해주는 개념. 0~1값으로 변환한다. 즉 개별 데이터의 크기를 모두 똑같은 단위로 변경하는 것. . $x_i _new$ = $ frac{x_i-min(x)}{max(x)-min(x)}$ . 주의 . 사이킷런의 전처리에서 제공하는 Normalizer 모듈과 일반적인 정규화는 약간의 차이가 있음. 사이킷런의 Normalizer 모듈은 선형 대수에서의 정규화 개념이 적용 됐으며, 개별 벡터의 크기를 맞추기 위해 변환하는 것을 의미함. . $x_i _new$ = $ frac{x_i}{ sqrt(x_i^2+y_i^2+z_i^2)}$ . 이를 벡터 정규화로 지칭하자 . . StandardScaler :표준화 . from sklearn.datasets import load_iris iris=load_iris() iris_data=iris.data iris_df=pd.DataFrame(data=iris_data,columns=iris.feature_names) print(&#39;feature들의 평균값&#39;) print(iris_df.mean()) print(&#39; nfeature들의 분산값&#39;) print(iris_df.var()) . feature들의 평균값 sepal length (cm) 5.843333 sepal width (cm) 3.057333 petal length (cm) 3.758000 petal width (cm) 1.199333 dtype: float64 feature들의 분산값 sepal length (cm) 0.685694 sepal width (cm) 0.189979 petal length (cm) 3.116278 petal width (cm) 0.581006 dtype: float64 . from sklearn.preprocessing import StandardScaler scaler=StandardScaler() # 객체 생성 scaler.fit(iris_df) iris_scaled=scaler.transform(iris_df) # transform()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DataFrame으로 변환 iris_df_scaled = pd.DataFrame(data=iris_scaled,columns=iris.feature_names) print(&#39;feature들의 평균값&#39;) print(iris_df_scaled.mean()) print(&#39; nfeature들의 분산값&#39;) print(iris_df_scaled.var()) . feature들의 평균값 sepal length (cm) -1.690315e-15 sepal width (cm) -1.842970e-15 petal length (cm) -1.698641e-15 petal width (cm) -1.409243e-15 dtype: float64 feature들의 분산값 sepal length (cm) 1.006711 sepal width (cm) 1.006711 petal length (cm) 1.006711 petal width (cm) 1.006711 dtype: float64 . 두 셀을 비교해보면 위 셀 $ to$ 아래 셀, 모든 칼럼 값의 평균이 0에 아주 가까운 값으로, 그리고 분산은 1에 아주 가까운 값으로 변환됐음을 알 수 있다. | . MinMaxScaler :정규화 . 데이터 값을 0과 1사이의 범위 값으로 변환한다. (음수 값이 있으면 -1에서 1값으로 변환) | 데이터의 분포가 가우시안 분포가 아닐 경우에 적용 가능 | . from sklearn.preprocessing import MinMaxScaler scaler=MinMaxScaler() scaler.fit(iris_df) iris_scaled=scaler.transform(iris_df) # transform()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DF으로 변환 iris_df_scaled = pd.DataFrame(data=iris_scaled,columns=iris.feature_names) print(&#39;feature들의 최솟값&#39;) print(iris_df_scaled.min()) print(&#39; nfeature들의 최댓값&#39;) print(iris_df_scaled.max()) . feature들의 최솟값 sepal length (cm) 0.0 sepal width (cm) 0.0 petal length (cm) 0.0 petal width (cm) 0.0 dtype: float64 feature들의 최댓값 sepal length (cm) 1.0 sepal width (cm) 1.0 petal length (cm) 1.0 petal width (cm) 1.0 dtype: float64 . 모든 feature가 0에서 1사이의 값으로 변환되는 스케일링이 적용됐음을 알 수 있다. | . . 유의점 128p 참고 . 학습 데이터로 fit()이 적용된 스케일링 기준 정보를 그대로 테스트 데이터에 적용해야 하며, 그렇지 않으면 학습 데이터와 테스트 데이터의 스케일링 기준 정보가 서로 달라지기 때문에 올바른 예측 결과를 도출하지 못할 수 있다. . test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform()만으로 변환해야한다. . 즉 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리하던가 이것이 여의치 않다면 테스트 데이터 변환 시에는 fit()이나 fit_transform()을 적용하지 않고 학습 데이터로 이미 fit()된 scaler 객체를 이용해 transform()으로 변환 | . . 131p 사이킷런으로 수행하는 타이타닉 생존자 예측, 꼭 한 번 실습해보기 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/02/intro.html",
            "relUrl": "/2022/01/02/intro.html",
            "date": " • Jan 2, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "2022/01/01/SAT(HappyNewYear)",
            "content": "datail-review 해보자 . feature_names = 높이,가로 길이 이런 것들, data = 각 featuredml 값들, target = 0,1,2...예를 들면 붓꽃의 이름을 대용한 것, target_names = 각 target이 가리키는 이름이 무엇인지? | . . model_selection 모듈은 학습 데이터와 테스트 데이터 세트를 분리하거나 교차 검증 분할 및 평가, 그리고 Estimator의 하이퍼 파라미터를 튜닝하기 위한 다양한 함수와 클래스를 제공, 전체 데이터를 학습 데이터와 테스트 데이터 세트로 분리해주는 train_test_split()부터 살펴보자 . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score iris=load_iris() # 붓꽃 데이터 세트 로딩 dt_clf=DecisionTreeClassifier() train_data=iris.data # 데이터 세트에서 feature만으로 구성된 데이터가 ndarray train_label=iris.target # 데이터 세트에서 label 데이터 dt_clf.fit(train_data, train_label) # 학습 수행중 pred=dt_clf.predict(train_data) # 예측 수행중 // 그런데 학습때 사용했던 train_data를 사용했음 -&gt; 예측도 1 나올 것 print(&#39;예측도: &#39;,accuracy_score(train_label,pred)) . 예측도: 1.0 . 정확도가 100% 나왔음 $ to$ 이미 학습한 학습 데이터 세트를 기반으로 예측했기 때문. 답을 알고 있는데 같은 문제를 낸 것이나 마찬가지 | 따라서 예측을 수행하는 데이터 세트는 학습을 수행한 학습용 데이터 세트가 아닌 전용의 테스트 데이터 세트여야 함. | . from sklearn.model_selection import train_test_split . dt_clf=DecisionTreeClassifier() iris=load_iris() # train_test_split()의 반환값은 튜플 형태이다. 순차적으로 네가지 요소들을 반환한다 X_train,X_test,y_train,y_test=train_test_split(iris.data, iris.target,test_size=0.3,random_state=121) dt_clf.fit(X_train,y_train) pred = dt_clf.predict(X_test) print(&#39;예측 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 예측 정확도: 0.9556 . . 지금까지의 방법은 모델이 학습 데이터에만 과도하게 최적화되어, 실제 예측을 다른 데이터로 수행할 경우에는 예측 성능이 과도하게 떨어지는 과적합이 발생할 수 있다. 즉 해당 테스트 데이터에만 과적합되는 학습 모델이 만들어져 다른 테스트용 데이터가들어올 경우에는 성능이 저하된다. $ to$ 개선하기 위해 교차검증을 이용해 다양한 학습과 평가를 수행해야 한다. . 교차검증? . : 본고사 치르기 전, 여러 모의고사를 치르는 것. 즉 본고사가 테스트 데이터 세트에 대해 평가하는 것이라면 모의고사는 교차 검증에서 많은 학습과 검증 세트에서 알고리즘 학습과 평가를 수행하는 것. . : 학습 데이터 세트를 검증 데이터 세트와 학습 데이터 세트로 분할하여 수행한 뒤, 모든 학습/검증 과정이 완료된 후 최종적으로 성능을 평가하기 위해 테스트 데이터 세트를 마련함. . K fold 교차 검증? . : K개의 데이터 폴드 세트를 만들어서 K번만큼 각 폴드 세트에 학습과 검증, 평가를 반복적으로 수행 / 개괄적 과정은 교재 104 참고 . 실습해보자 | . import numpy as np . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import KFold # 위에서는 trian_test_split을 import했었음 iris=load_iris() # 붓꽃 데이터 세트 로딩 features=iris.data label=iris.target dt_clf=DecisionTreeClassifier(random_state=156) kfold=KFold(n_splits=5) # KFold 객체 생성 cv_accuracy=[] # fold set별 정확도를 담을 리스트 객체 생성 print(&#39;붓꽃 데이터 세트 크기:&#39;,features.shape[0]) . 붓꽃 데이터 세트 크기: 150 . . kfold=KFold(n_splits=5) . 로 KFold객체를 생성했으니 객체의 split()을 호출해 전체 붓꽃 데이터를 5개의 fold 데이터 세트로 분리하자. 붓꽃 데이터 세트 크기가 150개니 120개는 학습용, 30개는 검증 테스트 데이터 세트이다. . n_iter=0 for train_index,test_index in kfold.split(features): # kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출 X_train, X_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] # 학습 및 예측 dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) n_iter+=1 # 반복 시마다 정확도 측정 accuracy = np.round(accuracy_score(y_test,pred),4) train_size = X_train.shape[0] test_size = X_test.shape[0] print(&#39; n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기 :{2}, 검증 데이터 크기 :{3}&#39;.format(n_iter,accuracy,train_size,test_size)) print(&#39;#{0} 검증 세트 인덱스:{1}&#39;.format(n_iter, test_index)) cv_accuracy.append(accuracy) # 개별 iteration별 정확도를 합하여 평균 정확도 계산 print(&#39; n *Conclusion* 평균 검증 정확도:&#39;, np.mean(cv_accuracy)) . #1 교차 검증 정확도 :1.0, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #1 검증 세트 인덱스:[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29] #2 교차 검증 정확도 :0.9667, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #2 검증 세트 인덱스:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59] #3 교차 검증 정확도 :0.8667, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #3 검증 세트 인덱스:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89] #4 교차 검증 정확도 :0.9333, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #4 검증 세트 인덱스:[ 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119] #5 교차 검증 정확도 :0.7333, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #5 검증 세트 인덱스:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] *Conclusion* 평균 검증 정확도: 0.9 . . 교차 검증시마다 검증 세트의 인덱스가 달라짐을 알 수 있다. . | 검증세트 인덱스를 살펴보면 104p에서 설명한 그림의 설명과 유사함 . | . . Stratified K 폴드 . : 불균형한 분포도를가진 레이블(결정 클래스) 데이터 집합을 위한 K 폴드 방식이다. 불균형한 분포도를 가진 레이블 데이터 집합은 특정 레이블 값이 특이하게 많거나 또는 적어서 분포가 한쪽으로 치우치는 것을 말함 . 가령 대출 사기 데이터를 예측한다고 가정해보자, 이 데이터 세트는 1억건이고 수십개의 feature와 대출 사기 여부를 뜻하는 label(정상 대출0, 대출사기 : 1)로 구성돼 있다. K폴드로 랜덤하게 학습 및 테스트 세트의 인덱스를 고르더라도 레이블 값인 0과1의 비율을 제대로 반영하지 못하게 됨. 따라서 원본 데이터와 유사한 대출 사기 레이블 값의 분포를 학습/테스트 세트에도 유지하는 게 매우 중요 . Stratified K 폴드는 이처럼 K폴드가 레이블 데이터 집합이 원본 데이터 집합의 레이블 분포를 학습 및 테스트 세트에 제대로 분배하지 못하는 경우의 문제를 해결해줌 | . 붓꽃 데이터 세트를 DataFrame으로 생성하고 레이블 값의 분포도를 먼저 확인해보자 . import pandas as pd iris=load_iris() iris_df=pd.DataFrame(data=iris.data,columns=iris.feature_names) iris_df[&#39;label&#39;]=iris.target print(iris_df[&#39;label&#39;].value_counts(),&#39; n&#39;) . 0 50 1 50 2 50 Name: label, dtype: int64 . label값은 모두 50개로 분배되어 있음 | . kfold=KFold(n_splits=3) n_iter=0 for train_index, test_index in kfold.split(iris_df): n_iter+=1 label_train = iris_df[&#39;label&#39;].iloc[train_index] label_test=iris_df[&#39;label&#39;].iloc[test_index] print(&#39;## 교차 검증: {}&#39;.format(n_iter)) print(&#39;학습 레이블 데이터 분포: n&#39;, label_train.value_counts()) print(&#39;검증 레이블 데이터 분포: n&#39;, label_test.value_counts()) print(&#39;&#39;) . ## 교차 검증: 1 학습 레이블 데이터 분포: 1 50 2 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 50 Name: label, dtype: int64 ## 교차 검증: 2 학습 레이블 데이터 분포: 0 50 2 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 1 50 Name: label, dtype: int64 ## 교차 검증: 3 학습 레이블 데이터 분포: 0 50 1 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 2 50 Name: label, dtype: int64 . 교차 검증 시마다 3개의 폴드 세트로 만들어지는 학습 레이블과 검증 레이블이 완전히 다른 값으로 추출되었다. 예를 들어 첫번째 교차 검증에서는 학습 레이블의 1,2값이 각각 50개가 추출되었고 검증 레이블의 0값이 50개 추출되었음, 즉 학습레이블은 1,2 밖에 없으므로 0의 경우는 전혀 학습하지 못함. 반대로 검증 레이블은 0밖에 없으므로 학습 모델은 절대 0을 예측하지 못함. 이런 유형으로 교차 검증 데이터 세트를 분할하면 검증 예측 정확도는 0이 될 수밖에 없다. | . StratifiedKFold는 이렇게 KFold로 분할된 레이블 데이터 세트가 전체 레이블 값의 분포도를 반영하지 못하는 문제를 해결함. | . . 실습해보자 . from sklearn.model_selection import StratifiedKFold skf=StratifiedKFold(n_splits=3) n_iter=0 # split 메소드에 인자로 feature데이터 세트뿐만 아니라 레이블 데이터 세트도 반드시 넣어줘야함 for train_index,test_index in skf.split(iris_df,iris_df[&#39;label&#39;]): n_iter+=1 label_train=iris_df[&#39;label&#39;].iloc[train_index] label_test=iris_df[&#39;label&#39;].iloc[test_index] print(&#39;## 교차검증: {}&#39;.format(n_iter)) print(&#39;학습 레이블 데이터 분포: n&#39;, label_train.value_counts()) print(&#39;검증 레이블 데이터 분포: n&#39;, label_test.value_counts()) print(&#39;--&#39;) . ## 교차검증: 1 학습 레이블 데이터 분포: 2 34 0 33 1 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 17 1 17 2 16 Name: label, dtype: int64 -- ## 교차검증: 2 학습 레이블 데이터 분포: 1 34 0 33 2 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 17 2 17 1 16 Name: label, dtype: int64 -- ## 교차검증: 3 학습 레이블 데이터 분포: 0 34 1 33 2 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 1 17 2 17 0 16 Name: label, dtype: int64 -- . 학습 레이블과 검증 레이블 데이터 값의 분포도가 동일하게 할당됐음을 알 수 있다. 이렇게 분할이 되어야 레이블 값 0,1,2를 모두 학습할 수 있고 이에 기반해 검증을 수행할 수 있다. | . 이제 StratifiedKFold를 이용해 붓꽃 데이터를 교차 검증해보자 | . df_clf=DecisionTreeClassifier(random_state=156) skfold=StratifiedKFold(n_splits=3) n_iter=3 cv_accuracy=[] # StratifiedKFol의 split() 호출시 반드시 레이블 데이터 세트도 추가 입력 필요 for train_index, test_ondex in skfold.split(features, label): # split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출 X_train,X_test=features[train_index],features[test_index] y_train,y_test=label[train_index], label[test_index] # 학습 및 예측 df_clf.fit(X_train,y_train) pred=dt_clf.predict(X_test) # 반복시마다 정확도 측정 n_iter+=1 accuracy=np.around(accuracy_score(y_test,pred),4) train_size=X_train.shape[0] test_size = X_test.shape[0] print(&#39; n#{} 교차 검증 정확도 : {}, 학습 데이터 크기 : {}, 검증 데이터 크기 : {}&#39;.format(n_iter,accuracy,train_size,test_size)) print(&#39;#{} 검증 세트 인덱스: {}&#39;.format(n_iter, test_index)) cv_accuracy.append(accuracy) # 교차 검증별 정확도 및 평균 정확도 계산 print(&#39; n## 교차 검증별 정확도:&#39;, np.around(cv_accuracy,4)) print(&#39;## 평균 검증 정확도:&#39;,np.mean(cv_accuracy)) . #4 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #4 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92] ## 평균 검증 정확도: 0.92 #5 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #5 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92 0.92] ## 평균 검증 정확도: 0.92 #6 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #6 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92 0.92 0.92] ## 평균 검증 정확도: 0.92 . . &#44368;&#52264; &#44160;&#51613;&#51012; &#48372;&#45796; &#44036;&#54200;&#54616;&#44172; - cross_val_score() . from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score,cross_validate from sklearn.datasets import load_iris iris_data=load_iris() dt_clf = DecisionTreeClassifier(random_state=156) data= iris_data.data label=iris_data.target # 성능 지표는 정확도 (accuracy), 교차 검증 세트는 3개 scores = cross_val_score(dt_clf, data, label, scoring=&#39;accuracy&#39;, cv=3) print(&#39;교차 검증별 정확도: &#39;,np.round(scores,4)) print(&#39;평균 검증 정확도: &#39;,np.round(np.mean(scores),4)) . 교차 검증별 정확도: [0.98 0.94 0.98] 평균 검증 정확도: 0.9667 . cv로 지정된 횟수만큼 scoring 파라미터로 지정된 평가지표로 평가 결과값을 배열로 반환 | . . GridSearchCV - &#44368;&#52264; &#44160;&#51613;&#44284; &#52572;&#51201; &#54616;&#51060;&#54140; &#54028;&#46972;&#48120;&#53552; &#53916;&#45789;&#51012; &#46041;&#49884;&#50640; . 하이퍼 파라미터? 머신러닝 알고리즘을 구성하는 주요 구성 요소이며, 이 값을 조정해 알고리즘의 예측 성능을 개선할 수 있음 | . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import GridSearchCV # 데이터를 로딩하고 학습 데이터와 테스트 데이터 분리 iris_data = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data,iris_data.target, test_size=0.2, random_state=121) dtree= DecisionTreeClassifier() # 파라미터를 딕셔너리 형태로 설정 parameters = {&#39;max_depth&#39; : [1,2,3], &#39;min_samples_split&#39; : [2,3]} import pandas as pd # param_grid의 하이퍼 파라미터를 3개의 train, test set fold로 나누어 테스트 수행 설정 # rifit=True가 default이며, 이때 가장 젛은 파라미터 설정으로 재학습시킴 grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True) # 붓꽃 학습 데이터로 param_grid의 하이퍼 파라미터를 순차적으로 학습/평가 grid_dtree.fit(X_train,y_train) #GridSearchCV 결과를 추출해 DataFrame으로 변환 scores_df = pd.DataFrame(grid_dtree.cv_results_) scores_df[[&#39;params&#39;,&#39;mean_test_score&#39;,&#39;rank_test_score&#39;,&#39;split0_test_score&#39;,&#39;split1_test_score&#39;,&#39;split2_test_score&#39;]] . params mean_test_score rank_test_score split0_test_score split1_test_score split2_test_score . 0 {&#39;max_depth&#39;: 1, &#39;min_samples_split&#39;: 2} | 0.700000 | 5 | 0.700 | 0.7 | 0.70 | . 1 {&#39;max_depth&#39;: 1, &#39;min_samples_split&#39;: 3} | 0.700000 | 5 | 0.700 | 0.7 | 0.70 | . 2 {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 2} | 0.958333 | 3 | 0.925 | 1.0 | 0.95 | . 3 {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 3} | 0.958333 | 3 | 0.925 | 1.0 | 0.95 | . 4 {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2} | 0.975000 | 1 | 0.975 | 1.0 | 0.95 | . 5 {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 3} | 0.975000 | 1 | 0.975 | 1.0 | 0.95 | . print(&#39;GridSearchCV 최적 파라미터:&#39;, grid_dtree.best_params_) print(&#39;GridSearchCV 최고 정확도:{:4f}&#39;.format(grid_dtree.best_score_)) . GridSearchCV 최적 파라미터: {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2} GridSearchCV 최고 정확도:0.975000 . 인덱스 4,5rk rank_test_score가 1인 것으로 보아 공동 1위이며 예측 성능 1등을 의미함. | 열 4,5,6은 cv=3 이라서 열2는 그 세개의 평균을 의미 | . estimator = grid_dtree.best_estimator_ # GridSearchCV의 best_estimator_는 이미 최적 학습이 됐으므로 별도 학습이 필요없음 pred = estimator.predict(X_test) print(&#39;테스트 데이터 세트 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 테스트 데이터 세트 정확도: 0.9667 . 일반적으로 학습 데이터를 GridSearchCV를 이용해 최적 하이퍼 파라미터 튜닝을 수행한 뒤에 별도의 테스트 세트에서 이를 평가하는 것이 일반적인 머신 러닝 모델 적용 방법이다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/01/intro.html",
            "relUrl": "/2022/01/01/intro.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "2021/12/31/FRI",
            "content": "Scikit-Learn :파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리 $ to$ 머신러닝을 위한 다양한 알고리즘과 편리한 프레임 워크, API를 제공 . import sklearn . &#48531;&#44867; &#54408;&#51333; &#50696;&#52769;&#54616;&#44592; . 붓꽃 데이터 세트로 붓꽃의 품종을 분류 . | 분류(Classification)는 대표적인 지도학습(Supervised Learning) 방법 중 하나 . | 지도학습은 학습을 위한 다양한 feature와 분류 결정값인 레이블 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터 세트에서 미지의 레이블을 예측 . | 학습을 위해 주어진 데이터 세트를 학습 데이터 세트, 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세트를 테스트 데이터 세트라 함 . | . . sklearn.datasets내의 모듈은 사이킷런에서 자체적으로 제공하는 데이터 세트를 생성하는 모듈의 모임 . | sklearn.tree내의 모듈은 트리 기반 ML 알고리즘을 구현한 클래스의 모임 . | sklearn.model_selection은 학습 데이터와 검증 데이터, 예측 데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가하기 위한 다양한 모듈의 모임 . | 하이퍼 파라미터 : 머신 러닝 알고리즘별로 최적의 학습을 위해 직접 입력하는 파라미터를 통칭, 하이퍼 파라미터를 통해 머신 러닝 알고리즘의 성능을 튜닝할 수 있다. . | . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split import pandas as pd . iris = load_iris() # 붓꽃 데이터 세트 로딩 . iris.keys() # 따라서 iris.키이름 또는 iris[&#39;키이름&#39;]을 통해 해당 값들을 확인할 수 있다. . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;, &#39;data_module&#39;]) . iris_data = iris.data # iris 데이터 세트에서 feature만으로 구성된 데이터를 numpy로 로딩 . iris_label = iris.target # 데이터 세트에서 레이블(결정값) 데이터를 numpy로 로딩 . iris.target_names . array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) . 붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환 | . iris.feature_names . [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] . iris_df = pd.DataFrame(data=iris_data,columns=iris.feature_names) . iris_df . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . ... ... | ... | ... | ... | . 145 6.7 | 3.0 | 5.2 | 2.3 | . 146 6.3 | 2.5 | 5.0 | 1.9 | . 147 6.5 | 3.0 | 5.2 | 2.0 | . 148 6.2 | 3.4 | 5.4 | 2.3 | . 149 5.9 | 3.0 | 5.1 | 1.8 | . 150 rows × 4 columns . iris_df[&#39;label&#39;]=iris_label . iris_df.head(3) . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) label . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . feature : columns를 의미함 | label(결정값) : 0,1,2 세 가지 값응로 돼 있으며 순서대로 Setosa,versicolor,virginica 품종을 의미 | . . 학습용 데이터와 테스트용 데이터를 분리해보자 | . 학습용 데이터와 테스트용 데이터는 반드시 분리해야 함, 이를 위해 Scikit-learn에선 train_test_split() API를 제공, 해당 API를 이용하면 학습 데이터와 테스트 데이터를 test_size 파라미터 입력 값의 비율로 쉽게 분할함 | . 예를 들어보자, teat_size=0.2로 입력 파라미터를 설정하면 전체 데이터 중 테스트 데이터가 20%, 학습 데이터가 80%로 데이터를 분할함 | . X_train,X_test,y_train,y_test=train_test_split(iris_data,iris_label,test_size=0.2, random_state=11) . train_test_split의 첫 번째 파라미터인 iris_data는 feature 데이터 세트이며, 두 번째 파라미터인 iris_label은 Label 데이터 세트. random_state=11은 호출할 때마다 같은 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 발생 값. . | train_test_split은 호출 시 무작위로 데이터를 분리하므로 random_state를 지정하지 않으면 수행할 때마다 다른 학습/테스트 용 데이터를 만듦. . | X_train,X_test,y_train,y_test = 학습용 feature데이터 세트, 테스트용 feature데이터 세트, 학습용 레이블 데이터 세트, 테스트용 레이블 데이터 세트를 의미 . | . . 이제 이 데이터를 기반으로 머신 러닝 분류 알고리즘의 하나인 의사 결정 트리를 이용해 학습과 예측을 수행 . dt_clf=DecisionTreeClassifier(random_state=11) . dt_clf.fit(X_train,y_train) # 학습용 feature 데이터 세트와 학습용 레이블 데이터 세트를 입력해 학습 수행중 . DecisionTreeClassifier(random_state=11) . 학습 완료/ 예측을 수행해야하는데 학습 데이터가 아닌 다른 데이터를 이용해야 하며, 일반적으로 테스트 데이터 세트를 이용함 | . pred=dt_clf.predict(X_test) # 예측 수행 중 # 예측 label data set . 예측 성능 평가, 여러 평가 방법 중 정확도를 측정해보자. 정확도는 예측 결과가 실제 레이블 값과 얼마나 정확하게 맞는지를 평가하는 지표 | . from sklearn.metrics import accuracy_score print(&#39;예측 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 예측 정확도: 0.9333 . 학습한 의사 결정 트리의 알고리즘 예측 정확도가 약 93.33% | . . Conclusion . 1) 데이터 세트 분리 : 데이터를 학습 데이터와 테스트 데이터로 분리 2) 모델 학습 : 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습 3) 예측 수행 : 학습된 ML 모델을 이용해 테스트 데이터의 분류(즉, 붓꽃 종류)를 예측 4) 평가 : 이렇게 예측된 결과값과 테스트 데이터의 실제 결과값을 비교해 ML 모델 성능을 평가 . . 간단한 실습을 해보았으니 전체적 틀을 review해보자 . ML 모델 학습을 위해서 fit(), 학습된 모델의 예측을 위해 predict()를 사용 | Scikit Learn에서는 분류 알고리즘을 구현한 클래스를 Classifier로, 그리고 회귀 알고리즘을 구현한 클래스를 Regressor로 지칭 | Classifier와 Regressor를 합쳐서 Estimator 클래스라고 부름. 즉, 지도학습의 모든 알고리즘을 구현한 클래스를 통칭해서 Estimator라고 부름 | . Scikit-Learn의 다양한 모듈은 교재 94p,95p 참고 . 머신러닝 모델을 구축하는 주요 프로세스 = feature의 가공, 변경, 추출을 수행하는 feature processing, ML 알고리즙 학습/예측 수행, 그리고 모델 평가 단계를 반복적으로 수행하는 것 | . 사이킷런에 내장되어 있는 데이터 세트는 일반적으로 dict형태 | 이때 key는 data(feature의 데이터 세트),target(레이블 값, 숫자 결과값 데이터 세트),target_names(개별 레이블의 이름), feature_names(feature의 이름), DESCR 데이터 세트에 대한 설명과 각 feature의 설명 . | 앞에 두 key는 ndarray, 다음 두개는 ndarray 또는 list 그 다음은 str . | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2021/12/31/intro.html",
            "relUrl": "/2021/12/31/intro.html",
            "date": " • Dec 31, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Machine Learning?",
            "content": "&#45936;&#51060;&#53552;&#50640; &#47784;&#45944;&#51012; &#47582;&#52632;&#45796; . 애플리케이션을 수정하지 않고도 데이터를 기반으로 패턴을 학습하고 결과를 예측하는 알고리즘 기법을 통칭 | . 데이터 기반으로 통계적인 신뢰도를 강화하고 예측 오류를 최소화하기 위한 다양한 수학적 기법을 적용해 데이터 내의 패턴을 스스로 인지하고 신뢰도 있는 예측 결과를 도출 | . 데이터를 관통하는 패턴을 학습, 이에 기반한 예측 수행 | . 모델을 맞춘다? 모델을 학습시키는 기법들에는 딥러닝, 나이브베이즈, 디시전트리 등이 있음, 모델을 맞추는 행위를 하지 않으면 이 모델은 어떤 문제도 해결할 수가 없음, 따라서 데이터를 모델에 맞추는 행위가 필요함 | . 따라서 이 데이터에 문제를 최대한 많이 맞출 수 있도록 모델을 최적화하여야 하는데 이렇게 최적화가 된 모델이 그 데이터에 해당된 문제를 해결할 수 있게 됨 | . 분류 :지도 학습(Supervised Learning), 비지도 학습(Un-supervised Learning), 강화 학습(Reinforcement Learning) 지도학습 $ to$ 분류(Classification)와 회귀(Regression)로 나눌 수 있음 . . data? Garbage In $ to$ Garbage Out : data도 질이 중요하다 | . 데이터를 이해하고 효율적으로 가공,처리,추출하여 최적의 데이터를 기반으로 알고리즘을 구동할 수 있도록 준비하는 능력 필요 | . . 만약 온도, 습도, 풍속을 정리해놓은 데이터가 있을 때 눈이 오는 여부를 다양한 기법으로 해결할 수 있음 . 1) 조건을 정해서 해결한다.(decision tree 등) . 2) 수식(가중치)으로 해결한다.(선형 회귀, 딥러닝 등) . 이러한 접근 방식을 통해서 가지고 있는 데이터를 50%정도만 해결했다면 이 접근 방식들은 썩 좋지 않은 방식일 것임 . 따라서 머신러닝을 학습한다는 것은 이 정답을 최대한 맞출 수 있도록 모델을 최대한 최적화한다는 의미임. 이렇게 가장 좋은 성능이 나올 수 있는 식과 조건을 찾아나가는 것을 기계가 스스로 하는 것을 머신러닝이라고 생각할 수도 있겠음. . . 딥러닝 등 머신러닝 기법들 전반적으로 공부할 필요가 있다 . 딥러닝 : 자연어와 이미지 처리에 강하다. 그렇지만 다른 과업처리에 있어서도 항상 우수한 결과를 도출해내는 것은 아니다. 대표적으로 KAGGLE에 있는 TITANIC자료에서 실제로 산 승객과 죽은 승객을 처리해내는 과업을 수행할 땐 딥러닝보다 머신러닝의 모델이 더 좋은 결과를 도출해냈음 | . &#44208;&#44397; &#47785;&#51201;&#51008; &#45936;&#51060;&#53552;&#47484; &#47784;&#45944;&#50640; &#52572;&#51201;&#54868; &#49884;&#53412;&#45716; &#44163;&#51060;&#45796; . 머신러닝 논문에 머신러닝 기법들간에 성능을 비교한 표도 있음. 즉, 머신러닝 기법들은 다양한 기법들이 있기 때문에 그 것들간의 차이점과, 각각의 알고리즘이 무엇을 최적화하려는 것인지의 관점에서 이해해보아야함 . . . &#54028;&#51060;&#50028; &#47672;&#49888;&#47084;&#45789; &#49373;&#53468;&#44228;&#47484; &#44396;&#49457;&#54616;&#45716; &#51452;&#50836; &#54056;&#53412;&#51648; . 머신러닝 패키지 : Scikit-Learn . | 행렬/ 선형대수/ 통계 패키지 : Numpy, SciPy . | 데이터 핸들링 : Pandas(Numpy는 행렬 기반의 데이터 처리에 특화) $ to$ 2차원 데이터 처리에 특화,Matplotlib . | 시각화 : matplotlib(너무 세분화 되어 있어서 익히기 어려움, 시각적인 면에서도 투박),Seaborn(matplotlib의 대안이 될 것) . | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2021/12/28/intro.html",
            "relUrl": "/2021/12/28/intro.html",
            "date": " • Dec 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rhkrehtjd.github.io/INTROml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rhkrehtjd.github.io/INTROml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}