{
  
    
        "post0": {
            "title": "2022/01/09/SUN",
            "content": "print(&#39;학습 데이터 레이블 값 비율&#39;) print(y_train.value_counts()/y_train.shape[0] * 100) print(&#39;테스트 데이터 레이블 값 비율&#39;) print(y_test.value_counts()/y_test.shape[0] * 100) . 학습 데이터 레이블 값 비율 0 99.827451 1 0.172549 Name: Class, dtype: float64 테스트 데이터 레이블 값 비율 0 99.826785 1 0.173215 Name: Class, dtype: float64 . 학습데이터 레이블과 테스트 레이블을 살펴본 결 과 잘 분할 됐음. 이제 로지스틱 회귀와 LightGBM 기반의 모델이 데이터 가공을 수행하면서 예측 성능이 어떻게 변하는지 살펴보자 . 먼저 로지스틱 회귀를 이용해 신용 카드 사기 여부를 예측해보자 . from sklearn.linear_model import LogisticRegression lr_clf = LogisticRegression() lr_clf.fit( X_train, y_train) lr_pred = lr_clf.predict(X_test) lr_pred_proba = lr_clf.predict_proba(X_test)[:, 1] # 3장에서 사용한 get_clf_eval() 함수를 이용하여 평가 수행. get_clf_eval(y_test, lr_pred, lr_pred_proba) . 이번에는 LightGBM을 이용한 모델을 만들어보자 . (앞으로 수행할 예제 코드에서 반복적으로 모델을 변경해 학습/예측/평가할 것이므로 이를 위한 별도의 함수를 생성해보자) . def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None): model.fit(ftr_train, tgt_train) pred = model.predict(ftr_test) pred_proba = model.predict_proba(ftr_test)[:, 1] get_clf_eval(tgt_test, pred, pred_proba) . 먼저, 본 데이터 세트는 극도로 불균형한 레이블 값 분포를 지녔음 $ to$ LightGBMClassifier객체 생성 시 boost_from_average=False로 파라미터를 설정해야한다. | . LightGBM으로 모델을 학습한 뒤 별도의 테스트 데이터 세트에서 예측 평가를 수행해보자 | . from lightgbm import LGBMClassifier lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . 오차 행렬 [[85290 5] [ 36 112]] 정확도: 0.9995, 정밀도: 0.9573, 재현율: 0.7568, F1: 0.8453, AUC:0.9790 . 재현율과 ROC-AUC가 회귀보다 높은 수치 기록. | . 왜곡된 분포도를 가지는 데이터를 재가공한 뒤 모델을 다시 테스트해보자. | 대부분의 선형 모델은 중요 feature들의 값이 정규 분포 형태를 유지하는 것을 선호. | Amount feature는 중요 feature일 가능성이 높음. | . import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings import seaborn as sns warnings.filterwarnings(&quot;ignore&quot;) %matplotlib inline card_df = pd.read_csv(&#39;./creditcard.csv&#39;) card_df.head(3) plt.figure(figsize=(8, 4)) plt.xticks(range(0, 30000, 1000), rotation=60) sns.distplot(card_df[&#39;Amount&#39;]) . &lt;AxesSubplot:xlabel=&#39;Amount&#39;, ylabel=&#39;Density&#39;&gt; . # Amount를 정규분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 수행. X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) print(&#39;### 로지스틱 회귀 예측 성능 ###&#39;) lr_clf = LogisticRegression() get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) print(&#39;### LightGBM 예측 성능 ###&#39;) lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . 성능이 크게 개선되지 X $ to$ 이번에는 StandardScaler가 아니라 로그 변환을 수행해보자. | 로그 변환은 데이터 분포도가 심하게 왜곡되어 있을 경우 적용하는 중요 기법 중 하나.(원래 값을 log 값으로 변환해 원래 큰 값을 상대적으로 작은 값으로 변환하기 때문에 데이터 분포도의 왜곡을 상당수준 개선해준다.) | . def get_preprocessed_df(df=None): df_copy = df.copy() # 넘파이의 log1p( )를 이용하여 Amount를 로그 변환 amount_n = np.log1p(df_copy[&#39;Amount&#39;]) df_copy.insert(0, &#39;Amount_Scaled&#39;, amount_n) df_copy.drop([&#39;Time&#39;,&#39;Amount&#39;], axis=1, inplace=True) return df_copy . 이제 Amount feature를 log 변환한 후 다시 로지스틱 회귀와 LightGBM 모델을 적용한 후 예측 성능을 확인해보자 | . . # log1p 와 expm1 설명 import numpy as np print(1e-1000 == 0.0) print(np.log(1e-1000)) print(np.log(1e-1000 + 1)) print(np.log1p(1e-1000)) . True -inf 0.0 0.0 . var_1 = np.log1p(100) var_2 = np.expm1(var_1) print(var_1, var_2) . 4.61512051684126 100.00000000000003 . . X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) print(&#39;### 로지스틱 회귀 예측 성능 ###&#39;) get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) print(&#39;### LightGBM 예측 성능 ###&#39;) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . ### 로지스틱 회귀 예측 성능 ### 오차 행렬 [[85283 12] [ 59 89]] 정확도: 0.9992, 정밀도: 0.8812, 재현율: 0.6014, F1: 0.7149, AUC:0.9727 ### LightGBM 예측 성능 ### 오차 행렬 [[85290 5] [ 35 113]] 정확도: 0.9995, 정밀도: 0.9576, 재현율: 0.7635, F1: 0.8496, AUC:0.9796 . 두 모델 모두 정밀도, 재현율, ROC-AUC에서 약간씩 성능이 개선되었음을 알 수 있음 | . &#51060;&#49345;&#52824; &#45936;&#51060;&#53552; :Outlier ?? &#51204;&#52404; &#45936;&#51060;&#53552;&#51032; &#54056;&#53556;&#50640;&#49436; &#48279;&#50612;&#45212; &#51060;&#49345; &#44050;&#51012; &#44032;&#51652; &#45936;&#51060;&#53552;. IQR&#48169;&#49885;&#51060; &#51080;&#51020;. IQR&#51008; &#49324;&#48516;&#50948; &#44050;&#51032; &#54200;&#52264;&#47484; &#51060;&#50857;&#54616;&#45716; &#44592;&#48277;&#51004;&#47196;&#49436; &#55124;&#55176; &#48149;&#49828;&#54540;&#46991;&#51004;&#47196; &#49884;&#44033;&#54868; &#54624; &#49688; &#51080;&#45796;. . 먼저 사분위란 전체 데이터를 값이 높은 순으로 정렬하고 이를 25%씩 구간 분할하는 것을 지칭. 가령 100명의 시험 성적이 0점부터 100점까지 있다면 이를 100등부터 1등까지 성적순으로 정렬한 뒤 1/4구간으로 나누는 것. 여기서 25%~75% 범위를 IQR이라고 한다. IQR에 보통은 1.5를 공해서 75%지점에 더해주고 25%지점에서 빼준 뒤 그 범위를 넘어가면 outlier로 간주!. 이를 시각화한 도표가 boxplot이다. . 먼저 어떤 feature의 이상치 데이터를 검출할 것인지 선택. | 매우 많은 feature가 있을 경우 상관성이 높은 feature들을 위주로 이상치를 검출하자. | . import seaborn as sns plt.figure(figsize=(9, 9)) corr = card_df.corr() sns.heatmap(corr, cmap=&#39;RdBu&#39;) . &lt;AxesSubplot:&gt; . 양의 상관관계가 높을수록 색이 진한 파란색. 음의 상관관계가 높을수록 색이 진한 빨간색. . | 이중 결정 레이블에 속하는 Class feature와 음의 상관관계가 가장 높은 feature는 V14와 V17, 이중 V14에서 outlier를 제거해보자 . | . import numpy as np def get_outlier(df=None, column=None, weight=1.5): # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함. fraud = df[df[&#39;Class&#39;]==1][column] quantile_25 = np.percentile(fraud.values, 25) quantile_75 = np.percentile(fraud.values, 75) # IQR을 구하고, IQR에 1.5를 곱하여 최대값과 최소값 지점 구함. iqr = quantile_75 - quantile_25 iqr_weight = iqr * weight lowest_val = quantile_25 - iqr_weight highest_val = quantile_75 + iqr_weight # 최대값 보다 크거나, 최소값 보다 작은 값을 아웃라이어로 설정하고 DataFrame index 반환. outlier_index = fraud[(fraud &lt; lowest_val) | (fraud &gt; highest_val)].index return outlier_index . outlier_index = get_outlier(df=card_df, column=&#39;V14&#39;, weight=1.5) print(&#39;이상치 데이터 인덱스:&#39;, outlier_index) . 이상치 데이터 인덱스: Int64Index([8296, 8615, 9035, 9252], dtype=&#39;int64&#39;) . 이를 삭제하는 로직을 추가 | . def get_preprocessed_df(df=None): df_copy = df.copy() amount_n = np.log1p(df_copy[&#39;Amount&#39;]) df_copy.insert(0, &#39;Amount_Scaled&#39;, amount_n) df_copy.drop([&#39;Time&#39;,&#39;Amount&#39;], axis=1, inplace=True) # 이상치 데이터 삭제하는 로직 추가 outlier_index = get_outlier(df=df_copy, column=&#39;V14&#39;, weight=1.5) df_copy.drop(outlier_index, axis=0, inplace=True) return df_copy X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) print(&#39;### 로지스틱 회귀 예측 성능 ###&#39;) get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) print(&#39;### LightGBM 예측 성능 ###&#39;) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . ### 로지스틱 회귀 예측 성능 ### 오차 행렬 [[85281 14] [ 48 98]] 정확도: 0.9993, 정밀도: 0.8750, 재현율: 0.6712, F1: 0.7597, AUC:0.9743 ### LightGBM 예측 성능 ### 오차 행렬 [[85290 5] [ 25 121]] 정확도: 0.9996, 정밀도: 0.9603, 재현율: 0.8288, F1: 0.8897, AUC:0.9780 . outlier 제거한 뒤 로지스틱 회귀와 LightGBM 모두 예측 성능이 크게 향상됐음을 알 수 있다. | . . SMOTE 기법으로 오버 샘플링을 적용한 뒤 로지스틱 회구와 LightGBM 모델의 예측 성능을 평가해보자 | SMOTE를 적용할 땐, 반드시 학습 데이터 세트만 오버 샘플링 해야함 | . from imblearn.over_sampling import SMOTE smote = SMOTE(random_state=0) X_train_over, y_train_over = smote.fit_sample(X_train, y_train) print(&#39;SMOTE 적용 전 학습용 피처/레이블 데이터 세트: &#39;, X_train.shape, y_train.shape) print(&#39;SMOTE 적용 후 학습용 피처/레이블 데이터 세트: &#39;, X_train_over.shape, y_train_over.shape) print(&#39;SMOTE 적용 후 레이블 값 분포: n&#39;, pd.Series(y_train_over).value_counts()) . SMOTE 적용 전 학습용 피처/레이블 데이터 세트: (199362, 29) (199362,) SMOTE 적용 후 학습용 피처/레이블 데이터 세트: (398040, 29) (398040,) SMOTE 적용 후 레이블 값 분포: 0 199020 1 199020 Name: Class, dtype: int64 . lr_clf = LogisticRegression() # ftr_train과 tgt_train 인자값이 SMOTE 증식된 X_train_over와 y_train_over로 변경됨에 유의 get_model_train_eval(lr_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test) . 오차 행렬 [[82937 2358] [ 11 135]] 정확도: 0.9723, 정밀도: 0.0542, 재현율: 0.9247, F1: 0.1023, AUC:0.9737 . 재현울은 크게 증가하나 정밀도가 급격히 저하 | 이 정도로 저조한 정밀도는 현실 업무에 적용할 수 없음 | . lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False) get_model_train_eval(lgbm_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test) . 오차 행렬 [[85283 12] [ 22 124]] 정확도: 0.9996, 정밀도: 0.9118, 재현율: 0.8493, F1: 0.8794, AUC:0.9814 . 재현율은 높아졌으나 정밀도는 좀 낮아짐. | SMOTE를 적용하면 재현율은 높아지나 정밀도는 낮아지는 것이 일반적임. | . &#49828;&#53468;&#53433; &#50521;&#49345;&#48660; . 스태킹은 개별적인 여러 알고리즘을 서로 결합해 예측 결과를 도출한다는 점에서 배깅,부스팅과 일맥상통. 허나 가장 큰 차이점은 개별 알고리즘으로 예측한 데이터를 기반으로 다시 마지막 특정 알고리즘으로 재예측시도함. 즉 개별 알고리즘의 예측 결과 데이터 세트를 최종적인 메타 데이터 세트로 만들어 별도의 ML 알고리즘으로 최종학습을 수행하고 테스트 데이터를 기반으로 다시 최종 예측을 수행하는 방식. 이렇게 개별 모델의 예측된 데이터 세트를 다시 기반으로 하여 학습하고 예측하는 방식을 메타 모델이라고 한다. 따라서 스태킹 모델은 두 종류의 모델 필요. 첫번째는 기반 모델, 두번째는 이 개별 기반 모델의 예측 데이터를 학습 데이터로 만들어서 학습하는 최종 메타 모델. 스태킹 모델의 핵심은 여러 개별 모델의 예측 데이터를 각각 스태킹 형태로 결합해, 즉 잘 쌓아서, 최종 메타 모델의 학습용 feature데이터 세트와 테스트용 feature 데이터 세트를 만드는 것이다. 이를 도식화 한 248p참고 . 기본 스태킹 모델을 위스콘신 암 데이터 세트에 적용해보자 | . import numpy as np from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score cancer_data = load_breast_cancer() X_data = cancer_data.data y_label = cancer_data.target X_train , X_test , y_train , y_test = train_test_split(X_data , y_label , test_size=0.2 , random_state=0) . 스태킹에 사용될 머신러닝 알고리즘 클래스를 생성하자. | 개별 모델은 KNN, 랜덤 포레스트, 결정 트리, 에이다 부스트이며 이들 모델의 예측 결과를 합한 데이터 세트로 학습,예측 수행하는 최종 모델은 로지스틱 회귀이다. | . knn_clf = KNeighborsClassifier(n_neighbors=4) rf_clf = RandomForestClassifier(n_estimators=100, random_state=0) dt_clf = DecisionTreeClassifier() ada_clf = AdaBoostClassifier(n_estimators=100) # 최종 Stacking 모델을 위한 Classifier생성. lr_final = LogisticRegression(C=10) . 개별 모델들을 학습시키자 | . knn_clf.fit(X_train, y_train) rf_clf.fit(X_train , y_train) dt_clf.fit(X_train , y_train) ada_clf.fit(X_train, y_train) . AdaBoostClassifier(n_estimators=100) . 개별 모델의 예측 데이터 세트를 반환하고 각 모델의 예측 정확도를 살펴보자 | . knn_pred = knn_clf.predict(X_test) rf_pred = rf_clf.predict(X_test) dt_pred = dt_clf.predict(X_test) ada_pred = ada_clf.predict(X_test) print(&#39;KNN 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, knn_pred))) print(&#39;랜덤 포레스트 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, rf_pred))) print(&#39;결정 트리 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, dt_pred))) print(&#39;에이다부스트 정확도: {0:.4f} :&#39;.format(accuracy_score(y_test, ada_pred))) . KNN 정확도: 0.9211 랜덤 포레스트 정확도: 0.9649 결정 트리 정확도: 0.9123 에이다부스트 정확도: 0.9561 : . 개별 알고리즘으로부터 예측된 예측값을 칼럼 레벨로 옆으로 붙여서 feature값으로 만들어, 최종 메타 모델인 로지스틱 회귀에서 학습 데이터로 다시 사용하자. | . pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred]) print(pred.shape) # transpose를 이용해 행과 열의 위치 교환. 컬럼 레벨로 각 알고리즘의 예측 결과를 피처로 만듦. pred = np.transpose(pred) print(pred.shape) . (4, 114) (114, 4) . lr_final.fit(pred, y_test) final = lr_final.predict(pred) print(&#39;최종 메타 모델의 예측 정확도: {0:.4f}&#39;.format(accuracy_score(y_test , final))) . 최종 메타 모델의 예측 정확도: 0.9737 . 개별 모델 정확도보다 스태킹으로 재구성해 최종 메타 모델에서 학습하고 예측한 결과가 더 높음. | . CV &#49464;&#53944; &#44592;&#48152;&#51032; &#49828;&#53468;&#53433; . 과적합을 개선하기 위해 최종 메타 모델을 위한 데이터 세트를 만들 때 교차 검증을 기반으로 예측된 결과 데이터 세트를 이용한다. 앞에서 마지막에 메타 모델인 로지스틱 회귀 모델을 기반으로 최종 학습할 때 레이블 데이터 세트로 학습데이터가 아닌 테스트용 레이블 데이터 세트를 기반으로 학습했기에 과적합 문제가 발생할 수 있다. CV세트 기반의 스태킹은 이에 대한 개선을 위해 개별 모델들이 각각 교차 검증으로 메타모델을 위한 학습용 스태킹 데이터 생성과 예측을 위한 테스트용 스태킹 데이터를 생성한 뒤 이를 기반으로 메타 모델이 학습과 예측을 수행한다. 자세한 건 284p참고하자 . from sklearn.model_selection import KFold from sklearn.metrics import mean_absolute_error # 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ): # 지정된 n_folds값으로 KFold 생성. kf = KFold(n_splits=n_folds, shuffle=False, random_state=0) #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 train_fold_pred = np.zeros((X_train_n.shape[0] ,1 )) test_pred = np.zeros((X_test_n.shape[0],n_folds)) print(model.__class__.__name__ , &#39; model 시작 &#39;) for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)): #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 print(&#39; t 폴드 세트: &#39;,folder_counter,&#39; 시작 &#39;) X_tr = X_train_n[train_index] y_tr = y_train_n[train_index] X_te = X_train_n[valid_index] #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행. model.fit(X_tr , y_tr) #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장. train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. test_pred[:, folder_counter] = model.predict(X_test_n) # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1) #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터 return train_fold_pred , test_pred_mean . knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7) rf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7) dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7) ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7) . KNeighborsClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 RandomForestClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 DecisionTreeClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 AdaBoostClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 . Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1) Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1) print(&#39;원본 학습 피처 데이터 Shape:&#39;,X_train.shape, &#39;원본 테스트 피처 Shape:&#39;,X_test.shape) print(&#39;스태킹 학습 피처 데이터 Shape:&#39;, Stack_final_X_train.shape, &#39;스태킹 테스트 피처 데이터 Shape:&#39;,Stack_final_X_test.shape) . 원본 학습 피처 데이터 Shape: (455, 30) 원본 테스트 피처 Shape: (114, 30) 스태킹 학습 피처 데이터 Shape: (455, 4) 스태킹 테스트 피처 데이터 Shape: (114, 4) . lr_final.fit(Stack_final_X_train, y_train) stack_final = lr_final.predict(Stack_final_X_test) print(&#39;최종 메타 모델의 예측 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, stack_final))) . 최종 메타 모델의 예측 정확도: 0.9737 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/09/intro.html",
            "relUrl": "/2022/01/09/intro.html",
            "date": " • Jan 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "2022/01/08/SAT",
            "content": "LightGBM . :XGBoost보다 학습에 걸리는 시간이 적다. 10,000건 이하의 적은 데이터 세트에 적용할 경우 과적합이 발생하기 쉽다. 리프 중심 트리(Leaf Wise) 분할 방식을 사용한다. 기존의 대부분 트리 기반 알고리즘은 트리의 깊이를 효과적으로 줄이기 위한 균형 트리 분할(Level wise) 방식을 사용. 즉 최대한 규형 잡힌 트리를 유지하면서 분할하기 때문에 트리의 깊이가 최소화될 수 있다. 하지만 LightGBM의 리프 중심 트리 분할 방식은 트리의 균형을 맞추지 않고 최대 손실 값(max delta loss)을 가지는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적인 규칙 트리가 생성된다. 하지만 이렇게 최대 손실값을 가지는 리프 노드를 지속적으로 분할해 생성된 규칙 트리는 학습을 반복할수록 결국은 균형 트리 방식보다 예측 오류 손실을 최소화할 수 있다는 것이 LightGBM의 구현 사상이다. Xgboost와 다르게 리프 노드가 계속 분할되면서 트리의 깊이가 깊어지므로 이러한 트리 특성에 맞는 하이퍼 파라미터 설정이 필요하다. (245p 균형 트리 분할과 리프 중심 트리 분할 도식화한 거 살펴보기) . learning_rate를 작게 하면서 n_estimators를 크게 하는 것이 부스팅 계열 튜닝에서 가장 기본적이 튜닝 방안이다. . LightGBM을 이용해 위스콘신 유방암 데이터 세트를 예측해보자. . from lightgbm import LGBMClassifier import pandas as pd import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split dataset = load_breast_cancer() ftr = dataset.data target = dataset.target # 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출 X_train, X_test, y_train, y_test=train_test_split(ftr, target, test_size=0.2, random_state=156 ) # 앞서 XGBoost와 동일하게 n_estimators는 400 설정. lgbm_wrapper = LGBMClassifier(n_estimators=400) # LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능. evals = [(X_test, y_test)] lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;logloss&quot;, eval_set=evals, verbose=True) preds = lgbm_wrapper.predict(X_test) pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1] . [1] valid_0&#39;s binary_logloss: 0.565079 [2] valid_0&#39;s binary_logloss: 0.507451 [3] valid_0&#39;s binary_logloss: 0.458489 [4] valid_0&#39;s binary_logloss: 0.417481 [5] valid_0&#39;s binary_logloss: 0.385507 [6] valid_0&#39;s binary_logloss: 0.355773 [7] valid_0&#39;s binary_logloss: 0.329587 [8] valid_0&#39;s binary_logloss: 0.308478 [9] valid_0&#39;s binary_logloss: 0.285395 [10] valid_0&#39;s binary_logloss: 0.267055 [11] valid_0&#39;s binary_logloss: 0.252013 [12] valid_0&#39;s binary_logloss: 0.237018 [13] valid_0&#39;s binary_logloss: 0.224756 [14] valid_0&#39;s binary_logloss: 0.213383 [15] valid_0&#39;s binary_logloss: 0.203058 [16] valid_0&#39;s binary_logloss: 0.194015 [17] valid_0&#39;s binary_logloss: 0.186412 [18] valid_0&#39;s binary_logloss: 0.179108 [19] valid_0&#39;s binary_logloss: 0.174004 [20] valid_0&#39;s binary_logloss: 0.167155 [21] valid_0&#39;s binary_logloss: 0.162494 [22] valid_0&#39;s binary_logloss: 0.156886 [23] valid_0&#39;s binary_logloss: 0.152855 [24] valid_0&#39;s binary_logloss: 0.151113 [25] valid_0&#39;s binary_logloss: 0.148395 [26] valid_0&#39;s binary_logloss: 0.145869 [27] valid_0&#39;s binary_logloss: 0.143036 [28] valid_0&#39;s binary_logloss: 0.14033 [29] valid_0&#39;s binary_logloss: 0.139609 [30] valid_0&#39;s binary_logloss: 0.136109 [31] valid_0&#39;s binary_logloss: 0.134867 [32] valid_0&#39;s binary_logloss: 0.134729 [33] valid_0&#39;s binary_logloss: 0.1311 [34] valid_0&#39;s binary_logloss: 0.131143 [35] valid_0&#39;s binary_logloss: 0.129435 [36] valid_0&#39;s binary_logloss: 0.128474 [37] valid_0&#39;s binary_logloss: 0.126683 [38] valid_0&#39;s binary_logloss: 0.126112 [39] valid_0&#39;s binary_logloss: 0.122831 [40] valid_0&#39;s binary_logloss: 0.123162 [41] valid_0&#39;s binary_logloss: 0.125592 [42] valid_0&#39;s binary_logloss: 0.128293 [43] valid_0&#39;s binary_logloss: 0.128123 [44] valid_0&#39;s binary_logloss: 0.12789 [45] valid_0&#39;s binary_logloss: 0.122818 [46] valid_0&#39;s binary_logloss: 0.12496 [47] valid_0&#39;s binary_logloss: 0.125578 [48] valid_0&#39;s binary_logloss: 0.127381 [49] valid_0&#39;s binary_logloss: 0.128349 [50] valid_0&#39;s binary_logloss: 0.127004 [51] valid_0&#39;s binary_logloss: 0.130288 [52] valid_0&#39;s binary_logloss: 0.131362 [53] valid_0&#39;s binary_logloss: 0.133363 [54] valid_0&#39;s binary_logloss: 0.1332 [55] valid_0&#39;s binary_logloss: 0.134543 [56] valid_0&#39;s binary_logloss: 0.130803 [57] valid_0&#39;s binary_logloss: 0.130306 [58] valid_0&#39;s binary_logloss: 0.132514 [59] valid_0&#39;s binary_logloss: 0.133278 [60] valid_0&#39;s binary_logloss: 0.134804 [61] valid_0&#39;s binary_logloss: 0.136888 [62] valid_0&#39;s binary_logloss: 0.138745 [63] valid_0&#39;s binary_logloss: 0.140497 [64] valid_0&#39;s binary_logloss: 0.141368 [65] valid_0&#39;s binary_logloss: 0.140764 [66] valid_0&#39;s binary_logloss: 0.14348 [67] valid_0&#39;s binary_logloss: 0.143418 [68] valid_0&#39;s binary_logloss: 0.143682 [69] valid_0&#39;s binary_logloss: 0.145076 [70] valid_0&#39;s binary_logloss: 0.14686 [71] valid_0&#39;s binary_logloss: 0.148051 [72] valid_0&#39;s binary_logloss: 0.147664 [73] valid_0&#39;s binary_logloss: 0.149478 [74] valid_0&#39;s binary_logloss: 0.14708 [75] valid_0&#39;s binary_logloss: 0.14545 [76] valid_0&#39;s binary_logloss: 0.148767 [77] valid_0&#39;s binary_logloss: 0.149959 [78] valid_0&#39;s binary_logloss: 0.146083 [79] valid_0&#39;s binary_logloss: 0.14638 [80] valid_0&#39;s binary_logloss: 0.148461 [81] valid_0&#39;s binary_logloss: 0.15091 [82] valid_0&#39;s binary_logloss: 0.153011 [83] valid_0&#39;s binary_logloss: 0.154807 [84] valid_0&#39;s binary_logloss: 0.156501 [85] valid_0&#39;s binary_logloss: 0.158586 [86] valid_0&#39;s binary_logloss: 0.159819 [87] valid_0&#39;s binary_logloss: 0.161745 [88] valid_0&#39;s binary_logloss: 0.162829 [89] valid_0&#39;s binary_logloss: 0.159142 [90] valid_0&#39;s binary_logloss: 0.156765 [91] valid_0&#39;s binary_logloss: 0.158625 [92] valid_0&#39;s binary_logloss: 0.156832 [93] valid_0&#39;s binary_logloss: 0.154616 [94] valid_0&#39;s binary_logloss: 0.154263 [95] valid_0&#39;s binary_logloss: 0.157156 [96] valid_0&#39;s binary_logloss: 0.158617 [97] valid_0&#39;s binary_logloss: 0.157495 [98] valid_0&#39;s binary_logloss: 0.159413 [99] valid_0&#39;s binary_logloss: 0.15847 [100] valid_0&#39;s binary_logloss: 0.160746 [101] valid_0&#39;s binary_logloss: 0.16217 [102] valid_0&#39;s binary_logloss: 0.165293 [103] valid_0&#39;s binary_logloss: 0.164749 [104] valid_0&#39;s binary_logloss: 0.167097 [105] valid_0&#39;s binary_logloss: 0.167697 [106] valid_0&#39;s binary_logloss: 0.169462 [107] valid_0&#39;s binary_logloss: 0.169947 [108] valid_0&#39;s binary_logloss: 0.171 [109] valid_0&#39;s binary_logloss: 0.16907 [110] valid_0&#39;s binary_logloss: 0.169521 [111] valid_0&#39;s binary_logloss: 0.167719 [112] valid_0&#39;s binary_logloss: 0.166648 [113] valid_0&#39;s binary_logloss: 0.169053 [114] valid_0&#39;s binary_logloss: 0.169613 [115] valid_0&#39;s binary_logloss: 0.170059 [116] valid_0&#39;s binary_logloss: 0.1723 [117] valid_0&#39;s binary_logloss: 0.174733 [118] valid_0&#39;s binary_logloss: 0.173526 [119] valid_0&#39;s binary_logloss: 0.1751 [120] valid_0&#39;s binary_logloss: 0.178254 [121] valid_0&#39;s binary_logloss: 0.182968 [122] valid_0&#39;s binary_logloss: 0.179017 [123] valid_0&#39;s binary_logloss: 0.178326 [124] valid_0&#39;s binary_logloss: 0.177149 [125] valid_0&#39;s binary_logloss: 0.179171 [126] valid_0&#39;s binary_logloss: 0.180948 [127] valid_0&#39;s binary_logloss: 0.183861 [128] valid_0&#39;s binary_logloss: 0.187579 [129] valid_0&#39;s binary_logloss: 0.188122 [130] valid_0&#39;s binary_logloss: 0.1857 [131] valid_0&#39;s binary_logloss: 0.187442 [132] valid_0&#39;s binary_logloss: 0.188578 [133] valid_0&#39;s binary_logloss: 0.189729 [134] valid_0&#39;s binary_logloss: 0.187313 [135] valid_0&#39;s binary_logloss: 0.189279 [136] valid_0&#39;s binary_logloss: 0.191068 [137] valid_0&#39;s binary_logloss: 0.192414 [138] valid_0&#39;s binary_logloss: 0.191255 [139] valid_0&#39;s binary_logloss: 0.193453 [140] valid_0&#39;s binary_logloss: 0.196969 [141] valid_0&#39;s binary_logloss: 0.196378 [142] valid_0&#39;s binary_logloss: 0.196367 [143] valid_0&#39;s binary_logloss: 0.19869 [144] valid_0&#39;s binary_logloss: 0.200352 [145] valid_0&#39;s binary_logloss: 0.19712 . C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:726: UserWarning: &#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;early_stopping()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:736: UserWarning: &#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;log_evaluation()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; . 조기 중단으로 145번 반복까지만 수행하고 학습을 종료. 이제 학습된 LightGBM 모델을 기반으로 예측 성능을 평가해보자 . from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.metrics import precision_score, recall_score from sklearn.metrics import f1_score, roc_auc_score # 수정된 get_clf_eval() 함수 def get_clf_eval(y_test, pred=None, pred_proba=None): confusion = confusion_matrix( y_test, pred) accuracy = accuracy_score(y_test , pred) precision = precision_score(y_test , pred) recall = recall_score(y_test , pred) f1 = f1_score(y_test,pred) # ROC-AUC 추가 roc_auc = roc_auc_score(y_test, pred_proba) print(&#39;오차 행렬&#39;) print(confusion) # ROC-AUC print 추가 print(&#39;정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}&#39;.format(accuracy, precision, recall, f1, roc_auc)) . get_clf_eval(y_test, preds, pred_proba) . 오차 행렬 [[33 4] [ 1 76]] 정확도: 0.9561, 정밀도: 0.9500, 재현율: 0.9870, F1: 0.9682, AUC:0.9905 . from lightgbm import plot_importance import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=(10, 12)) # 사이킷런 래퍼 클래스를 입력해도 무방. plot_importance(lgbm_wrapper, ax=ax) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}, xlabel=&#39;Feature importance&#39;, ylabel=&#39;Features&#39;&gt; . 이번에는 캐글의 산탄데르 고객 만족 데이터 세트에 대해 고객 만족 여부를 XGBoost와 LightGBM을 활용해 예측해보자 . 데이터 전처리 먼저! . import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib cust_df = pd.read_csv(&quot;./train_santander.csv&quot;,encoding=&#39;latin-1&#39;) print(&#39;dataset shape:&#39;, cust_df.shape) cust_df.head(3) . dataset shape: (76020, 371) . ID var3 var15 imp_ent_var16_ult1 imp_op_var39_comer_ult1 imp_op_var39_comer_ult3 imp_op_var40_comer_ult1 imp_op_var40_comer_ult3 imp_op_var40_efect_ult1 imp_op_var40_efect_ult3 ... saldo_medio_var33_hace2 saldo_medio_var33_hace3 saldo_medio_var33_ult1 saldo_medio_var33_ult3 saldo_medio_var44_hace2 saldo_medio_var44_hace3 saldo_medio_var44_ult1 saldo_medio_var44_ult3 var38 TARGET . 0 1 | 2 | 23 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 39205.17 | 0 | . 1 3 | 2 | 34 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 49278.03 | 0 | . 2 4 | 2 | 23 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 67333.77 | 0 | . 3 rows × 371 columns . 클래스 값 포함한 feature가 371개 | . cust_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 76020 entries, 0 to 76019 Columns: 371 entries, ID to TARGET dtypes: float64(111), int64(260) memory usage: 215.2 MB . null값 없음 $ to$ 그러나 null값이 없는 대신 극단적인 숫자로 대체해놨을수도 있음 . | 전체 데이터에서 만족과 불만족의 비율을 살펴보자 $ to$ 레이블인 Target 속성 값의 분포를 알아보자 . | . print(cust_df[&#39;TARGET&#39;].value_counts()) unsatisfied_cnt = cust_df[cust_df[&#39;TARGET&#39;] == 1][&#39;TARGET&#39;].count() total_cnt = cust_df[&#39;TARGET&#39;].count() print(&#39;unsatisfied 비율은 {0:.2f}&#39;.format((unsatisfied_cnt / total_cnt))) . 0 73012 1 3008 Name: TARGET, dtype: int64 unsatisfied 비율은 0.04 . 불만족 비율은 4%에 불과 | . cust_df.describe() . ID var3 var15 imp_ent_var16_ult1 imp_op_var39_comer_ult1 imp_op_var39_comer_ult3 imp_op_var40_comer_ult1 imp_op_var40_comer_ult3 imp_op_var40_efect_ult1 imp_op_var40_efect_ult3 ... saldo_medio_var33_hace2 saldo_medio_var33_hace3 saldo_medio_var33_ult1 saldo_medio_var33_ult3 saldo_medio_var44_hace2 saldo_medio_var44_hace3 saldo_medio_var44_ult1 saldo_medio_var44_ult3 var38 TARGET . count 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | ... | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 7.602000e+04 | 76020.000000 | . mean 75964.050723 | -1523.199277 | 33.212865 | 86.208265 | 72.363067 | 119.529632 | 3.559130 | 6.472698 | 0.412946 | 0.567352 | ... | 7.935824 | 1.365146 | 12.215580 | 8.784074 | 31.505324 | 1.858575 | 76.026165 | 56.614351 | 1.172358e+05 | 0.039569 | . std 43781.947379 | 39033.462364 | 12.956486 | 1614.757313 | 339.315831 | 546.266294 | 93.155749 | 153.737066 | 30.604864 | 36.513513 | ... | 455.887218 | 113.959637 | 783.207399 | 538.439211 | 2013.125393 | 147.786584 | 4040.337842 | 2852.579397 | 1.826646e+05 | 0.194945 | . min 1.000000 | -999999.000000 | 5.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 5.163750e+03 | 0.000000 | . 25% 38104.750000 | 2.000000 | 23.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 6.787061e+04 | 0.000000 | . 50% 76043.000000 | 2.000000 | 28.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.064092e+05 | 0.000000 | . 75% 113748.750000 | 2.000000 | 40.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.187563e+05 | 0.000000 | . max 151838.000000 | 238.000000 | 105.000000 | 210000.000000 | 12888.030000 | 21024.810000 | 8237.820000 | 11073.570000 | 6600.000000 | 6600.000000 | ... | 50003.880000 | 20385.720000 | 138831.630000 | 91778.730000 | 438329.220000 | 24650.010000 | 681462.900000 | 397884.300000 | 2.203474e+07 | 1.000000 | . 8 rows × 371 columns . var3 칼럼의 경우 min값이 -999999이다. NaN값이나 특정 예외 값을 -999999로 변환한 것으로 보임. | . print(cust_df.var3.value_counts()[:10]) . 2 74165 8 138 -999999 116 9 110 3 108 1 105 13 98 7 97 4 86 12 85 Name: var3, dtype: int64 . -999999값이 116개나 있음을 알 수 있다. . | var3은 숫자 형이고 다른 값에 비해 -999999은 편차가 너무 심하므로 -999999을 값이 가장 많은 2로 변환하자. . | ID feature는 단순 식별자에 불과하므로 feature를 drop하자. . | 그리고 클래스 데이터 세트와 faeture 데이터 세트를 분리해 별도의 데이터 세트로 별도로 저장하자 . | . cust_df[&#39;var3&#39;].replace(-999999, 2, inplace=True) cust_df.drop(&#39;ID&#39;,axis=1 , inplace=True) # 피처 세트와 레이블 세트분리. 레이블 컬럼은 DataFrame의 맨 마지막에 위치해 컬럼 위치 -1로 분리 X_features = cust_df.iloc[:, :-1] y_labels = cust_df.iloc[:, -1] print(&#39;피처 데이터 shape:{0}&#39;.format(X_features.shape)) . 피처 데이터 shape:(76020, 369) . 학습과 성능 평가를 위해서 원본 데이터 세트에서 학습 데이터 세트와 테스트 데이터 세트를 분리하자 . | 비대칭한 데이터 세트이므로 클래스인 Target 값 분포도가 학습 데이터와 테스트 데이터 세트에 모두 비슷하게 추출됐는지 확인하자. . | . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=0) train_cnt = y_train.count() test_cnt = y_test.count() print(&#39; n학습 세트 Shape:{0}, 테스트 세트 Shape:{1}&#39;.format(X_train.shape , X_test.shape)) print(&#39; n학습 세트 레이블 값 분포 비율&#39;) print(y_train.value_counts()/train_cnt) print(&#39; n 테스트 세트 레이블 값 분포 비율&#39;) print(y_test.value_counts()/test_cnt,&#39; n&#39;) . 학습 세트 Shape:(60816, 369), 테스트 세트 Shape:(15204, 369) 학습 세트 레이블 값 분포 비율 0 0.960964 1 0.039036 Name: TARGET, dtype: float64 테스트 세트 레이블 값 분포 비율 0 0.9583 1 0.0417 Name: TARGET, dtype: float64 . 학습과 테스트 데이터 세트 모두 Target의 값의 분포가 원본 데이터와 유사. | . XGBoost의 학습 모델을 생성하고 예측 결과를 ROC AUC로 평가해보자 | . from xgboost import XGBClassifier from sklearn.metrics import roc_auc_score # n_estimators는 500으로, random state는 예제 수행 시마다 동일 예측 결과를 위해 설정. xgb_clf = XGBClassifier(n_estimators=500, random_state=156) # 성능 평가 지표를 auc로, 조기 중단 파라미터는 100으로 설정하고 학습 수행. xgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;auc&quot;, eval_set=[(X_train, y_train), (X_test, y_test)]) xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(xgb_roc_score)) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [0] validation_0-auc:0.82005 validation_1-auc:0.81157 [1] validation_0-auc:0.83400 validation_1-auc:0.82452 [2] validation_0-auc:0.83870 validation_1-auc:0.82746 [3] validation_0-auc:0.84419 validation_1-auc:0.82922 [4] validation_0-auc:0.84783 validation_1-auc:0.83298 [5] validation_0-auc:0.85125 validation_1-auc:0.83500 [6] validation_0-auc:0.85501 validation_1-auc:0.83653 [7] validation_0-auc:0.85831 validation_1-auc:0.83782 [8] validation_0-auc:0.86143 validation_1-auc:0.83802 [9] validation_0-auc:0.86452 validation_1-auc:0.83914 [10] validation_0-auc:0.86717 validation_1-auc:0.83954 [11] validation_0-auc:0.87013 validation_1-auc:0.83983 [12] validation_0-auc:0.87369 validation_1-auc:0.84033 [13] validation_0-auc:0.87620 validation_1-auc:0.84054 [14] validation_0-auc:0.87799 validation_1-auc:0.84135 [15] validation_0-auc:0.88072 validation_1-auc:0.84117 [16] validation_0-auc:0.88238 validation_1-auc:0.84101 [17] validation_0-auc:0.88354 validation_1-auc:0.84071 [18] validation_0-auc:0.88458 validation_1-auc:0.84052 [19] validation_0-auc:0.88592 validation_1-auc:0.84023 [20] validation_0-auc:0.88790 validation_1-auc:0.84012 [21] validation_0-auc:0.88846 validation_1-auc:0.84022 [22] validation_0-auc:0.88980 validation_1-auc:0.84007 [23] validation_0-auc:0.89019 validation_1-auc:0.84009 [24] validation_0-auc:0.89195 validation_1-auc:0.83974 [25] validation_0-auc:0.89255 validation_1-auc:0.84015 [26] validation_0-auc:0.89332 validation_1-auc:0.84101 [27] validation_0-auc:0.89389 validation_1-auc:0.84088 [28] validation_0-auc:0.89420 validation_1-auc:0.84074 [29] validation_0-auc:0.89665 validation_1-auc:0.83999 [30] validation_0-auc:0.89741 validation_1-auc:0.83959 [31] validation_0-auc:0.89916 validation_1-auc:0.83952 [32] validation_0-auc:0.90106 validation_1-auc:0.83901 [33] validation_0-auc:0.90253 validation_1-auc:0.83885 [34] validation_0-auc:0.90278 validation_1-auc:0.83887 [35] validation_0-auc:0.90293 validation_1-auc:0.83864 [36] validation_0-auc:0.90463 validation_1-auc:0.83834 [37] validation_0-auc:0.90500 validation_1-auc:0.83810 [38] validation_0-auc:0.90519 validation_1-auc:0.83810 [39] validation_0-auc:0.90533 validation_1-auc:0.83813 [40] validation_0-auc:0.90575 validation_1-auc:0.83776 [41] validation_0-auc:0.90691 validation_1-auc:0.83720 [42] validation_0-auc:0.90716 validation_1-auc:0.83684 [43] validation_0-auc:0.90737 validation_1-auc:0.83672 [44] validation_0-auc:0.90759 validation_1-auc:0.83674 [45] validation_0-auc:0.90769 validation_1-auc:0.83693 [46] validation_0-auc:0.90779 validation_1-auc:0.83686 [47] validation_0-auc:0.90793 validation_1-auc:0.83678 [48] validation_0-auc:0.90831 validation_1-auc:0.83694 [49] validation_0-auc:0.90871 validation_1-auc:0.83676 [50] validation_0-auc:0.90892 validation_1-auc:0.83655 [51] validation_0-auc:0.91070 validation_1-auc:0.83669 [52] validation_0-auc:0.91240 validation_1-auc:0.83641 [53] validation_0-auc:0.91354 validation_1-auc:0.83690 [54] validation_0-auc:0.91389 validation_1-auc:0.83693 [55] validation_0-auc:0.91408 validation_1-auc:0.83681 [56] validation_0-auc:0.91548 validation_1-auc:0.83680 [57] validation_0-auc:0.91560 validation_1-auc:0.83667 [58] validation_0-auc:0.91631 validation_1-auc:0.83664 [59] validation_0-auc:0.91729 validation_1-auc:0.83591 [60] validation_0-auc:0.91765 validation_1-auc:0.83576 [61] validation_0-auc:0.91788 validation_1-auc:0.83534 [62] validation_0-auc:0.91876 validation_1-auc:0.83513 [63] validation_0-auc:0.91896 validation_1-auc:0.83510 [64] validation_0-auc:0.91900 validation_1-auc:0.83508 [65] validation_0-auc:0.91911 validation_1-auc:0.83518 [66] validation_0-auc:0.91975 validation_1-auc:0.83510 [67] validation_0-auc:0.91986 validation_1-auc:0.83523 [68] validation_0-auc:0.92012 validation_1-auc:0.83457 [69] validation_0-auc:0.92019 validation_1-auc:0.83460 [70] validation_0-auc:0.92029 validation_1-auc:0.83446 [71] validation_0-auc:0.92041 validation_1-auc:0.83462 [72] validation_0-auc:0.92093 validation_1-auc:0.83394 [73] validation_0-auc:0.92099 validation_1-auc:0.83410 [74] validation_0-auc:0.92140 validation_1-auc:0.83394 [75] validation_0-auc:0.92148 validation_1-auc:0.83368 [76] validation_0-auc:0.92330 validation_1-auc:0.83413 [77] validation_0-auc:0.92424 validation_1-auc:0.83359 [78] validation_0-auc:0.92512 validation_1-auc:0.83353 [79] validation_0-auc:0.92549 validation_1-auc:0.83293 [80] validation_0-auc:0.92586 validation_1-auc:0.83253 [81] validation_0-auc:0.92686 validation_1-auc:0.83187 [82] validation_0-auc:0.92714 validation_1-auc:0.83230 [83] validation_0-auc:0.92810 validation_1-auc:0.83216 [84] validation_0-auc:0.92832 validation_1-auc:0.83206 [85] validation_0-auc:0.92878 validation_1-auc:0.83196 [86] validation_0-auc:0.92883 validation_1-auc:0.83200 [87] validation_0-auc:0.92890 validation_1-auc:0.83208 [88] validation_0-auc:0.92928 validation_1-auc:0.83174 [89] validation_0-auc:0.92950 validation_1-auc:0.83160 [90] validation_0-auc:0.92958 validation_1-auc:0.83155 [91] validation_0-auc:0.92969 validation_1-auc:0.83165 [92] validation_0-auc:0.92974 validation_1-auc:0.83172 [93] validation_0-auc:0.93042 validation_1-auc:0.83160 [94] validation_0-auc:0.93043 validation_1-auc:0.83150 [95] validation_0-auc:0.93048 validation_1-auc:0.83132 [96] validation_0-auc:0.93094 validation_1-auc:0.83090 [97] validation_0-auc:0.93102 validation_1-auc:0.83091 [98] validation_0-auc:0.93179 validation_1-auc:0.83066 [99] validation_0-auc:0.93255 validation_1-auc:0.83058 [100] validation_0-auc:0.93296 validation_1-auc:0.83029 [101] validation_0-auc:0.93370 validation_1-auc:0.82955 [102] validation_0-auc:0.93369 validation_1-auc:0.82962 [103] validation_0-auc:0.93448 validation_1-auc:0.82893 [104] validation_0-auc:0.93460 validation_1-auc:0.82837 [105] validation_0-auc:0.93494 validation_1-auc:0.82815 [106] validation_0-auc:0.93594 validation_1-auc:0.82744 [107] validation_0-auc:0.93598 validation_1-auc:0.82728 [108] validation_0-auc:0.93625 validation_1-auc:0.82651 [109] validation_0-auc:0.93632 validation_1-auc:0.82650 [110] validation_0-auc:0.93673 validation_1-auc:0.82621 [111] validation_0-auc:0.93678 validation_1-auc:0.82620 [112] validation_0-auc:0.93726 validation_1-auc:0.82591 [113] validation_0-auc:0.93797 validation_1-auc:0.82498 ROC AUC: 0.8413 . from sklearn.model_selection import GridSearchCV # 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소 xgb_clf = XGBClassifier(n_estimators=100) params = {&#39;max_depth&#39;:[5, 7] , &#39;min_child_weight&#39;:[1,3] ,&#39;colsample_bytree&#39;:[0.5, 0.75] } # 하이퍼 파라미터 테스트의 수행속도를 향상 시키기 위해 cv 를 지정하지 않음. gridcv = GridSearchCV(xgb_clf, param_grid=params) gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=&quot;auc&quot;, eval_set=[(X_train, y_train), (X_test, y_test)]) print(&#39;GridSearchCV 최적 파라미터:&#39;,gridcv.best_params_) xgb_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:,1], average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(xgb_roc_score)) . 이전 예제의 ROC AUC보다 파라미터 조정후 성능이 좀 개선 됐음, 앞에서 구한 최적화 파라미터를 기반으로 다른 하이퍼 파라미터를 변경 또는 추가해 다시 최적화를 진행해보자 | . # n_estimators는 1000으로 증가시키고, learning_rate=0.02로 감소, reg_alpha=0.03으로 추가함. xgb_clf = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.02, max_depth=7, min_child_weight=1, colsample_bytree=0.75, reg_alpha=0.03) # evaluation metric을 auc로, early stopping은 200 으로 설정하고 학습 수행. xgb_clf.fit(X_train, y_train, early_stopping_rounds=200, eval_metric=&quot;auc&quot;,eval_set=[(X_train, y_train), (X_test, y_test)]) xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(xgb_roc_score)) . 이전 테스트보다 살짝 향상된 결과를 나타내고 있음. XGBoost가 GBM보다는 빠르지만 아무래도 GBM을 기반으로 하고 있기 때문에 수행시간이 상당히 요구된다. 이 때문에 하이퍼 파라미터를 다양하게 나열해 파라미터를 튜닝하는 것은 많은 시간이 소모. 앙상블 계열 알고리즘에서 하이퍼 파라미터 튜닝으로 성능 수치 개선이 급격하게 되긴 어렵다. . | 튜닝된 모델에서 각 feature의 중요도를 feature 중요도 그래프로 나타내보자 . | . from xgboost import plot_importance import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(1,1,figsize=(10,8)) plot_importance(xgb_clf, ax=ax , max_num_features=20,height=0.4) . LightGBM으로 학습을 수행하고 ROC-AUC를 측정해보자 | . from lightgbm import LGBMClassifier lgbm_clf = LGBMClassifier(n_estimators=500) evals = [(X_test, y_test)] lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;auc&quot;, eval_set=evals, verbose=True) lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(lgbm_roc_score)) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:726: UserWarning: &#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;early_stopping()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:736: UserWarning: &#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;log_evaluation()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; . [1] valid_0&#39;s auc: 0.817384 valid_0&#39;s binary_logloss: 0.165046 [2] valid_0&#39;s auc: 0.818903 valid_0&#39;s binary_logloss: 0.160006 [3] valid_0&#39;s auc: 0.827707 valid_0&#39;s binary_logloss: 0.156323 [4] valid_0&#39;s auc: 0.832155 valid_0&#39;s binary_logloss: 0.153463 [5] valid_0&#39;s auc: 0.834677 valid_0&#39;s binary_logloss: 0.151256 [6] valid_0&#39;s auc: 0.834093 valid_0&#39;s binary_logloss: 0.149427 [7] valid_0&#39;s auc: 0.837046 valid_0&#39;s binary_logloss: 0.147961 [8] valid_0&#39;s auc: 0.837838 valid_0&#39;s binary_logloss: 0.146591 [9] valid_0&#39;s auc: 0.839435 valid_0&#39;s binary_logloss: 0.145455 [10] valid_0&#39;s auc: 0.83973 valid_0&#39;s binary_logloss: 0.144486 [11] valid_0&#39;s auc: 0.839799 valid_0&#39;s binary_logloss: 0.143769 [12] valid_0&#39;s auc: 0.840034 valid_0&#39;s binary_logloss: 0.143146 [13] valid_0&#39;s auc: 0.840271 valid_0&#39;s binary_logloss: 0.142533 [14] valid_0&#39;s auc: 0.840342 valid_0&#39;s binary_logloss: 0.142036 [15] valid_0&#39;s auc: 0.840928 valid_0&#39;s binary_logloss: 0.14161 [16] valid_0&#39;s auc: 0.840337 valid_0&#39;s binary_logloss: 0.141307 [17] valid_0&#39;s auc: 0.839901 valid_0&#39;s binary_logloss: 0.141152 [18] valid_0&#39;s auc: 0.839742 valid_0&#39;s binary_logloss: 0.141018 [19] valid_0&#39;s auc: 0.839818 valid_0&#39;s binary_logloss: 0.14068 [20] valid_0&#39;s auc: 0.839307 valid_0&#39;s binary_logloss: 0.140562 [21] valid_0&#39;s auc: 0.839662 valid_0&#39;s binary_logloss: 0.140353 [22] valid_0&#39;s auc: 0.840411 valid_0&#39;s binary_logloss: 0.140144 [23] valid_0&#39;s auc: 0.840522 valid_0&#39;s binary_logloss: 0.139983 [24] valid_0&#39;s auc: 0.840208 valid_0&#39;s binary_logloss: 0.139943 [25] valid_0&#39;s auc: 0.839578 valid_0&#39;s binary_logloss: 0.139898 [26] valid_0&#39;s auc: 0.83975 valid_0&#39;s binary_logloss: 0.139814 [27] valid_0&#39;s auc: 0.83988 valid_0&#39;s binary_logloss: 0.139711 [28] valid_0&#39;s auc: 0.839704 valid_0&#39;s binary_logloss: 0.139681 [29] valid_0&#39;s auc: 0.839432 valid_0&#39;s binary_logloss: 0.139662 [30] valid_0&#39;s auc: 0.839196 valid_0&#39;s binary_logloss: 0.139641 [31] valid_0&#39;s auc: 0.838891 valid_0&#39;s binary_logloss: 0.139654 [32] valid_0&#39;s auc: 0.838943 valid_0&#39;s binary_logloss: 0.1396 [33] valid_0&#39;s auc: 0.838632 valid_0&#39;s binary_logloss: 0.139642 [34] valid_0&#39;s auc: 0.838314 valid_0&#39;s binary_logloss: 0.139687 [35] valid_0&#39;s auc: 0.83844 valid_0&#39;s binary_logloss: 0.139668 [36] valid_0&#39;s auc: 0.839074 valid_0&#39;s binary_logloss: 0.139562 [37] valid_0&#39;s auc: 0.838806 valid_0&#39;s binary_logloss: 0.139594 [38] valid_0&#39;s auc: 0.839041 valid_0&#39;s binary_logloss: 0.139574 [39] valid_0&#39;s auc: 0.839081 valid_0&#39;s binary_logloss: 0.139587 [40] valid_0&#39;s auc: 0.839276 valid_0&#39;s binary_logloss: 0.139504 [41] valid_0&#39;s auc: 0.83951 valid_0&#39;s binary_logloss: 0.139481 [42] valid_0&#39;s auc: 0.839544 valid_0&#39;s binary_logloss: 0.139487 [43] valid_0&#39;s auc: 0.839673 valid_0&#39;s binary_logloss: 0.139478 [44] valid_0&#39;s auc: 0.839677 valid_0&#39;s binary_logloss: 0.139453 [45] valid_0&#39;s auc: 0.839703 valid_0&#39;s binary_logloss: 0.139445 [46] valid_0&#39;s auc: 0.839601 valid_0&#39;s binary_logloss: 0.139468 [47] valid_0&#39;s auc: 0.839318 valid_0&#39;s binary_logloss: 0.139529 [48] valid_0&#39;s auc: 0.839462 valid_0&#39;s binary_logloss: 0.139486 [49] valid_0&#39;s auc: 0.839288 valid_0&#39;s binary_logloss: 0.139492 [50] valid_0&#39;s auc: 0.838987 valid_0&#39;s binary_logloss: 0.139572 [51] valid_0&#39;s auc: 0.838845 valid_0&#39;s binary_logloss: 0.139603 [52] valid_0&#39;s auc: 0.838655 valid_0&#39;s binary_logloss: 0.139623 [53] valid_0&#39;s auc: 0.838783 valid_0&#39;s binary_logloss: 0.139609 [54] valid_0&#39;s auc: 0.838695 valid_0&#39;s binary_logloss: 0.139638 [55] valid_0&#39;s auc: 0.838868 valid_0&#39;s binary_logloss: 0.139625 [56] valid_0&#39;s auc: 0.838653 valid_0&#39;s binary_logloss: 0.139645 [57] valid_0&#39;s auc: 0.83856 valid_0&#39;s binary_logloss: 0.139688 [58] valid_0&#39;s auc: 0.838475 valid_0&#39;s binary_logloss: 0.139694 [59] valid_0&#39;s auc: 0.8384 valid_0&#39;s binary_logloss: 0.139682 [60] valid_0&#39;s auc: 0.838319 valid_0&#39;s binary_logloss: 0.13969 [61] valid_0&#39;s auc: 0.838209 valid_0&#39;s binary_logloss: 0.13973 [62] valid_0&#39;s auc: 0.83806 valid_0&#39;s binary_logloss: 0.139765 [63] valid_0&#39;s auc: 0.838096 valid_0&#39;s binary_logloss: 0.139749 [64] valid_0&#39;s auc: 0.838163 valid_0&#39;s binary_logloss: 0.139746 [65] valid_0&#39;s auc: 0.838183 valid_0&#39;s binary_logloss: 0.139805 [66] valid_0&#39;s auc: 0.838215 valid_0&#39;s binary_logloss: 0.139815 [67] valid_0&#39;s auc: 0.838268 valid_0&#39;s binary_logloss: 0.139822 [68] valid_0&#39;s auc: 0.83836 valid_0&#39;s binary_logloss: 0.139816 [69] valid_0&#39;s auc: 0.838114 valid_0&#39;s binary_logloss: 0.139874 [70] valid_0&#39;s auc: 0.83832 valid_0&#39;s binary_logloss: 0.139816 [71] valid_0&#39;s auc: 0.838256 valid_0&#39;s binary_logloss: 0.139818 [72] valid_0&#39;s auc: 0.838231 valid_0&#39;s binary_logloss: 0.139845 [73] valid_0&#39;s auc: 0.838028 valid_0&#39;s binary_logloss: 0.139888 [74] valid_0&#39;s auc: 0.837912 valid_0&#39;s binary_logloss: 0.139905 [75] valid_0&#39;s auc: 0.83772 valid_0&#39;s binary_logloss: 0.13992 [76] valid_0&#39;s auc: 0.837606 valid_0&#39;s binary_logloss: 0.139899 [77] valid_0&#39;s auc: 0.837521 valid_0&#39;s binary_logloss: 0.139925 [78] valid_0&#39;s auc: 0.837462 valid_0&#39;s binary_logloss: 0.139957 [79] valid_0&#39;s auc: 0.837541 valid_0&#39;s binary_logloss: 0.139944 [80] valid_0&#39;s auc: 0.838013 valid_0&#39;s binary_logloss: 0.13983 [81] valid_0&#39;s auc: 0.83789 valid_0&#39;s binary_logloss: 0.139874 [82] valid_0&#39;s auc: 0.837671 valid_0&#39;s binary_logloss: 0.139975 [83] valid_0&#39;s auc: 0.837707 valid_0&#39;s binary_logloss: 0.139972 [84] valid_0&#39;s auc: 0.837631 valid_0&#39;s binary_logloss: 0.140011 [85] valid_0&#39;s auc: 0.837496 valid_0&#39;s binary_logloss: 0.140023 [86] valid_0&#39;s auc: 0.83757 valid_0&#39;s binary_logloss: 0.140021 [87] valid_0&#39;s auc: 0.837284 valid_0&#39;s binary_logloss: 0.140099 [88] valid_0&#39;s auc: 0.837228 valid_0&#39;s binary_logloss: 0.140115 [89] valid_0&#39;s auc: 0.836964 valid_0&#39;s binary_logloss: 0.140172 [90] valid_0&#39;s auc: 0.836752 valid_0&#39;s binary_logloss: 0.140225 [91] valid_0&#39;s auc: 0.836833 valid_0&#39;s binary_logloss: 0.140221 [92] valid_0&#39;s auc: 0.836648 valid_0&#39;s binary_logloss: 0.140277 [93] valid_0&#39;s auc: 0.836648 valid_0&#39;s binary_logloss: 0.140315 [94] valid_0&#39;s auc: 0.836677 valid_0&#39;s binary_logloss: 0.140321 [95] valid_0&#39;s auc: 0.836729 valid_0&#39;s binary_logloss: 0.140307 [96] valid_0&#39;s auc: 0.8368 valid_0&#39;s binary_logloss: 0.140313 [97] valid_0&#39;s auc: 0.836797 valid_0&#39;s binary_logloss: 0.140331 [98] valid_0&#39;s auc: 0.836675 valid_0&#39;s binary_logloss: 0.140361 [99] valid_0&#39;s auc: 0.83655 valid_0&#39;s binary_logloss: 0.14039 [100] valid_0&#39;s auc: 0.836518 valid_0&#39;s binary_logloss: 0.1404 [101] valid_0&#39;s auc: 0.836998 valid_0&#39;s binary_logloss: 0.140294 [102] valid_0&#39;s auc: 0.836778 valid_0&#39;s binary_logloss: 0.140366 [103] valid_0&#39;s auc: 0.83694 valid_0&#39;s binary_logloss: 0.140333 [104] valid_0&#39;s auc: 0.836749 valid_0&#39;s binary_logloss: 0.14039 [105] valid_0&#39;s auc: 0.836752 valid_0&#39;s binary_logloss: 0.140391 [106] valid_0&#39;s auc: 0.837197 valid_0&#39;s binary_logloss: 0.140305 [107] valid_0&#39;s auc: 0.837141 valid_0&#39;s binary_logloss: 0.140329 [108] valid_0&#39;s auc: 0.8371 valid_0&#39;s binary_logloss: 0.140344 [109] valid_0&#39;s auc: 0.837136 valid_0&#39;s binary_logloss: 0.14033 [110] valid_0&#39;s auc: 0.837102 valid_0&#39;s binary_logloss: 0.140388 [111] valid_0&#39;s auc: 0.836957 valid_0&#39;s binary_logloss: 0.140426 [112] valid_0&#39;s auc: 0.836779 valid_0&#39;s binary_logloss: 0.14051 [113] valid_0&#39;s auc: 0.836831 valid_0&#39;s binary_logloss: 0.140526 [114] valid_0&#39;s auc: 0.836783 valid_0&#39;s binary_logloss: 0.14055 [115] valid_0&#39;s auc: 0.836672 valid_0&#39;s binary_logloss: 0.140585 ROC AUC: 0.8409 . 수행시간이 더 단축됐음을 알 수 있다. . | GridSearchCV로 좀 더 다양한 하이퍼 파라미터에 대한 튜닝을 수행해보자 . | . from sklearn.model_selection import GridSearchCV # 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소 LGBM_clf = LGBMClassifier(n_estimators=200) params = {&#39;num_leaves&#39;: [32, 64 ], &#39;max_depth&#39;:[128, 160], &#39;min_child_samples&#39;:[60, 100], &#39;subsample&#39;:[0.8, 1]} # 하이퍼 파라미터 테스트의 수행속도를 향상 시키기 위해 cv 를 지정하지 않습니다. gridcv = GridSearchCV(lgbm_clf, param_grid=params) gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=&quot;auc&quot;, eval_set=[(X_train, y_train), (X_test, y_test)]) print(&#39;GridSearchCV 최적 파라미터:&#39;, gridcv.best_params_) lgbm_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:,1], average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(lgbm_roc_score)) . 해당 하이퍼 파라미터를 LightGBM에 적용하고 다시 학습해 ROC-AUC 측정 결과를 도출해보자 | . lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=32, sumbsample=0.8, min_child_samples=100, max_depth=128) evals = [(X_test, y_test)] lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;auc&quot;, eval_set=evals, verbose=True) lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(lgbm_roc_score)) . 캐글 신용카드 사기 검출 . 해당 데이터 세트의 레이블인 class 속성은 매우 불균형한 분포를 가지고 있다. 단 0.1%만이 사기 transaction이며 일반적으로 사기 검출(Fraud Detection)이나 이상 검출(Anomaly Detection)과 같은 데이터 세트는 극도로 불균형한 분포를 가질 수밖에 없음. 레이블이 불균형한 분포를 가진 데이터 세트를 학습시킬 때 예측 성능의 문제가 발생할 수 있는데 이는 이상 레이블을 가지는 데이터 건수가 정상 레이블을 가진 데이터 건수에 비해 너무 적기 때문에 발생. 즉 이상 레이블을 가지는 데이터 건수는 매우 적기 때문에 제대로 다양한 유형을 학습하지 못하는 반면 정상 레이블을 가지는 데이터 건수는 매우 많기 때문에 일방적으로 정상 레이블로 치우친 학습을 수행해 제대로 된 이상 데이터 검출이 어려워지기 쉽다. 지도 학습에서 극도로 불균형한 레이블 값 분포로 인한 문제점을 해결하기 위해서는 적절한 학습 데이터를 확보하는 방안이 필요한데 대표적으로 Oversampling과 Undersampling 방법이 있으며 Oversampling 방식이 예측성능상 더 유리한 경우가 많다. . Undersampling : 많은 데이터 세트를 적은 데이터 세트 수준으로 감소시키는 방식. 10,000 : 100 비율이라면 정상 데이터 건수를 100으로 감소시키는 것. 하지만 정상 레이블의 경우 오히려 제대로 된 학습을 수행할 수 없다는 단점이 있어 잘 적용 X . | Oversampling : 이상 데이터와 같이 적은 데이터 세트를 증식하여 학습을 위한 충분한 데이터를 확보하는 방법. 동일한 데이터를 단순 증식하는 것은 과적합(Overfitting) 되기 때문에 원본 데이터의 feature값들을 아주 약간만 변형하여 증식시킴. 대표적으로 SMOTE(Synthetic Minority Over-sampling Technique)이 있다. SMOTE는 적은 데이터 세트에 있는 개별 데이터들의 K 최근접 이웃을 찾아서 이 데이터와 K개 이웃들의 차이를 일정 값으로 만들어서 기존 데이터와 약간 차이가 나는 새로운 데이터들을 생성하는 방식이다. . | . . 데이터 세트를 로딩하고 신용카드 사기 검출 모델을 생성하자 | . import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&quot;ignore&quot;) %matplotlib inline card_df = pd.read_csv(&#39;./creditcard.csv&#39;) card_df.head(3) . Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class . 0 0.0 | -1.359807 | -0.072781 | 2.536347 | 1.378155 | -0.338321 | 0.462388 | 0.239599 | 0.098698 | 0.363787 | ... | -0.018307 | 0.277838 | -0.110474 | 0.066928 | 0.128539 | -0.189115 | 0.133558 | -0.021053 | 149.62 | 0 | . 1 0.0 | 1.191857 | 0.266151 | 0.166480 | 0.448154 | 0.060018 | -0.082361 | -0.078803 | 0.085102 | -0.255425 | ... | -0.225775 | -0.638672 | 0.101288 | -0.339846 | 0.167170 | 0.125895 | -0.008983 | 0.014724 | 2.69 | 0 | . 2 1.0 | -1.358354 | -1.340163 | 1.773209 | 0.379780 | -0.503198 | 1.800499 | 0.791461 | 0.247676 | -1.514654 | ... | 0.247998 | 0.771679 | 0.909412 | -0.689281 | -0.327642 | -0.139097 | -0.055353 | -0.059752 | 378.66 | 0 | . 3 rows × 31 columns . card_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 284807 entries, 0 to 284806 Data columns (total 31 columns): # Column Non-Null Count Dtype -- -- 0 Time 284807 non-null float64 1 V1 284807 non-null float64 2 V2 284807 non-null float64 3 V3 284807 non-null float64 4 V4 284807 non-null float64 5 V5 284807 non-null float64 6 V6 284807 non-null float64 7 V7 284807 non-null float64 8 V8 284807 non-null float64 9 V9 284807 non-null float64 10 V10 284807 non-null float64 11 V11 284807 non-null float64 12 V12 284807 non-null float64 13 V13 284807 non-null float64 14 V14 284807 non-null float64 15 V15 284807 non-null float64 16 V16 284807 non-null float64 17 V17 284807 non-null float64 18 V18 284807 non-null float64 19 V19 284807 non-null float64 20 V20 284807 non-null float64 21 V21 284807 non-null float64 22 V22 284807 non-null float64 23 V23 284807 non-null float64 24 V24 284807 non-null float64 25 V25 284807 non-null float64 26 V26 284807 non-null float64 27 V27 284807 non-null float64 28 V28 284807 non-null float64 29 Amount 284807 non-null float64 30 Class 284807 non-null int64 dtypes: float64(30), int64(1) memory usage: 67.4 MB . null count 없음. | . from sklearn.model_selection import train_test_split # 인자로 입력받은 DataFrame을 복사 한 뒤 Time 컬럼만 삭제하고 복사된 DataFrame 반환 def get_preprocessed_df(df=None): df_copy = df.copy() df_copy.drop(&#39;Time&#39;, axis=1, inplace=True) return df_copy # 사전 데이터 가공 후 학습과 테스트 데이터 세트를 반환하는 함수. def get_train_test_dataset(df=None): # 인자로 입력된 DataFrame의 사전 데이터 가공이 완료된 복사 DataFrame 반환 df_copy = get_preprocessed_df(df) # DataFrame의 맨 마지막 컬럼이 레이블, 나머지는 피처들 X_features = df_copy.iloc[:, :-1] y_target = df_copy.iloc[:, -1] # train_test_split( )으로 학습과 테스트 데이터 분할. stratify=y_target으로 Stratified 기반 분할 X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target) # 학습과 테스트 데이터 세트 반환 return X_train, X_test, y_train, y_test X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) . 생성한 학습 데이터 세트와 테스트 데이터 세트의 레이블 값 비율이 비슷하게 분할된 것을 알 수 있다. . print(&#39;학습 데이터 레이블 값 비율&#39;) print(y_train.value_counts()/y_train.shape[0] * 100) print(&#39;테스트 데이터 레이블 값 비율&#39;) print(y_test.value_counts()/y_test.shape[0] * 100) . 학습 데이터 레이블 값 비율 0 99.827451 1 0.172549 Name: Class, dtype: float64 테스트 데이터 레이블 값 비율 0 99.826785 1 0.173215 Name: Class, dtype: float64 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/08/intro.html",
            "relUrl": "/2022/01/08/intro.html",
            "date": " • Jan 8, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "2022/01/07/FRI",
            "content": "Random Forest . 배깅은 앞서 소개한 보팅과는 달리 같은 알고리즘으로 여러 개의 분류기를 만들어서 보팅으로 최종 결정하는 알고리즘이다. 그 중 랜덤 포레스트의 기반 알고리즘은 결정 트리이다. 개별 트리가 학습하는 데이터 세트는 전체 데이터 세트에서 일부가 중첩되게 샘플링된 데이터 세트이다. 이렇게 여러 개의 데이터 세트를 중첩되게 분리하는 것을 부트스트래핑 분할 방식이라 한다. 이때 서브 데이터 건수는 전체 데이터 건수와 동일(218p참고),사이킷런은 RandomForestClassifier 클래스를 통해 랜덤 포레스트 기반의 분류를 지원한다. . from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score import pandas as pd import warnings warnings.filterwarnings(&#39;ignore&#39;) def get_human_dataset( ): # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당. feature_name_df = pd.read_csv(&#39;./human_activity/features.txt&#39;,sep=&#39; s+&#39;, header=None,names=[&#39;column_index&#39;,&#39;column_name&#39;]) # 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. new_feature_name_df = get_new_feature_name_df(feature_name_df) # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환 feature_name = new_feature_name_df.iloc[:, 1].values.tolist() # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용 X_train = pd.read_csv(&#39;./human_activity/train/X_train.txt&#39;,sep=&#39; s+&#39;, names=feature_name ) X_test = pd.read_csv(&#39;./human_activity/test/X_test.txt&#39;,sep=&#39; s+&#39;, names=feature_name) # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여 y_train = pd.read_csv(&#39;./human_activity/train/y_train.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) y_test = pd.read_csv(&#39;./human_activity/test/y_test.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) # 로드된 학습/테스트용 DataFrame을 모두 반환 return X_train, X_test, y_train, y_test def get_new_feature_name_df(old_feature_name_df): #column_name으로 중복된 컬럼명에 대해서는 중복 차수 부여, col1, col1과 같이 2개의 중복 컬럼이 있을 경우 1, 2 feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(&#39;column_name&#39;).cumcount(), columns=[&#39;dup_cnt&#39;]) # feature_dup_df의 index인 column_name을 reset_index()를 이용하여 컬럼으로 변환. feature_dup_df = feature_dup_df.reset_index() # 인자로 받은 features_txt의 컬럼명 DataFrame과 feature_dup_df를 조인. new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=&#39;outer&#39;) # 새로운 컬럼명은 앞에 중복 차수를 접미어로 결합. new_feature_name_df[&#39;column_name&#39;] = new_feature_name_df[[&#39;column_name&#39;, &#39;dup_cnt&#39;]].apply(lambda x : x[0]+&#39;_&#39;+str(x[1]) if x[1] &gt;0 else x[0] , axis=1) new_feature_name_df = new_feature_name_df.drop([&#39;index&#39;], axis=1) return new_feature_name_df # 결정 트리에서 사용한 get_human_dataset( )을 이용해 학습/테스트용 DataFrame 반환 X_train, X_test, y_train, y_test = get_human_dataset() # 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가 rf_clf = RandomForestClassifier(random_state=0) rf_clf.fit(X_train , y_train) pred = rf_clf.predict(X_test) accuracy = accuracy_score(y_test , pred) print(&#39;랜덤 포레스트 정확도: {0:.4f}&#39;.format(accuracy)) . 랜덤 포레스트 정확도: 0.9253 . . GridSearchCV를 이용해 랜덤 포레스트의 하이퍼 파라미터를 튜닝해보자 . from sklearn.model_selection import GridSearchCV params = { &#39;n_estimators&#39;:[100], &#39;max_depth&#39; : [6, 8, 10, 12], &#39;min_samples_leaf&#39; : [8, 12, 18 ], &#39;min_samples_split&#39; : [8, 16, 20] } # RandomForestClassifier 객체 생성 후 GridSearchCV 수행 rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1) grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 ) grid_cv.fit(X_train , y_train) print(&#39;최적 하이퍼 파라미터: n&#39;, grid_cv.best_params_) print(&#39;최고 예측 정확도: {0:.4f}&#39;.format(grid_cv.best_score_)) . 최적 하이퍼 파라미터: {&#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 8, &#39;min_samples_split&#39;: 8, &#39;n_estimators&#39;: 100} 최고 예측 정확도: 0.9180 . n_estimators를 300으로 증가시키고, 최적화 하이퍼 파라미터로 다시 RandomForestClassifier를 학습시킨 뒤 이번에는 별도의 데이터 테스트 세트에서 예측을 수행해보자 | . rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8, min_samples_split=8, random_state=0) rf_clf1.fit(X_train , y_train) pred = rf_clf1.predict(X_test) print(&#39;예측 정확도: {0:.4f}&#39;.format(accuracy_score(y_test , pred))) . 예측 정확도: 0.9165 . feature_importances_를 이용해 알고리즘이 선택한 feature의 중요도를 알아보자 | . import matplotlib.pyplot as plt import seaborn as sns ftr_importances_values = rf_clf1.feature_importances_ ftr_importances = pd.Series(ftr_importances_values,index=X_train.columns ) ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] plt.figure(figsize=(8,6)) plt.title(&#39;Feature importances Top 20&#39;) sns.barplot(x=ftr_top20 , y = ftr_top20.index) plt.show() . GBM(Gradient Boosting Machine) . 부스팅 알고리즘은 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식이다. 부스팅의 대표적인 구현은 AdaBoost(Adaptive boosting)와 그래디언트 부스트가 있다. 에이다 부스트는 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 대표적인 알고리즘이다. (223p의 그림을 통해 에이다 부스트의 진행 과정에 대해 알아보자) 예를 들어 첫번째 약한 학습기는 가중치 0.3을 부여하며, 두번째 학습기는 가중치 0.5 이런 식으로 가중치를 늘려가며 학습을 진행한다. . GBM도 에이다 부스트와 유사하나 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용한다. 오류값은 실제 값-예측 값이며, 이 오류값을 최소화하는 방향성을 가지고 반복적으로 가중치 값을 업데이트 하는 것이 경사하강법이다. . 사이킷런은 GBM 기반의 분류를 위해서 GradientBoostingClassifier클래스를 제공. . from sklearn.ensemble import GradientBoostingClassifier import time import warnings warnings.filterwarnings(&#39;ignore&#39;) X_train, X_test, y_train, y_test = get_human_dataset() # GBM 수행 시간 측정을 위함. 시작 시간 설정. start_time = time.time() gb_clf = GradientBoostingClassifier(random_state=0) gb_clf.fit(X_train , y_train) gb_pred = gb_clf.predict(X_test) gb_accuracy = accuracy_score(y_test, gb_pred) print(&#39;GBM 정확도: {0:.4f}&#39;.format(gb_accuracy)) print(&quot;GBM 수행 시간: {0:.1f} 초 &quot;.format(time.time() - start_time)) . GBM 정확도: 0.9389 GBM 수행 시간: 508.8 초 . 8분이나 걸렸음 ; . (트리 기반 자체의 파라미터더 동일하게 적용되며, GBM에서 사용하는 하이퍼 파라미터 및 튜닝에 대해 225p 참고) . GridSearchCV를 이용해 하이퍼 파라미터를 최적화해보자. 다만 꽤 오랜 시간이 걸릴 것으로 예상되어 markdown처리 하겠다. . from sklearn.model_selection import GridSearchCV params = { &#39;n_estimators&#39;:[100, 500], &#39;learning_rate&#39; : [ 0.05, 0.1] } grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=2 ,verbose=1) grid_cv.fit(X_train , y_train) print(&#39;최적 하이퍼 파라미터: n&#39;, grid_cv.best_params_) print(&#39;최고 예측 정확도: {0:.4f}&#39;.format(grid_cv.best_score_)) . 이 설정을 그대로 테스트 데이터 세트에 적용해 예측 정확도를 확인해보자. 동일하게 markdown처리 하겠음 . # GridSearchCV를 이용하여 최적으로 학습된 estimator로 predict 수행. gb_pred = grid_cv.best_estimator_.predict(X_test) gb_accuracy = accuracy_score(y_test, gb_pred) print(&#39;GBM 정확도: {0:.4f}&#39;.format(gb_accuracy)) . 이처럼 GBM은 과적합에도 강한 뛰어난 예측성능을 가진 알고리즘이지만 수행 시간이 오래 걸린다. . 이제 머신러닝 세계에서 가장 각광 받고 있는 두 개의 그래디언트 부스팅 기반 패키지에 대해 알아보자 . . XGBoost . 트리 기반임. 분류에 있어서 일반적으로 다른 머신러닝보다 뛰어난 예측성능을 나타낸다. GBM에 기반하지만 더울 빠른 수행 시간 및 과적합 규제 부재 등의 문제를 해결해 각광받음. 병렬 CPU환경에서 병렬학습이 가능해 수행시간 UP,다만 일반적인 GBM에 비해 빠르다는 것이지 다른 머신러닝 알고리즘(예를 들어 랜덤 포레스트)에 비해 빠르다는 것은 아니다. . 파이썬 래퍼 XGBoost를 적용하여 위스콘신 유방암을 예측해보자 XGBoost의 파이썬 패키지인 xgboost는 자체적으로 교차 검증, 성능 평가, feature중요도등의 시각화 기능이 있음. 또한 조기 중단 기능이 있어 num_rounds로 지정한 부스팅 반복 횟수에 도달하지 않더라도 더 이상 예측 오류가 개선되지 않으면 반복을 끝까지 수행하지 않고 중지해 수행시간을 개선할 수 있음. . import xgboost as xgb from xgboost import plot_importance import pandas as pd import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split import warnings warnings.filterwarnings(&#39;ignore&#39;) dataset = load_breast_cancer() X_features= dataset.data y_label = dataset.target cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names) cancer_df[&#39;target&#39;]= y_label cancer_df.head(3) . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension ... worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension target . 0 17.99 | 10.38 | 122.8 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | 0.07871 | ... | 17.33 | 184.6 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | 0 | . 1 20.57 | 17.77 | 132.9 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | 0.05667 | ... | 23.41 | 158.8 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | 0 | . 2 19.69 | 21.25 | 130.0 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | 0.05999 | ... | 25.53 | 152.5 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | 0 | . 3 rows × 31 columns . 종양의 크기와 모양에 관한 많은 속성이 숫자형 값으로 돼 있음. 맨 마지막 column인 target label값의 종류는 악성인 &#39;malignant&#39;가 0값으로 양성인&#39;benign&#39;이 1값으로 돼있음. | . print(dataset.target_names) print(cancer_df[&#39;target&#39;].value_counts()) . [&#39;malignant&#39; &#39;benign&#39;] 1 357 0 212 Name: target, dtype: int64 . 전체 데이터 세트 중 80%를 학습용으로 20%를 테스트 용으로 분할해보자 | . X_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=156 ) print(X_train.shape , X_test.shape) . (455, 30) (114, 30) . 파이썬 래퍼 XGBoost는 사이킷런과 여러 차이가 있지만 눈에 띄는 차이는 학습용 테스트용 데이터 세트를 위해 별도의 객체인 DMatrix를 생성한다는 점이다. DMatrix는 주로 넘파이 입력 파라미터를 받아서 만들어지는 XGBoost만의 전용 데이터 세트이다. | . dtrain = xgb.DMatrix(data=X_train , label=y_train) dtest = xgb.DMatrix(data=X_test , label=y_test) . params = { &#39;max_depth&#39;:3, &#39;eta&#39;: 0.1, &#39;objective&#39;:&#39;binary:logistic&#39;, &#39;eval_metric&#39;:&#39;logloss&#39;, &#39;early_stoppings&#39;:100 } num_rounds = 400 . 이제 지정된 하이퍼 파라미터로 XGBoost 모델을 학습시켜보자 . # train 데이터 셋은 ‘train’ , evaluation(test) 데이터 셋은 ‘eval’ 로 명기합니다. wlist = [(dtrain,&#39;train&#39;),(dtest,&#39;eval&#39;) ] # 하이퍼 파라미터와 early stopping 파라미터를 train( ) 함수의 파라미터로 전달 xgb_model = xgb.train(params = params , dtrain=dtrain , num_boost_round=num_rounds , evals=wlist ) . train()으로 학습을 수행하면 반복 시 train-error와 eval_logloss가 지속적으로 감소함. xgboost를 이용해 모델의 학습이 완료됐으면 이를 이용해 테스트 데이터 세트에 예측을 수행. . pred_probs = xgb_model.predict(dtest) print(&#39;predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨&#39;) print(np.round(pred_probs[:10],3)) # 예측 확률이 0.5 보다 크면 1 , 그렇지 않으면 0 으로 예측값 결정하여 List 객체인 preds에 저장 preds = [ 1 if x &gt; 0.5 else 0 for x in pred_probs ] print(&#39;예측값 10개만 표시:&#39;,preds[:10]) . predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨 [0.95 0.003 0.9 0.086 0.993 1. 1. 0.999 0.998 0. ] 예측값 10개만 표시: [1, 0, 1, 0, 1, 1, 1, 1, 1, 0] . from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.metrics import precision_score, recall_score from sklearn.metrics import f1_score, roc_auc_score # 수정된 get_clf_eval() 함수 def get_clf_eval(y_test, pred=None, pred_proba=None): confusion = confusion_matrix( y_test, pred) accuracy = accuracy_score(y_test , pred) precision = precision_score(y_test , pred) recall = recall_score(y_test , pred) f1 = f1_score(y_test,pred) # ROC-AUC 추가 roc_auc = roc_auc_score(y_test, pred_proba) print(&#39;오차 행렬&#39;) print(confusion) # ROC-AUC print 추가 print(&#39;정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}&#39;.format(accuracy, precision, recall, f1, roc_auc)) . . xgboost 패키지에 내장된 시각화 기능을 수행해보자 . import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(figsize=(10, 12)) plot_importance(xgb_model, ax=ax) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}, xlabel=&#39;F score&#39;, ylabel=&#39;Features&#39;&gt; . 파이썬 래퍼 XGBoost는 사이킷런의 GridSearchCV와 유사하게 데이터 세트에 대한 교차 검증 수행 후 최적 파라미터를 구할 수 있는 방법을 cv() API로 제공. xgb.cv의 반환값은 DataFrame 형태임. . XGBoost를 위한 사이킷런 래퍼는 사이킷런과 호환돼 편리하게 사용할 수 있기 때문에 앞으로는 파이썬 래퍼XGBoost가 아닌 사이킷런 래퍼 XGBoost를 사용하겠음 . XGBoost는 크게 분류를 위한 XGBClassifier, 회귀를 위한 래퍼 클래스인 XGBRegressor가 있음 . from xgboost import XGBClassifier xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3) xgb_wrapper.fit(X_train , y_train) w_preds = xgb_wrapper.predict(X_test) w_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1] . [12:59:52] WARNING: .. src learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;multi:softprob&#39; was changed from &#39;merror&#39; to &#39;mlogloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. . . 앞 예제의 파이썬 래퍼 XGBoost와 동일한 평가 결과가 나옴을 알 수 있다. 사이킷런 래퍼 XGBoost에서도 조기 중단을 수행할 수 있다. 조기 중단 관련 파라미터를 fit()에 입력하면 된다. . from xgboost import XGBClassifier xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3) evals = [(X_test, y_test)] xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;logloss&quot;, eval_set=evals, verbose=True) ws100_preds = xgb_wrapper.predict(X_test) ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1] . n_estimator를 400으로 설정해도 400번 수행하지 않고 311번 반복한 후 학습을 완료함. (211번에서 311번까지 early_stopping_rounds=100으로 지정된 100번의 반복 동안 성능 평가 지수가 향상되지 않았기 때문에) . get_clf_eval(y_test , ws100_preds, ws100_pred_proba) . 조기 중단이 적용되지 않은 결과보다 약간 저조한 성능을 나타냈지만, 큰 차이는 아님 . 조기 중단값을 너무 급격하게 줄이면 예측 성능이 저하될 우려가 크다. 만일 early_stopping_rounds를 10으로 하면 아직 성능이 향상될 여지가 있음에도 불구하고 10번 반복하는 동안 성능 평가 지표가 향상디지 않으면 반복이 멈춰 버려서 충분한 학습이 되지 않아 예측 성능이 나빠질 수 있다. . feature의 중요도를 시각화하는 모듈인 plot_importance() API에 사이킷런 래퍼 클래스를 입력해도 앞에서 파이썬 래퍼 클래스를 입력한 결과와 똑같이 시각화 결과를 얻을 수 있음 | . from xgboost import plot_importance import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(figsize=(10,12)) # 사이킷런 래퍼 클래스를 입력해도 무방. plot_importance(xgb_wrapper, ax=ax) . 그림이 상당히 지저분해서 markdown처리 하겠음 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/07/FRI.html",
            "relUrl": "/2022/01/07/FRI.html",
            "date": " • Jan 7, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "2022/01/06/THU",
            "content": "결정 트리를 이용해 데이터 세트에 대한 예측 분류를 수행해보자 . import pandas as pd import matplotlib.pyplot as plt # features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드. feature_name_df = pd.read_csv(&#39;./human_activity/features.txt&#39;,sep=&#39; s+&#39;, header=None,names=[&#39;column_index&#39;,&#39;column_name&#39;]) # 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출 feature_name = feature_name_df.iloc[:, 1].values.tolist() print(&#39;전체 피처명에서 10개만 추출:&#39;, feature_name[:10]) feature_name_df.head(20) . 전체 피처명에서 10개만 추출: [&#39;tBodyAcc-mean()-X&#39;, &#39;tBodyAcc-mean()-Y&#39;, &#39;tBodyAcc-mean()-Z&#39;, &#39;tBodyAcc-std()-X&#39;, &#39;tBodyAcc-std()-Y&#39;, &#39;tBodyAcc-std()-Z&#39;, &#39;tBodyAcc-mad()-X&#39;, &#39;tBodyAcc-mad()-Y&#39;, &#39;tBodyAcc-mad()-Z&#39;, &#39;tBodyAcc-max()-X&#39;] . column_index column_name . 0 1 | tBodyAcc-mean()-X | . 1 2 | tBodyAcc-mean()-Y | . 2 3 | tBodyAcc-mean()-Z | . 3 4 | tBodyAcc-std()-X | . 4 5 | tBodyAcc-std()-Y | . 5 6 | tBodyAcc-std()-Z | . 6 7 | tBodyAcc-mad()-X | . 7 8 | tBodyAcc-mad()-Y | . 8 9 | tBodyAcc-mad()-Z | . 9 10 | tBodyAcc-max()-X | . 10 11 | tBodyAcc-max()-Y | . 11 12 | tBodyAcc-max()-Z | . 12 13 | tBodyAcc-min()-X | . 13 14 | tBodyAcc-min()-Y | . 14 15 | tBodyAcc-min()-Z | . 15 16 | tBodyAcc-sma() | . 16 17 | tBodyAcc-energy()-X | . 17 18 | tBodyAcc-energy()-Y | . 18 19 | tBodyAcc-energy()-Z | . 19 20 | tBodyAcc-iqr()-X | . feature명을 보면 인체의 움직임과 관련된 속성의 평균/표준편차가 X,Y,Z축 값으로 돼 있음을 유추할 수 있다. . 주의 : 위에서 feature명을 가지고 있는 feature.txt 파일은 중복된 feature명을 가지고 있음. 이 중복된 feature명들을 이용해 데이터 파일을 데이터 세트 DataFrame에 로드하면 오류가 발생. 따라서 중복된 feature명에 대해서는 원본 feature명에 _1 또는 _2를 부여해 변경한 뒤에 이를 이용해서 데이터를 DataFrame에 로드하자 . 먼저 중복된 feature명이 얼마나 있는지 확인해보자 . feature_dup_df = feature_name_df.groupby(&#39;column_name&#39;).count() print(feature_dup_df[feature_dup_df[&#39;column_index&#39;]&gt;1].count()) feature_dup_df[feature_dup_df[&#39;column_index&#39;]&gt;1].head() . column_index 42 dtype: int64 . column_index . column_name . fBodyAcc-bandsEnergy()-1,16 3 | . fBodyAcc-bandsEnergy()-1,24 3 | . fBodyAcc-bandsEnergy()-1,8 3 | . fBodyAcc-bandsEnergy()-17,24 3 | . fBodyAcc-bandsEnergy()-17,32 3 | . 총 42개의 feature명이 중복돼 있다. . 이 중복된 feature명에 대해서 원본 feature명에 _1또는 _2를 추가 부여해 새로운 feature명을 가지는 DataFrame을 반환하는 함수를 정의하자 . def get_new_feature_name_df(old_feature_name_df): #column_name으로 중복된 컬럼명에 대해서는 중복 차수 부여, col1, col1과 같이 2개의 중복 컬럼이 있을 경우 1, 2 feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(&#39;column_name&#39;).cumcount(), columns=[&#39;dup_cnt&#39;]) # feature_dup_df의 index인 column_name을 reset_index()를 이용하여 컬럼으로 변환. feature_dup_df = feature_dup_df.reset_index() # 인자로 받은 features_txt의 컬럼명 DataFrame과 feature_dup_df를 조인. new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=&#39;outer&#39;) # 새로운 컬럼명은 앞에 중복 차수를 접미어로 결합. new_feature_name_df[&#39;column_name&#39;] = new_feature_name_df[[&#39;column_name&#39;, &#39;dup_cnt&#39;]].apply(lambda x : x[0]+&#39;_&#39;+str(x[1]) if x[1] &gt;0 else x[0] , axis=1) new_feature_name_df = new_feature_name_df.drop([&#39;index&#39;], axis=1) return new_feature_name_df . import pandas as pd def get_human_dataset( ): # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당. feature_name_df = pd.read_csv(&#39;./human_activity/features.txt&#39;,sep=&#39; s+&#39;, header=None,names=[&#39;column_index&#39;,&#39;column_name&#39;]) # 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. new_feature_name_df = get_new_feature_name_df(feature_name_df) # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환 feature_name = new_feature_name_df.iloc[:, 1].values.tolist() # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용 X_train = pd.read_csv(&#39;./human_activity/train/X_train.txt&#39;,sep=&#39; s+&#39;, names=feature_name ) X_test = pd.read_csv(&#39;./human_activity/test/X_test.txt&#39;,sep=&#39; s+&#39;, names=feature_name) # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여 y_train = pd.read_csv(&#39;./human_activity/train/y_train.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) y_test = pd.read_csv(&#39;./human_activity/test/y_test.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) # 로드된 학습/테스트용 DataFrame을 모두 반환 return X_train, X_test, y_train, y_test X_train, X_test, y_train, y_test = get_human_dataset() . load한 학습용 feature 데이터 세트를 간략히 살펴보자 . print(&#39; n## 학습 피처 데이터셋 info() n&#39;) print(X_train.info()) . ## 학습 피처 데이터셋 info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7352 entries, 0 to 7351 Columns: 561 entries, tBodyAcc-mean()-X to angle(Z,gravityMean) dtypes: float64(561) memory usage: 31.5 MB None . 학습 데이터 세트는 7352개의 레코드로 561개의 feature를 가지고 있다. 또한 feature가 전부 float 형의 숫자 형이므로 별도의 카테고리 인코딩은 수행할 필요가 없음. . print(y_train[&#39;action&#39;].value_counts()) . 6 1407 5 1374 4 1286 1 1226 2 1073 3 986 Name: action, dtype: int64 . 레이블 값은 1,2,3,4,5,6 즉, 6개 값이고 꽤 고르게 분포돼 있음 . 이제 사이킷런의 DecisionTreeClassifier를 이용해 동작 예측 분류를 수행하보자. 먼저 DecisionTreeClassifier의 하이퍼 파라미터는 모두 default값으로 설정해 수행하고, 이때의 하이퍼 파라미터 값을 모두 추출해보자 . from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정 dt_clf = DecisionTreeClassifier(random_state=156) dt_clf.fit(X_train , y_train) pred = dt_clf.predict(X_test) accuracy = accuracy_score(y_test , pred) print(&#39;결정 트리 예측 정확도: {0:.4f}&#39;.format(accuracy)) # DecisionTreeClassifier의 하이퍼 파라미터 추출 print(&#39;DecisionTreeClassifier 기본 하이퍼 파라미터: n&#39;, dt_clf.get_params()) . 결정 트리 예측 정확도: 0.8548 DecisionTreeClassifier 기본 하이퍼 파라미터: {&#39;ccp_alpha&#39;: 0.0, &#39;class_weight&#39;: None, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: None, &#39;max_leaf_nodes&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;random_state&#39;: 156, &#39;splitter&#39;: &#39;best&#39;} . . 이번에는 결정 트리의 트리 깊이(Tree Depth)가 예측 정확도에 주는 영향을 살펴보자. 결정 트리의 경우 분류를 위해 리프 노드(클래스 결정 노드)가 될 수 있는 적합한 수준이 될 때까지 지속해서 트리의 분할을 수행하면서 깊이가 깊어진다고 했음. 다음은 GridSearchCV를 이용해 사이킷런 결정 트리의 깊이를 조절할 수 있는 하이퍼 파라미터인 max_depth 값을 변화시키면서 예측 성능을 확인해보자. max_depth를 6~24까지 계속 늘리면서 예측 성능을 측정할 것이며, 교차 검증은 4개 세트이다. . from sklearn.model_selection import GridSearchCV params = { &#39;max_depth&#39; : [ 6, 8 ,10, 12, 16 ,20, 24] } grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=&#39;accuracy&#39;, cv=5, verbose=1 ) grid_cv.fit(X_train , y_train) print(&#39;GridSearchCV 최고 평균 정확도 수치:{0:.4f}&#39;.format(grid_cv.best_score_)) print(&#39;GridSearchCV 최적 하이퍼 파라미터:&#39;, grid_cv.best_params_) . Fitting 5 folds for each of 7 candidates, totalling 35 fits GridSearchCV 최고 평균 정확도 수치:0.8513 GridSearchCV 최적 하이퍼 파라미터: {&#39;max_depth&#39;: 16} . max_depth가 16일 때 5개의 폴드 세트의 최고 평균 정확도 결과가 약 85.13%로 도출됐다. max_depth 값 증가에 따라 예측 성능이 어떻게 변하는지 확인해보자 . cv_results_df = pd.DataFrame(grid_cv.cv_results_) # max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출 cv_results_df[[&#39;param_max_depth&#39;, &#39;mean_test_score&#39;]] . param_max_depth mean_test_score . 0 6 | 0.850791 | . 1 8 | 0.851069 | . 2 10 | 0.851209 | . 3 12 | 0.844135 | . 4 16 | 0.851344 | . 5 20 | 0.850800 | . 6 24 | 0.849440 | . mean_test_score는 5개 CV 세트에서 검증용 데이터 세트의 정확도 평균 수치이다. . 깊어진 트리는 학습 데이터 세트에는 올바른 예측 결과를 가져올지 모르지만 검증 데이터 세트에서는 오히려 과적합으로 인한 성능 저하를 유발하게 됨. . 별도의 테스트 데이터 세트에서 max_depth의 변화에 따른 값을 측정해보자 . max_depths = [ 6, 8 ,10, 12, 16 ,20, 24] # max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정 for depth in max_depths: dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156) dt_clf.fit(X_train , y_train) pred = dt_clf.predict(X_test) accuracy = accuracy_score(y_test , pred) print(&#39;max_depth = {0} 정확도: {1:.4f}&#39;.format(depth , accuracy)) . max_depth = 6 정확도: 0.8558 max_depth = 8 정확도: 0.8707 max_depth = 10 정확도: 0.8673 max_depth = 12 정확도: 0.8646 max_depth = 16 정확도: 0.8575 max_depth = 20 정확도: 0.8548 max_depth = 24 정확도: 0.8548 . max_depth = 8 일 때 가장 높은 정확도를 가짐. 8이후부터 정확도 계속 감소중. 즉 트리 깊이가 깊어질수록 테스트 데이터 세트의 정확도는 더 떨어진다. 이처럼 결정 트리는 깊이가 깊어질수록 과적합의 영향력이 커지므로 하이퍼 파라미터를 이용해 깊이를 제어해야 한다. . max_depth와 min_sample_split을 같이 변경하면서 정확도 성능을 튜닝해보자 . params = { &#39;max_depth&#39; : [ 8 , 12, 16 ,20], &#39;min_samples_split&#39; : [16,24], } grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=&#39;accuracy&#39;, cv=5, verbose=1 ) grid_cv.fit(X_train , y_train) print(&#39;GridSearchCV 최고 평균 정확도 수치: {0:.4f}&#39;.format(grid_cv.best_score_)) print(&#39;GridSearchCV 최적 하이퍼 파라미터:&#39;, grid_cv.best_params_) . Fitting 5 folds for each of 8 candidates, totalling 40 fits GridSearchCV 최고 평균 정확도 수치: 0.8549 GridSearchCV 최적 하이퍼 파라미터: {&#39;max_depth&#39;: 8, &#39;min_samples_split&#39;: 16} . max_depth가 8일 때, min_samples_split이 16일 때 가장 최고의 정확도. . 별도 분리된 테스트 데이터 세트에 해당 하이퍼 파라미터를 적용해보자 . (앞 예제의 GridSearchCV객체인 grid_cv의 속성인 best_estimator_는 최적 하이퍼 파라미터인 max_depth와 min_samples_split이 각각 8과 16으로 학습이 완료된 Estimator 객체이다) . best_df_clf = grid_cv.best_estimator_ pred1 = best_df_clf.predict(X_test) accuracy = accuracy_score(y_test , pred1) print(&#39;결정 트리 예측 정확도:{0:.4f}&#39;.format(accuracy)) . 결정 트리 예측 정확도:0.8717 . 마지막으로 결정 트리에서 각 feature의 중요도를 feature_importances_속성을 이용해 알아보자 . import seaborn as sns ftr_importances_values = best_df_clf.feature_importances_ # Top 중요도로 정렬을 쉽게 하고, 시본(Seaborn)의 막대그래프로 쉽게 표현하기 위해 Series변환 ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns ) # 중요도값 순으로 Series를 정렬 ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] plt.figure(figsize=(8,6)) plt.title(&#39;Feature importances Top 20&#39;) sns.barplot(x=ftr_top20 , y = ftr_top20.index) plt.show() . 막대 그래프상에서 확인해보면 이 중 가장 높은 중요도를 가진 Top5의 feature들이 매우 중요하게 규칙 생성에 영향을 미치고 있음을 알 수 있다. . . &#50521;&#49345;&#48660; &#54617;&#49845; . 앙상블 학습(Ensemble Learning)을 통한 분류는 여러 개의 분류기(Classifier)를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법. 단일 분류기(Classifier)보다 신뢰성이 높은 예측값을 얻을 수 있다. . 대부분의 정형 데이터 분류 시에는 앙상블이 뛰어난 성능을 나타내고 있다. . | 앙상블 학습의 유형은 전통적으로 보팅(Voting),배깅(Bagging), 부스팅(Boosting)의 세 가지로 나눌 수 있으며, 이외에도 스태킹(Stacking)을 포함한 다양한 앙상블 방법이 있음. . | . 보팅과 배깅은 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식. . . 보팅의 경우 일반적으로 서로 다른 알고리즘을 가진 분류기를 결합. 배깅은 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 보팅을 수행하는 것. | . 대표적인 배깅 방식이 랜덤 포레스트 알고리즘이다. . | 배깅은 학습하는 데이터 세트가 보팅 방식과는 다르다. 개별 분류기에 할당된 학습 데이터는 원본 학습 데이터를 샘플링해 추출하는데 이렇게 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식을 부트스트래핑(Bootstrapping) 분할 방식이라고 부름 . | 교차 검증이 데이터 세트간 중첩을 허용하지 않는 것과 다르게 배깅 방식은 중첩을 허용. . | 앙상블 학습의 유형 중 부스팅은 여러개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측한 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에게는 가중치(weight)를 부여하면서 학습과 예측을 진행. . | 대표적 부스틴 모듈로 그래디언트 부스트,XGBoost, LightGBM이 있다. . | 앙상블 학습의 유형 중 스태킹은 여러 가지 다른 모델의 예측 결과값을 다시 학습 데이터로 만들어서 다른 모델(메타 모델)로 재학습시켜 결과를 예측하는 방법 . | . . &#54616;&#46300; &#48372;&#54021;&#44284; &#49548;&#54532;&#53944; &#48372;&#54021; . 하드 보팅 : 다수결, 예측한 결과값들중 다수의 분류기가 결정한 예측값을 최종 보팅 결과값으로 선정 . 소프트 보팅 : 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결과값으로 선정.(일반적으로 이 보팅방식이 보팅 방법으로 적용됨) . 사이킷런은 보팅 방식의 앙상블을 구현한 VotingClassifier 클래스를 제공 | . import pandas as pd from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score cancer = load_breast_cancer() data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names) data_df.head(3) . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension ... worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension . 0 17.99 | 10.38 | 122.8 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | 0.07871 | ... | 25.38 | 17.33 | 184.6 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | . 1 20.57 | 17.77 | 132.9 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | 0.05667 | ... | 24.99 | 23.41 | 158.8 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | . 2 19.69 | 21.25 | 130.0 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | 0.05999 | ... | 23.57 | 25.53 | 152.5 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | . 3 rows × 30 columns . 로지스틱 회귀와 KNN을 기반으로 하여 소프트 보팅 방식으로 새롭게 보팅 분류기를 만들어보자 . VotingClassifier 클래스를 이용해 보팅 분류기를 만들 수 있으며, VotingClassifier 클래스는 주요 생성 인자로 estimators와 voting값을 입력받음. estimators는 리스트 값으로 보팅에 사용될 여러 개의 Classifier 객체들을 튜플 형식으로 입력 받으며 voting은 hard,soft선택. default는 hard . lr_clf = LogisticRegression() knn_clf = KNeighborsClassifier(n_neighbors=8) # 개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기 vo_clf = VotingClassifier( estimators=[(&#39;LR&#39;,lr_clf),(&#39;KNN&#39;,knn_clf)] , voting=&#39;soft&#39; ) X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2 , random_state= 156) # VotingClassifier 학습/예측/평가. vo_clf.fit(X_train , y_train) pred = vo_clf.predict(X_test) print(&#39;Voting 분류기 정확도: {0:.4f}&#39;.format(accuracy_score(y_test , pred))) import warnings warnings.filterwarnings(&#39;ignore&#39;) # 개별 모델의 학습/예측/평가. classifiers = [lr_clf, knn_clf] for classifier in classifiers: classifier.fit(X_train , y_train) pred = classifier.predict(X_test) class_name= classifier.__class__.__name__ print(&#39;{0} 정확도: {1:.4f}&#39;.format(class_name, accuracy_score(y_test , pred))) . Voting 분류기 정확도: 0.9474 LogisticRegression 정확도: 0.9386 KNeighborsClassifier 정확도: 0.9386 . C: Users ehfus Anaconda3 envs dv2021 lib site-packages sklearn linear_model _logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . 보팅 분류기의 정확도가 살짝 높음, 보팅으로 여러 개의 분류기를 결합한다고 해서 무조건 기반이 되는 분류기들 보다 예측 성능이 향상되진 않음. 데이터의 특성과 분포, 다양한 요건에 따라 오히려 기반 분류기 중 가장 좋은 분류기의 성능이 보팅했을 때보다 나을 수 있다. . 머신 러닝 모델의 성능은 이렇게 다양한 테스트 데이터에 의해 검증되므로 어떻게 높은 유연성을 가지고 현실에 대처할 수 있는가가 중요한 ML모델의 평가요소가 된다. 이런 관점에서 편향-분산 트레이드 오프는 ML이 극복해야할 중요 과제이다. .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/06/intro.html",
            "relUrl": "/2022/01/06/intro.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "2022/01/05/WED",
            "content": "지도학습은 레이블, 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식이다. 대표적인 유형으로는 분류(Classification)가 있으며 분류는 학습 데이터로 주어진 데이터의 feature와 레이블값(결정 값, 클래스 값)을 머신 러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것이다. 즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것이다. | . 분류는 다양한 머신러닝 알고리즘으로 구현할 수 있다. 베이즈 통계와 생성 모델에 기반한 나이브 베이즈 | 독립 변수와 종속 변수의 선형 관계성에 기반한 로지스틱 회귀 | 데이터 균일도에 따른 규칙 기반의 결정 트리 | 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신 | 근접 거리를 기준으로 하는 최소 근접 알고리즘 | 심층 연결 기반의 신경망 | 서로 다른 (또는 같은) 머신 러닝 알고리즘을 결합한 앙상블 | . | . 그 중 앙상블에 대해 배워보자 . | 앙상블에는 서로 다른 또는 서로 동일한 알고리즘을 단순히 결합한 형태도 있으나, 일반적으로는 배깅(Bagging)과 부스팅(Boosting) 방식으로 나뉜다. . | 앙상블은 서로 다른 또는 서로 동일한 알고리즘을 결합한다고 했는데 대부분은 동일한 알고리즘을 결합한다. . | . &#44208;&#51221;&#53944;&#47532; . :데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 것이다. : 스무고개 게임과 유사하며 룰 기반의 프로그램에 적용되는 if/else를 자동으로 찾아내 예측을 위한 규칙을 만드는 알고리즘을 이해하면 된다. : 따라서 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우한다. : 규칙 노드로 표시된 노드는 규칙 조건이 되는 것이며, 리프 노드로 표시된 노드는 결절된 라벨 값 즉 클래스 값을 의미한다. : 새러운 규칙 조건 마다 서브 트리가 생성된다. : 데이터 세트에 featrue가 있고 이러한 feature가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어진다. : 하지만 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 얘기이며 이는 곧 과적합으로 이어지기 쉽다. : 즉 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높다. : 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙 필요 . 위에서 분류의 다양한 머신 러닝 알고리즘 중 결정 트리는 데이터 균일도에 따른 규칙 기반이라고 했다. 그렇다면 데이터 균일도는 무엇일까? 균일도라 하면 여러 자료가 균등하게 분포되어 있을수록 균일도가 높다고 착각할 수 있는데 그렇지 않다. 예를 들어 검은 공과 흰공이 있을 때 섞여있을 수록 균일도는 낮은 것이며 오로지 검은공으로만 이루어질 수록 균일도가 높다고 할 수 있다. | . | . 만약 눈을 감고 세 주머니에서 공을 뽑을 때 오로지 검은 공으로만 이루어진 주머니에서 공을 뽑을 때 우리는 검은 공을 쉽게 예측할 수 있다. 만약 검은공과 흰공이 섞여있는 혼잡도가 높고 균일도가 낮은 주머니에서 공을 뽑을 때 검은 공을 예측하려면 더 많은 정보가 필요로 할 것이다. | . 결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만든다. 예를 들어보자, 박스 안에 서른 개의 레고 블록이 있는데 각 레고 블록은 형태 속성으로 동그라미, 네모, 세모 색깔 속성으로 노랑, 빨강, 파랑이 있다. 이 중 노랑색 블록의 경우 모두 동그라미로 구성되고 빨강과 파랑의 경우 동그라미, 네모, 세모가 골고루 섞여 있다고 한다면 각 레고 블록을 형태와 색깔 속성으로 분류하고자 할 떄 가장 첫 번째로 만들어져야 하는 규칙 조건은 if 색깔==&#39;노란색&#39;이 될것이다. 왜냐하면 노란색 블록이면 모두 노란 동그라미 블록으로 가장 쉽게 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도 조건을 찾아 분류하는것이 가장 효율적인 분류 방식이기 때문이다. | . | . 이러한 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 정보 이득($Information$ $Gain$)지수와 지니 계수가 있다. . | 정보 이득 지수 : 1 - 엔트로피 지수(주어진 데이터 집합의 혼잡도) $ to$ 결정 트리는 이 정보 이득 지수로 분할 기준을 정한다. 정보 이득이 높은 속성을 기준으로 분할한다. . | 지니 계수 : 낮을수록 데이터 균일도가 높은 것으로 해석햐 지니 계수가 낮은 속성을 기준으로 분할한다. . | . 결정트리의 일반적인 알고리즘은 데이터 세트를 분할하는 데 가장 좋은 조건, 즉 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정한다. . - 결정 트리의 가장 큰 단점은 과적합으로 적합도가 떨어진다. 트리의 깊이가 너무 깊어지면 깊어질수록 예측도는 낮아지기에 사전에 트리의 크기를 제한하는 것이 오히려 성능 튜닝에 더 도움이 된다. . 사이킷런은 결정 트리 알고리즘을 구현한 DecisionTreeClassifier(분류를 위한 클래스)와 DecisionTreeRegressor(회귀를 위한 클래스) 클래스를 제공한다. . 두 클래스 모두 동일한 파라미터를 사용하며 파라미터에 대한 설명은 188~189p 참고 . from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import warnings warnings.filterwarnings(&#39;ignore&#39;) # DecisionTree Classifier 생성 dt_clf = DecisionTreeClassifier(random_state=156) # 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리 iris_data = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=.2, random_state=11) # DecisionTreeClassifier 학습 dt_clf.fit(X_train, y_train) from sklearn.tree import export_graphviz # export_graphviz()의 호출결과로 out_file로 지정된 tree.dot 파일을 생성함 export_graphviz(dt_clf, out_file=&quot;tree.dot&quot;, class_names=iris_data.target_names, feature_names= iris_data.feature_names, impurity=True,filled=True) import graphviz # 위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북상에서 시각화 with open(&quot;tree.dot&quot;) as f: dot_graph = f.read() graphviz.Source(dot_graph) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 petal length (cm) &lt;= 2.45 gini = 0.667 samples = 120 value = [41, 40, 39] class = setosa 1 gini = 0.0 samples = 41 value = [41, 0, 0] class = setosa 0&#45;&gt;1 True 2 petal width (cm) &lt;= 1.55 gini = 0.5 samples = 79 value = [0, 40, 39] class = versicolor 0&#45;&gt;2 False 3 petal length (cm) &lt;= 5.25 gini = 0.051 samples = 38 value = [0, 37, 1] class = versicolor 2&#45;&gt;3 6 petal width (cm) &lt;= 1.75 gini = 0.136 samples = 41 value = [0, 3, 38] class = virginica 2&#45;&gt;6 4 gini = 0.0 samples = 37 value = [0, 37, 0] class = versicolor 3&#45;&gt;4 5 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 3&#45;&gt;5 7 sepal length (cm) &lt;= 5.45 gini = 0.5 samples = 4 value = [0, 2, 2] class = versicolor 6&#45;&gt;7 12 petal length (cm) &lt;= 4.85 gini = 0.053 samples = 37 value = [0, 1, 36] class = virginica 6&#45;&gt;12 8 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 7&#45;&gt;8 9 petal length (cm) &lt;= 5.45 gini = 0.444 samples = 3 value = [0, 2, 1] class = versicolor 7&#45;&gt;9 10 gini = 0.0 samples = 2 value = [0, 2, 0] class = versicolor 9&#45;&gt;10 11 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 9&#45;&gt;11 13 sepal length (cm) &lt;= 5.95 gini = 0.444 samples = 3 value = [0, 1, 2] class = virginica 12&#45;&gt;13 16 gini = 0.0 samples = 34 value = [0, 0, 34] class = virginica 12&#45;&gt;16 14 gini = 0.0 samples = 1 value = [0, 1, 0] class = versicolor 13&#45;&gt;14 15 gini = 0.0 samples = 2 value = [0, 0, 2] class = virginica 13&#45;&gt;15 더 이상 자식 노드가 없는 노드는 리프 노드이다. 리프 노드는 최종 클래스(레이블) 값이 결정되는 노드이다. . | 리프 노드가 되려면 오직 하나의 클래스 값으로 최종 데이터가 구성되거나 리프 노드가 될 수 있는 하이퍼 파라미터 조건을 충족하면 된다. . | 자식 노드가 있는 노드는 브랜치 노드이며 자식 노드를 만들기 위한 분할 규칙 조건을 가지고 있다. . | . 루트 노드인 가장 상위 1번 노드를 설명해보자면, smaples :전체 데이터가 120개라는 의미, value의 값 각각은 레이블 0,1,2값이 가지는 데이터 수를 의미함, gini는 지니 계수, class=setosa는 하위 노드를 가질 경우에 setosa의 개수가 41개로 제일 많다는 의미임 . 이처럼 결정 트리는 규칙 생성 로직을 미리 제어하지 않으면 완벽하게 클래스 값을 구별해내기 위해 트리 노드를 계속해서 만들어간다. 이로 인해 결국 매우 복잡한 규칙 트리가 만들어져 모델이 쉽게 과적합되는 문제점을 갖게 됨, 이러한 이유로 결정 트리는 과적합이 상당히 높은 ML알고리즘이다. 이때문에 결정트리 알고리즘을 제어하는 대부분 하이퍼 파라미터는 복잡한 트리가 생성되는 것을 막기 위한 용도이다. . 따라서 결정 트리의 max_depth 하이퍼 파라미터를 제한 없음에서 3개로 설정하면 더 간단한 결정 트리가 된다. . $+$ 결정트리의 또 다른 하이퍼 파라미터 요소인 min_samples_split을 4로 설정하면 서로 다른 클래스가 혼재해 있더라도 데이터 개수가 4보다 낮아지면 더이상 Split하지 않는다. . $+$ 마지막으로 min_samples_leaf 하이퍼 파라미터 변경에 따른 결정 트리의 변화를 살펴보자, 더 이상 자식 노드가 없는 리프 노드는 클래스 결정 값이 되는데, min_samples_leaf는 이 리프 노드가 될 수 있는 샘플 데이터 건수의 최솟값을 지정함 . . 결정 트리는 균일도에 기반해 어떠한 속성을 규칙 조건으로 선택하느냐가 중요한 요건이다 . 사이킷런은 결정 트리 알고리즘이 학습을 통해 규칙을 정하는 데 있어 feature의 중요한 역할 지표를 DecisionTreeClassifier 객체의 featureimportances 속성으로 제공한다. . | 해보자 . | . import seaborn as sns import numpy as np # feature importance 추출 print(&#39;Feature importance: n{}&#39;.format(np.round(dt_clf.feature_importances_,3))) # feature 별 importance 매핑 for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_): print(&#39;{}:{:.3f}&#39;.format(name,value)) # feature importance를 column 별로 시각화 해보자 sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names) . Feature importance: [0.025 0. 0.555 0.42 ] sepal length (cm):0.025 sepal width (cm):0.000 petal length (cm):0.555 petal width (cm):0.420 . &lt;AxesSubplot:&gt; . 이들 중 petal length가 가장 feature importance가 높음을 알 수 있다. . &#44208;&#51221; &#53944;&#47532; &#44284;&#51201;&#54633; (Overfitiing) . 결정 트리가 어떻게 학습 데이터를 분할해 예측을 수행하는지와 이로 인한 과적합 문제를 시각화해보자 . # 이 메서드를 이용해 2개의 feature가 3가지 유형의 클래스 값을 가지는 데이터 세트를 만들고 이를 그래프 형태로 시각화하자. from sklearn.datasets import make_classification import matplotlib.pyplot as plt plt.title(&#39;3 Class values with 2 Features Sample data creation&#39;) # 2차원 시각화를 위해서 feature는 2개, 클래스는 3가지 유형의 분류 샘플 데이터 생성 X_features,y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=0) # 그래프 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스 값은 다른 색 plt.scatter(X_features[:,0],X_features[:,1],marker=&#39;o&#39;,c=y_labels,s=25,edgecolors=&#39;k&#39;) . &lt;matplotlib.collections.PathCollection at 0x165b900c700&gt; . x축 y축은 두 개의 feature가 각 나열된 것이며, 3개의 클래스 값 구분은 색으로 하였음 . 이제 결정 트리를 기반으로 학습해보자 . from sklearn.tree import DecisionTreeClassifier # 특정한 트리 생성 제약 없는 결정 트리의 학습과 결정 경계 시각화 dt_clf=DecisionTreeClassifier().fit(X_features, y_labels) # Classifier의 Decision Boundary를 시각화 하는 함수- 이건 몰라도 될듯 def visualize_boundary(model, X, y): fig,ax = plt.subplots() # 학습 데이타 scatter plot으로 나타내기 ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap=&#39;rainbow&#39;, edgecolor=&#39;k&#39;, clim=(y.min(), y.max()), zorder=3) ax.axis(&#39;tight&#39;) ax.axis(&#39;off&#39;) xlim_start , xlim_end = ax.get_xlim() ylim_start , ylim_end = ax.get_ylim() # 호출 파라미터로 들어온 training 데이타로 model 학습 . model.fit(X, y) # meshgrid 형태인 모든 좌표값으로 예측 수행. xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) # contourf() 를 이용하여 class boundary 를 visualization 수행. n_classes = len(np.unique(y)) contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap=&#39;rainbow&#39;, clim=(y.min(), y.max()), zorder=1) visualize_boundary(dt_clf, X_features,y_labels) . 이번에는 min_samples_leaf=6을 설정해 6개 이하의 데이터는 리프 노드를 생성할 수 잇도록 리프 노트 생성 규칙을 완화한 뒤 하이퍼 파라미터를 변경해 어떻게 결정 기준 경계가 변하는지 살펴보자 . dt_clf = DecisionTreeClassifier(min_samples_leaf=6).fit(X_features, y_labels) visualize_boundary(dt_clf, X_features, y_labels) . 이상치에 크게 반응하지 않으면서 좀 더 일반화 된 분류 규칙에 따라 분류됐음을 알 수 있다. .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/05/intro.html",
            "relUrl": "/2022/01/05/intro.html",
            "date": " • Jan 5, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "2022/01/04/TUE",
            "content": "F1 &#49828;&#53076;&#50612; . - 정밀도와 재현율이 어느 한 쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가진다. . $F1 = 2 * frac{precision * recall}{precision + recall}$ . API : f1_score() . (168~169p 코드 참고) . . ROC &#44257;&#49440;&#44284; AUC &#49828;&#53076;&#50612; . - 이진 분류의 예측 성능 측정에서 중요하게 사용됨 . ROC 곡선은 FPR을 0부터 1까지 변경하면서 TPR의 변화 값을 구함. 분류 결정 임계값을 변경하면 FPR을 0부터 1까지 변경할 수 있음. FPR을 0으로 만드려면 임계값을 1로 지정하면 된다. . API : roc_curve() . 171~172,174p 참고 . 일반적으로 ROC 곡선 자체는 FPR과 TPR의 변화값을 보는 데 이용하며 분류의 성능 지표로 사용되는 것을 ROC 곡선 면적에 기반한 AUC 값으로 결정한다. AUC가 1에 가까울수록 좋은 수치이다. AUC 수치가 커지려면 FPR이 작은 상태에서 얼마나 더 큰 TPR을 얻을 수 있느냐가 관건이다. | . . &#54588;&#47560; &#51064;&#46356;&#50616; &#45817;&#45544;&#48337; &#50696;&#52769; . import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression diabetes_data = pd.read_csv(&#39;diabetes.csv&#39;) print(diabetes_data[&#39;Outcome&#39;].value_counts()) diabetes_data.head(3) . 0 500 1 268 Name: Outcome, dtype: int64 . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . Negative : 500, Positive : 268 | . diabetes_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Pregnancies 768 non-null int64 1 Glucose 768 non-null int64 2 BloodPressure 768 non-null int64 3 SkinThickness 768 non-null int64 4 Insulin 768 non-null int64 5 BMI 768 non-null float64 6 DiabetesPedigreeFunction 768 non-null float64 7 Age 768 non-null int64 8 Outcome 768 non-null int64 dtypes: float64(2), int64(7) memory usage: 54.1 KB . null값 없고, feature타입은 모두 숫자형 | 따라서 별도의 feature incoding은 불필요 | . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#47484; &#51060;&#50857;&#54644; &#50696;&#52769; &#47784;&#45944;&#51012; &#49373;&#49457;&#54644;&#48372;&#51088; . # feature 데이터 세트 X, 레이블 데이터 세트 y를 추출 # 맨 끝이 Outcome 칼럼으로서 레이블 값임. 따라서 그 칼럼의 위치인 -1을 이용해 빼줌 X=diabetes_data.iloc[:,:-1] # feature 데이터 세트 y=diabetes_data.iloc[:,-1] # 레이블 데이터 세트 X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state = 156, stratify=y) # 로지스틱 회귀로 학습, 예측 및 평가 lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) pred = lr_clf.predict(X_test) pred_proba=lr_clf.predict_proba(X_test)[:,1] get_clf_eval(y_test, pred, pred_proba) . 재현율 : 59.26%, 전체 데이터의 65%가Negative이므로 정확도보다는 재현율 성능에 조금 더 초점을 맞추기 위해 임곗값별 정밀도와 재현율 값의 변화를 확인해보자 . pred_proba_c1 = lr_clf.predict_proba(X_test)[:,1] precision_recall_curve_plot(y_test,pred_proba_c1) . 임계값 0.42 정도에서 정밀도와 재현율이 어느 정도 균형을 맞춤. 그렇지만 두 지표 모두 0.7이 안 되는 수치로서 아직도 낮은 지표 값임. 임계값을 임의적으로 조정하기 전에 데이터값을 다시 보자 . diabetes_data.describe() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . count 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | . mean 3.845052 | 120.894531 | 69.105469 | 20.536458 | 79.799479 | 31.992578 | 0.471876 | 33.240885 | 0.348958 | . std 3.369578 | 31.972618 | 19.355807 | 15.952218 | 115.244002 | 7.884160 | 0.331329 | 11.760232 | 0.476951 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.078000 | 21.000000 | 0.000000 | . 25% 1.000000 | 99.000000 | 62.000000 | 0.000000 | 0.000000 | 27.300000 | 0.243750 | 24.000000 | 0.000000 | . 50% 3.000000 | 117.000000 | 72.000000 | 23.000000 | 30.500000 | 32.000000 | 0.372500 | 29.000000 | 0.000000 | . 75% 6.000000 | 140.250000 | 80.000000 | 32.000000 | 127.250000 | 36.600000 | 0.626250 | 41.000000 | 1.000000 | . max 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 | 67.100000 | 2.420000 | 81.000000 | 1.000000 | . min행이 0이 굉장히 많음. 예를 들어 Glucose는 포도당 수치인데 0인 것은 말이 안 됨. . plt.hist(diabetes_data[&#39;Glucose&#39;],bins=10) . (array([ 5., 0., 4., 32., 156., 211., 163., 95., 56., 46.]), array([ 0. , 19.9, 39.8, 59.7, 79.6, 99.5, 119.4, 139.3, 159.2, 179.1, 199. ]), &lt;BarContainer object of 10 artists&gt;) . 0 값이 일정 수준 존재함을 알 수 있다. . min() 값이 0으로 돼 있는 feature에 대해 0 값의 건수 및 전체 데이터 건수 대비 및 몇 퍼센트의 비율로 존재하는지 확인해보자. . zero_features=[&#39;Glucose&#39;,&#39;BloodPressure&#39;,&#39;SkinThickness&#39;,&#39;Insulin&#39;,&#39;BMI&#39;] # 전체 데이터 건수 total_count=diabetes_data[&#39;Glucose&#39;].count() #feature별로 반복하면서 데이터 값이 0인 데이터 건수를 추출하고, 퍼센트 계산해보자 for feature in zero_features: zero_count = diabetes_data[diabetes_data[feature]==0][feature].count() print(&#39;{} 0건수는 {}, 퍼센트는 {:.2f}%&#39;.format(feature, zero_count, 100*zero_count/total_count)) . Glucose 0건수는 5, 퍼센트는 0.65% BloodPressure 0건수는 35, 퍼센트는 4.56% SkinThickness 0건수는 227, 퍼센트는 29.56% Insulin 0건수는 374, 퍼센트는 48.70% BMI 0건수는 11, 퍼센트는 1.43% . SkimThickness와 Insulin의 0 값은 각각 전체의 약 30,50%로 대단히 많다. 전체 데이터 건수가 약 800개 이므로 많지 않음, 따라서 0 데이터를 일괄적으로 삭제할 경우에는 학습을 효과적으로 수행하기 어렵다. 따라서 위 feature의 0값을 평균으로 대체해보자 | . mean_zero_features = diabetes_data[zero_features].mean() diabetes_data[zero_features]=diabetes_data[zero_features].replace(0,mean_zero_features) . 로지스틱 회귀의 경우 일반적으로 숫자 데이터에 스케일링을 적용하는 것이 좋음. 따라서 0값을 평균값으로 대체한 데이터 세트에 feature 스케일링을 적용해 변환해보자 . X=diabetes_data.iloc[:,:-1] y=diabetes_data.iloc[:,-1] # StandardScaler 클래스를 이용해 feature 데이터 세트에 일괄적으로 스케일링 적용 scaler = StandardScaler() X_scaled = scaler.fit_transform(X) X_train,X_test,y_train,y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=156,stratify=y) # 로지스틱 회귀로 학습, 예측 및 평가 수행 lr_clf=LogisticRegression() lr_clf.fit(X_train,y_train) pred = lr_clf.predict(X_test) pred_proba=lr_clf.predict_proba(X_test)[:,1] get_clf_eval(y_test,pred,pred_proba) . 데이터 변환과 스케일링을 통해 성능 수치가 일정 수준 개선됐지만 재현율 수치는 아직 개선이 더 필요, 분류 결정 임계값을 0.3에서 0.5까지 0.03씩 변화시키면서 재현율과 다른 평가 지표의 값 변화를 출력해보자 . thresholds = [0.3,0.33,0.36,0.39,0.42,0.45,0.5] pred_proba = lr_clf.predict_proba(X_test) get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1),thresholds) . 값을 반환했을 때 정확도와 정밀도를 어느 정도 희생하고 재현율을 높이는 데 가장 좋은 임계값은 0.33으로 재현율 값이 0.7963이다. 하지만 정밀도가 매우 저조해져서 극단적 선택임. 임계값 0.48이 전체적인 성능 평가 지표를 유지하면서 재현율을 약간 향상시키는 좋은 임계값으로 보임. . 181p 코드 참고 . &#51648;&#44552;&#44620;&#51648; &#48516;&#47448;&#50640; &#49324;&#50857;&#46104;&#45716; &#51221;&#54869;&#46020;, &#50724;&#52264; &#54665;&#47148;, &#51221;&#48128;&#46020;, &#51116;&#54788;&#50984;, F1 &#49828;&#53076;&#50612;, ROC-AUC&#50752; &#44057;&#51008; &#49457;&#45733; &#54217;&#44032; &#51648;&#54364;&#44032; &#51080;&#50632;&#51020;. .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/04/intro.html",
            "relUrl": "/2022/01/04/intro.html",
            "date": " • Jan 4, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "2022/01/03/MON",
            "content": "평가 process에 대해 알아보자 . - &#51221;&#54869;&#46020; ? = &#50696;&#52769; &#44208;&#44284;&#44032; &#46041;&#51068;&#54620; &#45936;&#51060;&#53552; &#44148;&#49688; / &#51204;&#52404; &#50696;&#52769; &#45936;&#51060;&#53552; &#44148;&#49688; . - 이진 부류의 경우 데이터 구성에 따라 ML 모델의 성능을 왜곡할 수 있기 때문에 정확도 수치 하나만 가지고 성능을 평가하는 건 위험함 - 그 예를 살펴보자 . from sklearn.base import BaseEstimator import numpy as np class MyDummyClassifier(BaseEstimator): # fit 메서드는 아무것도 학습하지 않음 def fit(self, X, y=None): pass # predict() 메서드는 단순히 Sex feature 1이면 0 그렇지 않으면 1로 예측함 def predict(self, X): pred = np.zeros((X.shape[0],1)) for i in range(X.shape[0]) : if X[&#39;Sex&#39;].iloc[i]==1 : pred[i]=0 else : pred[i]=1 return pred . import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # 원본 데이터를 재로딩, 데이터 가공, 학습 데이터/테스트 데이터 분할 titanic_df= pd.read_csv(&#39;C:/Users/ehfus/Downloads/titanic/train.csv&#39;) y_titanic_df=titanic_df[&#39;Survived&#39;] X_titanic_df=titanic_df.drop(&#39;Survived&#39;,axis=1) X_titanic_df=transform_features(X_titanic_df) # transform_features 함수 정의 안 했기 때문에 에러 발생(140p) x_train,X_test,y_train,y_test = train_test_split(X_titanic_df, y_titanic_df,test_size=.2,random_state=0) # 위에서 생성한 Dummy Classifier를 이용해 학습/예측/평가 수행 myclf=MydummyClassifier() myclf.fit(X_train,y_train) mypredictions=myclf.predict(X_train,y_train) mypredictions = myclf.predict(X_test) print(&#39;Dummy Classifier의 정확도는: {0:.4f}&#39;.format(accuracy_score(y_test,mypredictions))) . Dummy Classifier의 정확도는 0.7877정도 나온다 . 즉 이렇게 단순한 알고리즘으로 예측을 하더라도 데이터의 구성에 따라 정확도의 결과는 약 78%정도로 높은 수치가 나올 수 있음. 따라서 정확도를 지표로 사용할 때는 매우 신중해야 함. . 특히 불균형한 레이블 값 분포에서 ML 모델의 성능을 판단할 경우 적합한 평가 지표가 아님, 예를 들어보자 . from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.base import BaseEstimator from sklearn.metrics import accuracy_score import numpy as np import pandas as pd class MyFakeClassifier(BaseEstimator): def fit(self, X, y): pass # 입력값으로 들어오는 X 데이터 세트의 크기만큼 모두 0값으로 만들어서 반환 def predict(self, X): return np.zeros((len(X),1),dtype=bool) # 사이킷런의 내장 데이터 세트인 load_digits()를 이용해 MNIST 데이터 로딩 digits = load_digits() # digits 번호가 7번이면 True이고 이를 astype(int)로 1로 변환, 7번이 아니면 Fasle이고 0으로 변환. y=(digits.target==7).astype(int) X_train,X_test,y_train,y_test= train_test_split(digits.data,y,random_state=11) # 불균형한 레이블 데이터 분포도 확인 print(&#39;레이블 테스트 세트 크기: &#39;,y_test.shape) print(&#39;테스트 세트 레이블 0과 1의 분포도&#39;) print(pd.Series(y_test).value_counts()) # Dummy Classifier로 학습/예측/정확도 평가 fakeclf=MyFakeClassifier() fakeclf.fit(X_train,y_train) fakepred=fakeclf.predict(X_test) print(&#39;모든 예측을 0으로 하여도 정확도는:{:.3f}&#39;.format(accuracy_score(y_test,fakepred))) . 레이블 테스트 세트 크기: (450,) 테스트 세트 레이블 0과 1의 분포도 0 405 1 45 dtype: int64 모든 예측을 0으로 하여도 정확도는:0.900 . 이처럼 정확도 평가 지표는 불균형한 레이블 데이터 세트에서는 성능 수치로 사용 되어서는 안 된다. 여러가지 분류 지표와 함께 이용하자 | . - &#50724;&#52264;&#54665;&#47148; &#46608;&#45716; &#54844;&#46041;&#54665;&#47148; ? . - 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리고 있는지도 함께 보여주는 지표 - TN,FP,FN,TP로 나뉘며, 앞문자는 예측값과 실제값이 &#39;같은가/ 틀린가&#39;를 의미, 뒤 문자는 예측 결과 값이 부정(0)/긍정(1)을 의미 - 이 값을 조합해 정확도, 정밀도, 재현율 값을 알 수 있음 . from sklearn.metrics import confusion_matrix confusion_matrix(y_test, fakepred) . array([[405, 0], [ 45, 0]], dtype=int64) . TN은 405개, FN은 45개이다. | . 정확도 = 예측 결과와 실제 값이 동일한 건수/전체 데이터 수 = (TN + TP) / (TN + FP + FN + TP) | 정밀도 = TP / (FP + TP) | 재현율 = TP = (FN + TP) | . (158p 참고) . 분류하려는 업무의 특성상 정밀도 또는 재현율이 특별히 강조되어야 할 경우 분류의 결정 임계값(Threshold)을 조정해 정밀도 또는 재현율의 수치를 높일 수 있다. . 개별 데이터 별로 예측 확률을 반환하는 메서드 = predict_proba() predict_proba() 메서드는 학습이 완료된 사이킷런 Classifier 객체에서 호출이 가능하며 테스트 feature 데이터 세트를 파라미터로 입력해주면 테스트 feature 레코드의 개별 클래스 예측 확률을 반환함. predict() 메서드와 유사하지만 단지 반환 결과가 예측 결과 클래스값이 아닌 예측 확률 결과이다. . (160p 참고) . threshold 값을 조정해보자 | . from sklearn.preprocessing import Binarizer X=[[1,-1,2], [2,0,0], [0,1.1,1.2]] # X의 개별 원소들이 threshold 값보다 같거나 작으면 0을 크면 1을 반환 binarizer = Binarizer(threshold=1.1) print(binarizer.fit_transform(X)) . [[0. 0. 1.] [1. 0. 0.] [0. 0. 1.]] . 이제 이 Binarizer를 이용해 사이킷런 predict()의 의사(pseudo)코드를 만들어보자 . from sklearn.preprocessing import Binarizer # Binarizer의 threshold 설정값, 즉 분류 결정 임계값임. custom_threshold=0.5 # predict_proba() 반환값의 두 번째 칼럼, 즉 Positive 클래스 칼럼 하나만 추출해 Binirizer를 적용 pred_proba_1=pred_proba[:,1].reshape(-1,1) binarizer = Binarizer(threshold=cistom_threshold).fit(pred_proba_1) custom_predict = binarizer.transform(pred_proba_1) get_clf_eval(y_test, custom_predict) . 이때 threshold 설정값을 0.4로 설정, 즉 분류 결정 임계값을 낮추면 True값이 많아질 것이고 재현율 값이 올라가고 정밀도는 떨어질 것이다. . 임계값을 0.4에서부터 0.6까지 0.05씩 증가시키며 평가 지표를 조사해보자, 이를 위해 get_eval_by_threshold() 함수를 만듦 . # 테스트를 수행할 모든 임계값을 리스트 객체로 저장 thresholds = [0.4,0.45,0.5,0.55,0.6] def get_eval_by_threshold(y_test,pred_proba_c1,thresholds): # thresholds list 객체 내의 값을 차례로 iteration하면서 Evaluation 수행. for custom_threshold in thresholds: binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) custom_threshold = binarizer.transform(pres_proba_c1) print(&#39;임계값:&#39;,custom_threshold) get_clf_eval(y_test,custom_predict) get_eval_by_threshold(y_test,pred_proba[:,1].reshapep(-1,1), thresholds) . 임계값의 변경은 업무 환경에 맞게 두 개의 수치를 상호 보완할 수 있는 수준에서 적용돼야 한다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/03/intro.html",
            "relUrl": "/2022/01/03/intro.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "2022/01/02/SUN",
            "content": "&#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 데이터 인코딩 - 레이블 인코딩 . | . from sklearn.preprocessing import LabelEncoder items=[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;] # LabelEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행 encoder = LabelEncoder() # 객체 생성 encoder.fit(items) labels=encoder.transform(items) print(&#39;인코딩 변환값: &#39;, labels) print(&#39;-&#39;) print(&#39;인코딩 클래스: &#39;, encoder.classes_) print(&#39;차례대로 0부터 5까지 부여됨&#39;) print(&#39;-&#39;) print(&#39;디코딩 원본값: &#39;,encoder.inverse_transform([0,4,5,5,1,1,2,0])) print(&#39;이렇게 원하는 인코딩 값의 리스트를 통해 디코딩 할 수 있다&#39;) . 인코딩 변환값: [0 1 4 5 3 3 2 2] - 인코딩 클래스: [&#39;TV&#39; &#39;냉장고&#39; &#39;믹서&#39; &#39;선풍기&#39; &#39;전자레인지&#39; &#39;컴퓨터&#39;] 차례대로 0부터 5까지 부여됨 - 디코딩 원본값: [&#39;TV&#39; &#39;전자레인지&#39; &#39;컴퓨터&#39; &#39;컴퓨터&#39; &#39;냉장고&#39; &#39;냉장고&#39; &#39;믹서&#39; &#39;TV&#39;] 이렇게 원하는 인코딩 값의 리스트를 통해 디코딩 할 수 있다 . ***주의*** . 레이블 인코딩이 1,2일때 특정 ML알고리즘에서 가중치가 더 부여되거나 더 중요하게 인식할 가능성이 발생함. 하지만 단순 인코딩 숫자이기에 이러한 현상은 피해야함. 따라서 이러한 레이블 인코딩은 선형 회귀와 같은 ML 알고리즘에는 적용 X, 트리 계열의 알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 레이블 인코딩도 별 문제 X, 원-핫 인코딩은 레이블 인코딩의 이러한 문제점을 해결하기 위한 인코딩 방식임 . . 데이터 인코딩 - 원-핫 인코딩 . | . from sklearn.preprocessing import OneHotEncoder import numpy as np items=[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;] # 먼저 숫자 값으로 변환하기 위해 LabelEncoder로 변환해야함 encoder=LabelEncoder() # 객체 생성 encoder.fit(items) labels=encoder.transform(items) # 꼭 2차원 데이터로 변경해야 함 labels=labels.reshape(-1,1) # 이제 원-핫 인코딩 oh_encoder= OneHotEncoder() # 객체 생성 oh_encoder.fit(labels) oh_labels=oh_encoder.transform(labels) print(&#39;원-핫 인코딩 데이터&#39;) print(oh_labels.toarray()) # array 형태로 print(&#39;원-핫 인코딩 데이터 차원&#39;) print(oh_labels.shape) # 8행 6열의 행렬 형태 . 원-핫 인코딩 데이터 [[1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0.] [0. 0. 0. 1. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0.]] 원-핫 인코딩 데이터 차원 (8, 6) . 이러한 과정을 pandas를 통해 한 번에? = get_dummies . import pandas as pd df=pd.DataFrame({&#39;item&#39;:[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;]}) pd.get_dummies(df) . item_TV item_냉장고 item_믹서 item_선풍기 item_전자레인지 item_컴퓨터 . 0 1 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 1 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 1 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 1 | . 4 0 | 0 | 0 | 1 | 0 | 0 | . 5 0 | 0 | 0 | 1 | 0 | 0 | . 6 0 | 0 | 1 | 0 | 0 | 0 | . 7 0 | 0 | 1 | 0 | 0 | 0 | . get_dummies()를 이용하면 숫자형 값으로 변환 없이도 바로 변환이 가능함 | . . feature &#49828;&#52992;&#51068;&#47553;&#44284; &#51221;&#44508;&#54868; . feature scaling에는 표준화와 정규화가 있으며, 표준화는 feature각각이 평균0, 분산1인 가우시안 정규 분포를 가진 값으로 변환하는 것을 의미 . $x_i _new$ = $ frac{x_i-mean(x)}{stdev(x)}$ 이며 $stdev(x)$는 표준편차를 의미함. . 일반적으로 정규화는 서로 다른 feature의 크기를 통일하기 위해 크기는 변환해주는 개념. 0~1값으로 변환한다. 즉 개별 데이터의 크기를 모두 똑같은 단위로 변경하는 것. . $x_i _new$ = $ frac{x_i-min(x)}{max(x)-min(x)}$ . 주의 . 사이킷런의 전처리에서 제공하는 Normalizer 모듈과 일반적인 정규화는 약간의 차이가 있음. 사이킷런의 Normalizer 모듈은 선형 대수에서의 정규화 개념이 적용 됐으며, 개별 벡터의 크기를 맞추기 위해 변환하는 것을 의미함. . $x_i _new$ = $ frac{x_i}{ sqrt(x_i^2+y_i^2+z_i^2)}$ . 이를 벡터 정규화로 지칭하자 . . StandardScaler :표준화 . from sklearn.datasets import load_iris iris=load_iris() iris_data=iris.data iris_df=pd.DataFrame(data=iris_data,columns=iris.feature_names) print(&#39;feature들의 평균값&#39;) print(iris_df.mean()) print(&#39; nfeature들의 분산값&#39;) print(iris_df.var()) . feature들의 평균값 sepal length (cm) 5.843333 sepal width (cm) 3.057333 petal length (cm) 3.758000 petal width (cm) 1.199333 dtype: float64 feature들의 분산값 sepal length (cm) 0.685694 sepal width (cm) 0.189979 petal length (cm) 3.116278 petal width (cm) 0.581006 dtype: float64 . from sklearn.preprocessing import StandardScaler scaler=StandardScaler() # 객체 생성 scaler.fit(iris_df) iris_scaled=scaler.transform(iris_df) # transform()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DataFrame으로 변환 iris_df_scaled = pd.DataFrame(data=iris_scaled,columns=iris.feature_names) print(&#39;feature들의 평균값&#39;) print(iris_df_scaled.mean()) print(&#39; nfeature들의 분산값&#39;) print(iris_df_scaled.var()) . feature들의 평균값 sepal length (cm) -1.690315e-15 sepal width (cm) -1.842970e-15 petal length (cm) -1.698641e-15 petal width (cm) -1.409243e-15 dtype: float64 feature들의 분산값 sepal length (cm) 1.006711 sepal width (cm) 1.006711 petal length (cm) 1.006711 petal width (cm) 1.006711 dtype: float64 . 두 셀을 비교해보면 위 셀 $ to$ 아래 셀, 모든 칼럼 값의 평균이 0에 아주 가까운 값으로, 그리고 분산은 1에 아주 가까운 값으로 변환됐음을 알 수 있다. | . MinMaxScaler :정규화 . 데이터 값을 0과 1사이의 범위 값으로 변환한다. (음수 값이 있으면 -1에서 1값으로 변환) | 데이터의 분포가 가우시안 분포가 아닐 경우에 적용 가능 | . from sklearn.preprocessing import MinMaxScaler scaler=MinMaxScaler() scaler.fit(iris_df) iris_scaled=scaler.transform(iris_df) # transform()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DF으로 변환 iris_df_scaled = pd.DataFrame(data=iris_scaled,columns=iris.feature_names) print(&#39;feature들의 최솟값&#39;) print(iris_df_scaled.min()) print(&#39; nfeature들의 최댓값&#39;) print(iris_df_scaled.max()) . feature들의 최솟값 sepal length (cm) 0.0 sepal width (cm) 0.0 petal length (cm) 0.0 petal width (cm) 0.0 dtype: float64 feature들의 최댓값 sepal length (cm) 1.0 sepal width (cm) 1.0 petal length (cm) 1.0 petal width (cm) 1.0 dtype: float64 . 모든 feature가 0에서 1사이의 값으로 변환되는 스케일링이 적용됐음을 알 수 있다. | . . 유의점 128p 참고 . 학습 데이터로 fit()이 적용된 스케일링 기준 정보를 그대로 테스트 데이터에 적용해야 하며, 그렇지 않으면 학습 데이터와 테스트 데이터의 스케일링 기준 정보가 서로 달라지기 때문에 올바른 예측 결과를 도출하지 못할 수 있다. . test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform()만으로 변환해야한다. . 즉 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리하던가 이것이 여의치 않다면 테스트 데이터 변환 시에는 fit()이나 fit_transform()을 적용하지 않고 학습 데이터로 이미 fit()된 scaler 객체를 이용해 transform()으로 변환 | . . 131p 사이킷런으로 수행하는 타이타닉 생존자 예측, 꼭 한 번 실습해보기 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/02/intro.html",
            "relUrl": "/2022/01/02/intro.html",
            "date": " • Jan 2, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "2022/01/01/SAT(HappyNewYear)",
            "content": "datail-review 해보자 . feature_names = 높이,가로 길이 이런 것들, data = 각 featuredml 값들, target = 0,1,2...예를 들면 붓꽃의 이름을 대용한 것, target_names = 각 target이 가리키는 이름이 무엇인지? | . . model_selection 모듈은 학습 데이터와 테스트 데이터 세트를 분리하거나 교차 검증 분할 및 평가, 그리고 Estimator의 하이퍼 파라미터를 튜닝하기 위한 다양한 함수와 클래스를 제공, 전체 데이터를 학습 데이터와 테스트 데이터 세트로 분리해주는 train_test_split()부터 살펴보자 . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score iris=load_iris() # 붓꽃 데이터 세트 로딩 dt_clf=DecisionTreeClassifier() train_data=iris.data # 데이터 세트에서 feature만으로 구성된 데이터가 ndarray train_label=iris.target # 데이터 세트에서 label 데이터 dt_clf.fit(train_data, train_label) # 학습 수행중 pred=dt_clf.predict(train_data) # 예측 수행중 // 그런데 학습때 사용했던 train_data를 사용했음 -&gt; 예측도 1 나올 것 print(&#39;예측도: &#39;,accuracy_score(train_label,pred)) . 예측도: 1.0 . 정확도가 100% 나왔음 $ to$ 이미 학습한 학습 데이터 세트를 기반으로 예측했기 때문. 답을 알고 있는데 같은 문제를 낸 것이나 마찬가지 | 따라서 예측을 수행하는 데이터 세트는 학습을 수행한 학습용 데이터 세트가 아닌 전용의 테스트 데이터 세트여야 함. | . from sklearn.model_selection import train_test_split . dt_clf=DecisionTreeClassifier() iris=load_iris() # train_test_split()의 반환값은 튜플 형태이다. 순차적으로 네가지 요소들을 반환한다 X_train,X_test,y_train,y_test=train_test_split(iris.data, iris.target,test_size=0.3,random_state=121) dt_clf.fit(X_train,y_train) pred = dt_clf.predict(X_test) print(&#39;예측 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 예측 정확도: 0.9556 . . 지금까지의 방법은 모델이 학습 데이터에만 과도하게 최적화되어, 실제 예측을 다른 데이터로 수행할 경우에는 예측 성능이 과도하게 떨어지는 과적합이 발생할 수 있다. 즉 해당 테스트 데이터에만 과적합되는 학습 모델이 만들어져 다른 테스트용 데이터가들어올 경우에는 성능이 저하된다. $ to$ 개선하기 위해 교차검증을 이용해 다양한 학습과 평가를 수행해야 한다. . 교차검증? . : 본고사 치르기 전, 여러 모의고사를 치르는 것. 즉 본고사가 테스트 데이터 세트에 대해 평가하는 것이라면 모의고사는 교차 검증에서 많은 학습과 검증 세트에서 알고리즘 학습과 평가를 수행하는 것. . : 학습 데이터 세트를 검증 데이터 세트와 학습 데이터 세트로 분할하여 수행한 뒤, 모든 학습/검증 과정이 완료된 후 최종적으로 성능을 평가하기 위해 테스트 데이터 세트를 마련함. . K fold 교차 검증? . : K개의 데이터 폴드 세트를 만들어서 K번만큼 각 폴드 세트에 학습과 검증, 평가를 반복적으로 수행 / 개괄적 과정은 교재 104 참고 . 실습해보자 | . import numpy as np . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import KFold # 위에서는 trian_test_split을 import했었음 iris=load_iris() # 붓꽃 데이터 세트 로딩 features=iris.data label=iris.target dt_clf=DecisionTreeClassifier(random_state=156) kfold=KFold(n_splits=5) # KFold 객체 생성 cv_accuracy=[] # fold set별 정확도를 담을 리스트 객체 생성 print(&#39;붓꽃 데이터 세트 크기:&#39;,features.shape[0]) . 붓꽃 데이터 세트 크기: 150 . . kfold=KFold(n_splits=5) . 로 KFold객체를 생성했으니 객체의 split()을 호출해 전체 붓꽃 데이터를 5개의 fold 데이터 세트로 분리하자. 붓꽃 데이터 세트 크기가 150개니 120개는 학습용, 30개는 검증 테스트 데이터 세트이다. . n_iter=0 for train_index,test_index in kfold.split(features): # kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출 X_train, X_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] # 학습 및 예측 dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) n_iter+=1 # 반복 시마다 정확도 측정 accuracy = np.round(accuracy_score(y_test,pred),4) train_size = X_train.shape[0] test_size = X_test.shape[0] print(&#39; n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기 :{2}, 검증 데이터 크기 :{3}&#39;.format(n_iter,accuracy,train_size,test_size)) print(&#39;#{0} 검증 세트 인덱스:{1}&#39;.format(n_iter, test_index)) cv_accuracy.append(accuracy) # 개별 iteration별 정확도를 합하여 평균 정확도 계산 print(&#39; n *Conclusion* 평균 검증 정확도:&#39;, np.mean(cv_accuracy)) . #1 교차 검증 정확도 :1.0, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #1 검증 세트 인덱스:[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29] #2 교차 검증 정확도 :0.9667, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #2 검증 세트 인덱스:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59] #3 교차 검증 정확도 :0.8667, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #3 검증 세트 인덱스:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89] #4 교차 검증 정확도 :0.9333, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #4 검증 세트 인덱스:[ 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119] #5 교차 검증 정확도 :0.7333, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #5 검증 세트 인덱스:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] *Conclusion* 평균 검증 정확도: 0.9 . . 교차 검증시마다 검증 세트의 인덱스가 달라짐을 알 수 있다. . | 검증세트 인덱스를 살펴보면 104p에서 설명한 그림의 설명과 유사함 . | . . Stratified K 폴드 . : 불균형한 분포도를가진 레이블(결정 클래스) 데이터 집합을 위한 K 폴드 방식이다. 불균형한 분포도를 가진 레이블 데이터 집합은 특정 레이블 값이 특이하게 많거나 또는 적어서 분포가 한쪽으로 치우치는 것을 말함 . 가령 대출 사기 데이터를 예측한다고 가정해보자, 이 데이터 세트는 1억건이고 수십개의 feature와 대출 사기 여부를 뜻하는 label(정상 대출0, 대출사기 : 1)로 구성돼 있다. K폴드로 랜덤하게 학습 및 테스트 세트의 인덱스를 고르더라도 레이블 값인 0과1의 비율을 제대로 반영하지 못하게 됨. 따라서 원본 데이터와 유사한 대출 사기 레이블 값의 분포를 학습/테스트 세트에도 유지하는 게 매우 중요 . Stratified K 폴드는 이처럼 K폴드가 레이블 데이터 집합이 원본 데이터 집합의 레이블 분포를 학습 및 테스트 세트에 제대로 분배하지 못하는 경우의 문제를 해결해줌 | . 붓꽃 데이터 세트를 DataFrame으로 생성하고 레이블 값의 분포도를 먼저 확인해보자 . import pandas as pd iris=load_iris() iris_df=pd.DataFrame(data=iris.data,columns=iris.feature_names) iris_df[&#39;label&#39;]=iris.target print(iris_df[&#39;label&#39;].value_counts(),&#39; n&#39;) . 0 50 1 50 2 50 Name: label, dtype: int64 . label값은 모두 50개로 분배되어 있음 | . kfold=KFold(n_splits=3) n_iter=0 for train_index, test_index in kfold.split(iris_df): n_iter+=1 label_train = iris_df[&#39;label&#39;].iloc[train_index] label_test=iris_df[&#39;label&#39;].iloc[test_index] print(&#39;## 교차 검증: {}&#39;.format(n_iter)) print(&#39;학습 레이블 데이터 분포: n&#39;, label_train.value_counts()) print(&#39;검증 레이블 데이터 분포: n&#39;, label_test.value_counts()) print(&#39;&#39;) . ## 교차 검증: 1 학습 레이블 데이터 분포: 1 50 2 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 50 Name: label, dtype: int64 ## 교차 검증: 2 학습 레이블 데이터 분포: 0 50 2 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 1 50 Name: label, dtype: int64 ## 교차 검증: 3 학습 레이블 데이터 분포: 0 50 1 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 2 50 Name: label, dtype: int64 . 교차 검증 시마다 3개의 폴드 세트로 만들어지는 학습 레이블과 검증 레이블이 완전히 다른 값으로 추출되었다. 예를 들어 첫번째 교차 검증에서는 학습 레이블의 1,2값이 각각 50개가 추출되었고 검증 레이블의 0값이 50개 추출되었음, 즉 학습레이블은 1,2 밖에 없으므로 0의 경우는 전혀 학습하지 못함. 반대로 검증 레이블은 0밖에 없으므로 학습 모델은 절대 0을 예측하지 못함. 이런 유형으로 교차 검증 데이터 세트를 분할하면 검증 예측 정확도는 0이 될 수밖에 없다. | . StratifiedKFold는 이렇게 KFold로 분할된 레이블 데이터 세트가 전체 레이블 값의 분포도를 반영하지 못하는 문제를 해결함. | . . 실습해보자 . from sklearn.model_selection import StratifiedKFold skf=StratifiedKFold(n_splits=3) n_iter=0 # split 메소드에 인자로 feature데이터 세트뿐만 아니라 레이블 데이터 세트도 반드시 넣어줘야함 for train_index,test_index in skf.split(iris_df,iris_df[&#39;label&#39;]): n_iter+=1 label_train=iris_df[&#39;label&#39;].iloc[train_index] label_test=iris_df[&#39;label&#39;].iloc[test_index] print(&#39;## 교차검증: {}&#39;.format(n_iter)) print(&#39;학습 레이블 데이터 분포: n&#39;, label_train.value_counts()) print(&#39;검증 레이블 데이터 분포: n&#39;, label_test.value_counts()) print(&#39;--&#39;) . ## 교차검증: 1 학습 레이블 데이터 분포: 2 34 0 33 1 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 17 1 17 2 16 Name: label, dtype: int64 -- ## 교차검증: 2 학습 레이블 데이터 분포: 1 34 0 33 2 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 17 2 17 1 16 Name: label, dtype: int64 -- ## 교차검증: 3 학습 레이블 데이터 분포: 0 34 1 33 2 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 1 17 2 17 0 16 Name: label, dtype: int64 -- . 학습 레이블과 검증 레이블 데이터 값의 분포도가 동일하게 할당됐음을 알 수 있다. 이렇게 분할이 되어야 레이블 값 0,1,2를 모두 학습할 수 있고 이에 기반해 검증을 수행할 수 있다. | . 이제 StratifiedKFold를 이용해 붓꽃 데이터를 교차 검증해보자 | . df_clf=DecisionTreeClassifier(random_state=156) skfold=StratifiedKFold(n_splits=3) n_iter=3 cv_accuracy=[] # StratifiedKFol의 split() 호출시 반드시 레이블 데이터 세트도 추가 입력 필요 for train_index, test_ondex in skfold.split(features, label): # split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출 X_train,X_test=features[train_index],features[test_index] y_train,y_test=label[train_index], label[test_index] # 학습 및 예측 df_clf.fit(X_train,y_train) pred=dt_clf.predict(X_test) # 반복시마다 정확도 측정 n_iter+=1 accuracy=np.around(accuracy_score(y_test,pred),4) train_size=X_train.shape[0] test_size = X_test.shape[0] print(&#39; n#{} 교차 검증 정확도 : {}, 학습 데이터 크기 : {}, 검증 데이터 크기 : {}&#39;.format(n_iter,accuracy,train_size,test_size)) print(&#39;#{} 검증 세트 인덱스: {}&#39;.format(n_iter, test_index)) cv_accuracy.append(accuracy) # 교차 검증별 정확도 및 평균 정확도 계산 print(&#39; n## 교차 검증별 정확도:&#39;, np.around(cv_accuracy,4)) print(&#39;## 평균 검증 정확도:&#39;,np.mean(cv_accuracy)) . #4 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #4 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92] ## 평균 검증 정확도: 0.92 #5 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #5 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92 0.92] ## 평균 검증 정확도: 0.92 #6 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #6 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92 0.92 0.92] ## 평균 검증 정확도: 0.92 . . &#44368;&#52264; &#44160;&#51613;&#51012; &#48372;&#45796; &#44036;&#54200;&#54616;&#44172; - cross_val_score() . from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score,cross_validate from sklearn.datasets import load_iris iris_data=load_iris() dt_clf = DecisionTreeClassifier(random_state=156) data= iris_data.data label=iris_data.target # 성능 지표는 정확도 (accuracy), 교차 검증 세트는 3개 scores = cross_val_score(dt_clf, data, label, scoring=&#39;accuracy&#39;, cv=3) print(&#39;교차 검증별 정확도: &#39;,np.round(scores,4)) print(&#39;평균 검증 정확도: &#39;,np.round(np.mean(scores),4)) . 교차 검증별 정확도: [0.98 0.94 0.98] 평균 검증 정확도: 0.9667 . cv로 지정된 횟수만큼 scoring 파라미터로 지정된 평가지표로 평가 결과값을 배열로 반환 | . . GridSearchCV - &#44368;&#52264; &#44160;&#51613;&#44284; &#52572;&#51201; &#54616;&#51060;&#54140; &#54028;&#46972;&#48120;&#53552; &#53916;&#45789;&#51012; &#46041;&#49884;&#50640; . 하이퍼 파라미터? 머신러닝 알고리즘을 구성하는 주요 구성 요소이며, 이 값을 조정해 알고리즘의 예측 성능을 개선할 수 있음 | . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import GridSearchCV # 데이터를 로딩하고 학습 데이터와 테스트 데이터 분리 iris_data = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data,iris_data.target, test_size=0.2, random_state=121) dtree= DecisionTreeClassifier() # 파라미터를 딕셔너리 형태로 설정 parameters = {&#39;max_depth&#39; : [1,2,3], &#39;min_samples_split&#39; : [2,3]} import pandas as pd # param_grid의 하이퍼 파라미터를 3개의 train, test set fold로 나누어 테스트 수행 설정 # rifit=True가 default이며, 이때 가장 젛은 파라미터 설정으로 재학습시킴 grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True) # 붓꽃 학습 데이터로 param_grid의 하이퍼 파라미터를 순차적으로 학습/평가 grid_dtree.fit(X_train,y_train) #GridSearchCV 결과를 추출해 DataFrame으로 변환 scores_df = pd.DataFrame(grid_dtree.cv_results_) scores_df[[&#39;params&#39;,&#39;mean_test_score&#39;,&#39;rank_test_score&#39;,&#39;split0_test_score&#39;,&#39;split1_test_score&#39;,&#39;split2_test_score&#39;]] . params mean_test_score rank_test_score split0_test_score split1_test_score split2_test_score . 0 {&#39;max_depth&#39;: 1, &#39;min_samples_split&#39;: 2} | 0.700000 | 5 | 0.700 | 0.7 | 0.70 | . 1 {&#39;max_depth&#39;: 1, &#39;min_samples_split&#39;: 3} | 0.700000 | 5 | 0.700 | 0.7 | 0.70 | . 2 {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 2} | 0.958333 | 3 | 0.925 | 1.0 | 0.95 | . 3 {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 3} | 0.958333 | 3 | 0.925 | 1.0 | 0.95 | . 4 {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2} | 0.975000 | 1 | 0.975 | 1.0 | 0.95 | . 5 {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 3} | 0.975000 | 1 | 0.975 | 1.0 | 0.95 | . print(&#39;GridSearchCV 최적 파라미터:&#39;, grid_dtree.best_params_) print(&#39;GridSearchCV 최고 정확도:{:4f}&#39;.format(grid_dtree.best_score_)) . GridSearchCV 최적 파라미터: {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2} GridSearchCV 최고 정확도:0.975000 . 인덱스 4,5rk rank_test_score가 1인 것으로 보아 공동 1위이며 예측 성능 1등을 의미함. | 열 4,5,6은 cv=3 이라서 열2는 그 세개의 평균을 의미 | . estimator = grid_dtree.best_estimator_ # GridSearchCV의 best_estimator_는 이미 최적 학습이 됐으므로 별도 학습이 필요없음 pred = estimator.predict(X_test) print(&#39;테스트 데이터 세트 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 테스트 데이터 세트 정확도: 0.9667 . 일반적으로 학습 데이터를 GridSearchCV를 이용해 최적 하이퍼 파라미터 튜닝을 수행한 뒤에 별도의 테스트 세트에서 이를 평가하는 것이 일반적인 머신 러닝 모델 적용 방법이다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/01/intro.html",
            "relUrl": "/2022/01/01/intro.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "2021/12/31/FRI",
            "content": "Scikit-Learn :파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리 $ to$ 머신러닝을 위한 다양한 알고리즘과 편리한 프레임 워크, API를 제공 . import sklearn . &#48531;&#44867; &#54408;&#51333; &#50696;&#52769;&#54616;&#44592; . 붓꽃 데이터 세트로 붓꽃의 품종을 분류 . | 분류(Classification)는 대표적인 지도학습(Supervised Learning) 방법 중 하나 . | 지도학습은 학습을 위한 다양한 feature와 분류 결정값인 레이블 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터 세트에서 미지의 레이블을 예측 . | 학습을 위해 주어진 데이터 세트를 학습 데이터 세트, 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세트를 테스트 데이터 세트라 함 . | . . sklearn.datasets내의 모듈은 사이킷런에서 자체적으로 제공하는 데이터 세트를 생성하는 모듈의 모임 . | sklearn.tree내의 모듈은 트리 기반 ML 알고리즘을 구현한 클래스의 모임 . | sklearn.model_selection은 학습 데이터와 검증 데이터, 예측 데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가하기 위한 다양한 모듈의 모임 . | 하이퍼 파라미터 : 머신 러닝 알고리즘별로 최적의 학습을 위해 직접 입력하는 파라미터를 통칭, 하이퍼 파라미터를 통해 머신 러닝 알고리즘의 성능을 튜닝할 수 있다. . | . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split import pandas as pd . iris = load_iris() # 붓꽃 데이터 세트 로딩 . iris.keys() # 따라서 iris.키이름 또는 iris[&#39;키이름&#39;]을 통해 해당 값들을 확인할 수 있다. . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;, &#39;data_module&#39;]) . iris_data = iris.data # iris 데이터 세트에서 feature만으로 구성된 데이터를 numpy로 로딩 . iris_label = iris.target # 데이터 세트에서 레이블(결정값) 데이터를 numpy로 로딩 . iris.target_names . array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) . 붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환 | . iris.feature_names . [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] . iris_df = pd.DataFrame(data=iris_data,columns=iris.feature_names) . iris_df . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . ... ... | ... | ... | ... | . 145 6.7 | 3.0 | 5.2 | 2.3 | . 146 6.3 | 2.5 | 5.0 | 1.9 | . 147 6.5 | 3.0 | 5.2 | 2.0 | . 148 6.2 | 3.4 | 5.4 | 2.3 | . 149 5.9 | 3.0 | 5.1 | 1.8 | . 150 rows × 4 columns . iris_df[&#39;label&#39;]=iris_label . iris_df.head(3) . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) label . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . feature : columns를 의미함 | label(결정값) : 0,1,2 세 가지 값응로 돼 있으며 순서대로 Setosa,versicolor,virginica 품종을 의미 | . . 학습용 데이터와 테스트용 데이터를 분리해보자 | . 학습용 데이터와 테스트용 데이터는 반드시 분리해야 함, 이를 위해 Scikit-learn에선 train_test_split() API를 제공, 해당 API를 이용하면 학습 데이터와 테스트 데이터를 test_size 파라미터 입력 값의 비율로 쉽게 분할함 | . 예를 들어보자, teat_size=0.2로 입력 파라미터를 설정하면 전체 데이터 중 테스트 데이터가 20%, 학습 데이터가 80%로 데이터를 분할함 | . X_train,X_test,y_train,y_test=train_test_split(iris_data,iris_label,test_size=0.2, random_state=11) . train_test_split의 첫 번째 파라미터인 iris_data는 feature 데이터 세트이며, 두 번째 파라미터인 iris_label은 Label 데이터 세트. random_state=11은 호출할 때마다 같은 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 발생 값. . | train_test_split은 호출 시 무작위로 데이터를 분리하므로 random_state를 지정하지 않으면 수행할 때마다 다른 학습/테스트 용 데이터를 만듦. . | X_train,X_test,y_train,y_test = 학습용 feature데이터 세트, 테스트용 feature데이터 세트, 학습용 레이블 데이터 세트, 테스트용 레이블 데이터 세트를 의미 . | . . 이제 이 데이터를 기반으로 머신 러닝 분류 알고리즘의 하나인 의사 결정 트리를 이용해 학습과 예측을 수행 . dt_clf=DecisionTreeClassifier(random_state=11) . dt_clf.fit(X_train,y_train) # 학습용 feature 데이터 세트와 학습용 레이블 데이터 세트를 입력해 학습 수행중 . DecisionTreeClassifier(random_state=11) . 학습 완료/ 예측을 수행해야하는데 학습 데이터가 아닌 다른 데이터를 이용해야 하며, 일반적으로 테스트 데이터 세트를 이용함 | . pred=dt_clf.predict(X_test) # 예측 수행 중 # 예측 label data set . 예측 성능 평가, 여러 평가 방법 중 정확도를 측정해보자. 정확도는 예측 결과가 실제 레이블 값과 얼마나 정확하게 맞는지를 평가하는 지표 | . from sklearn.metrics import accuracy_score print(&#39;예측 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 예측 정확도: 0.9333 . 학습한 의사 결정 트리의 알고리즘 예측 정확도가 약 93.33% | . . Conclusion . 1) 데이터 세트 분리 : 데이터를 학습 데이터와 테스트 데이터로 분리 2) 모델 학습 : 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습 3) 예측 수행 : 학습된 ML 모델을 이용해 테스트 데이터의 분류(즉, 붓꽃 종류)를 예측 4) 평가 : 이렇게 예측된 결과값과 테스트 데이터의 실제 결과값을 비교해 ML 모델 성능을 평가 . . 간단한 실습을 해보았으니 전체적 틀을 review해보자 . ML 모델 학습을 위해서 fit(), 학습된 모델의 예측을 위해 predict()를 사용 | Scikit Learn에서는 분류 알고리즘을 구현한 클래스를 Classifier로, 그리고 회귀 알고리즘을 구현한 클래스를 Regressor로 지칭 | Classifier와 Regressor를 합쳐서 Estimator 클래스라고 부름. 즉, 지도학습의 모든 알고리즘을 구현한 클래스를 통칭해서 Estimator라고 부름 | . Scikit-Learn의 다양한 모듈은 교재 94p,95p 참고 . 머신러닝 모델을 구축하는 주요 프로세스 = feature의 가공, 변경, 추출을 수행하는 feature processing, ML 알고리즙 학습/예측 수행, 그리고 모델 평가 단계를 반복적으로 수행하는 것 | . 사이킷런에 내장되어 있는 데이터 세트는 일반적으로 dict형태 | 이때 key는 data(feature의 데이터 세트),target(레이블 값, 숫자 결과값 데이터 세트),target_names(개별 레이블의 이름), feature_names(feature의 이름), DESCR 데이터 세트에 대한 설명과 각 feature의 설명 . | 앞에 두 key는 ndarray, 다음 두개는 ndarray 또는 list 그 다음은 str . | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2021/12/31/intro.html",
            "relUrl": "/2021/12/31/intro.html",
            "date": " • Dec 31, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "2021/12/30/THU",
            "content": "&#45936;&#51060;&#53552;&#50640; &#47784;&#45944;&#51012; &#47582;&#52632;&#45796; . 애플리케이션을 수정하지 않고도 데이터를 기반으로 패턴을 학습하고 결과를 예측하는 알고리즘 기법을 통칭 | . 데이터 기반으로 통계적인 신뢰도를 강화하고 예측 오류를 최소화하기 위한 다양한 수학적 기법을 적용해 데이터 내의 패턴을 스스로 인지하고 신뢰도 있는 예측 결과를 도출 | . 데이터를 관통하는 패턴을 학습, 이에 기반한 예측 수행 | . 모델을 맞춘다? 모델을 학습시키는 기법들에는 딥러닝, 나이브베이즈, 디시전트리 등이 있음, 모델을 맞추는 행위를 하지 않으면 이 모델은 어떤 문제도 해결할 수가 없음, 따라서 데이터를 모델에 맞추는 행위가 필요함 | . 따라서 이 데이터에 문제를 최대한 많이 맞출 수 있도록 모델을 최적화하여야 하는데 이렇게 최적화가 된 모델이 그 데이터에 해당된 문제를 해결할 수 있게 됨 | . 분류 :지도 학습(Supervised Learning), 비지도 학습(Un-supervised Learning), 강화 학습(Reinforcement Learning) 지도학습 $ to$ 분류(Classification)와 회귀(Regression)로 나눌 수 있음 . . data? Garbage In $ to$ Garbage Out : data도 질이 중요하다 | . 데이터를 이해하고 효율적으로 가공,처리,추출하여 최적의 데이터를 기반으로 알고리즘을 구동할 수 있도록 준비하는 능력 필요 | . . 만약 온도, 습도, 풍속을 정리해놓은 데이터가 있을 때 눈이 오는 여부를 다양한 기법으로 해결할 수 있음 . 1) 조건을 정해서 해결한다.(decision tree 등) . 2) 수식(가중치)으로 해결한다.(선형 회귀, 딥러닝 등) . 이러한 접근 방식을 통해서 가지고 있는 데이터를 50%정도만 해결했다면 이 접근 방식들은 썩 좋지 않은 방식일 것임 . 따라서 머신러닝을 학습한다는 것은 이 정답을 최대한 맞출 수 있도록 모델을 최대한 최적화한다는 의미임. 이렇게 가장 좋은 성능이 나올 수 있는 식과 조건을 찾아나가는 것을 기계가 스스로 하는 것을 머신러닝이라고 생각할 수도 있겠음. . . 딥러닝 등 머신러닝 기법들 전반적으로 공부할 필요가 있다 . 딥러닝 : 자연어와 이미지 처리에 강하다. 그렇지만 다른 과업처리에 있어서도 항상 우수한 결과를 도출해내는 것은 아니다. 대표적으로 KAGGLE에 있는 TITANIC자료에서 실제로 산 승객과 죽은 승객을 처리해내는 과업을 수행할 땐 딥러닝보다 머신러닝의 모델이 더 좋은 결과를 도출해냈음 | . &#44208;&#44397; &#47785;&#51201;&#51008; &#45936;&#51060;&#53552;&#47484; &#47784;&#45944;&#50640; &#52572;&#51201;&#54868; &#49884;&#53412;&#45716; &#44163;&#51060;&#45796; . 머신러닝 논문에 머신러닝 기법들간에 성능을 비교한 표도 있음. 즉, 머신러닝 기법들은 다양한 기법들이 있기 때문에 그 것들간의 차이점과, 각각의 알고리즘이 무엇을 최적화하려는 것인지의 관점에서 이해해보아야함 . . . &#54028;&#51060;&#50028; &#47672;&#49888;&#47084;&#45789; &#49373;&#53468;&#44228;&#47484; &#44396;&#49457;&#54616;&#45716; &#51452;&#50836; &#54056;&#53412;&#51648; . 머신러닝 패키지 : Scikit-Learn . | 행렬/ 선형대수/ 통계 패키지 : Numpy, SciPy . | 데이터 핸들링 : Pandas(Numpy는 행렬 기반의 데이터 처리에 특화) $ to$ 2차원 데이터 처리에 특화,Matplotlib . | 시각화 : matplotlib(너무 세분화 되어 있어서 익히기 어려움, 시각적인 면에서도 투박),Seaborn(matplotlib의 대안이 될 것) . | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2021/12/28/intro.html",
            "relUrl": "/2021/12/28/intro.html",
            "date": " • Dec 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rhkrehtjd.github.io/INTROml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rhkrehtjd.github.io/INTROml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}