{
  
    
        "post0": {
            "title": "2022/01/22/SUN",
            "content": "형태소 : 단어로서 의미를 가지는 최소 단위 | 형태소 분석이란 말뭉치를 이러한 형태소 어근 단위로 쪼개고 각 형태소에 품사 태깅을 부착하는 작업을 일반적으로 지칭한다. | . import warnings warnings.filterwarnings(&#39;ignore&#39;) import pandas as pd train_df=pd.read_csv(&#39;https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt&#39;,sep=&#39; t&#39;) train_df.head(3) . id document label . 0 9976970 | 아 더빙.. 진짜 짜증나네요 목소리 | 0 | . 1 3819312 | 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나 | 1 | . 2 10265843 | 너무재밓었다그래서보는것을추천한다 | 0 | . train_df[&#39;label&#39;].value_counts( ) . 0 75173 1 74827 Name: label, dtype: int64 . train_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 150000 entries, 0 to 149999 Data columns (total 3 columns): # Column Non-Null Count Dtype -- -- 0 id 150000 non-null int64 1 document 149995 non-null object 2 label 150000 non-null int64 dtypes: int64(2), object(1) memory usage: 3.4+ MB . 1이 긍정, 0이 부정 감성. 균등한 분포를 나타내고 있다. | document 칼럼에 Null이 일부 존재. 이 값은 공백으로 변환. | 문자가 아닌 숫자의 경우 단어적인 의미로 부족하므로 파이썬 정규 표현식 모듈인 re를 이용해 이 역시 공백으로 변환 | 테스트 데이터 세트도 로딩후 동일 과정 적용 | . import re train_df = train_df.fillna(&#39; &#39;) # 정규 표현식을 이용하여 숫자를 공백으로 변경(정규 표현식으로 d 는 숫자를 의미함.) train_df[&#39;document&#39;] = train_df[&#39;document&#39;].apply( lambda x : re.sub(r&quot; d+&quot;, &quot; &quot;, x) ) train_df.drop(&#39;id&#39;, axis=1, inplace=True) # 테스트 데이터 셋을 로딩하고 동일하게 Null 및 숫자를 공백으로 변환 test_df = pd.read_csv(&#39;https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt&#39;, sep=&#39; t&#39;) test_df = test_df.fillna(&#39; &#39;) test_df[&#39;document&#39;] = test_df[&#39;document&#39;].apply( lambda x : re.sub(r&quot; d+&quot;, &quot; &quot;, x) ) test_df.drop(&#39;id&#39;, axis=1, inplace=True) . 이제는 TF-IDF 방식으로 단어를 벡터화하는데 먼저 각 문장을 한글 형태소 분석을 통해 형태소 단어로 토큰화한다. | 한글 형태소 엔진은 SNS분석에 적합한 Twitter 클래스를 이용한다. | Twitter 객체의 morphs() 메서드를 이용하면 입력 인자로 들어온 문장을 형태소 단어 형태로 토큰화해 list 객체로 반환. | . from konlpy.tag import Twitter twitter = Twitter() def tw_tokenizer(text): # 입력 인자로 들어온 text 를 형태소 단어로 토큰화 하여 list 객체 반환 tokens_ko = twitter.morphs(text) return tokens_ko . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Twitter 객체의 morphs( ) 객체를 이용한 tokenizer를 사용. ngram_range는 (1,2) tfidf_vect = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2), min_df=3, max_df=0.9) tfidf_vect.fit(train_df[&#39;document&#39;]) tfidf_matrix_train = tfidf_vect.transform(train_df[&#39;document&#39;]) . 로지스틱 회귀를 이용해 분류 기반의 감성 분석을 수행한다. | 로지스틱 회귀의 하이퍼 파라미터 C의 최적화를 위해 GridSearchCV를 이용한다. | . # Logistic Regression 을 이용하여 감성 분석 Classification 수행. lg_clf = LogisticRegression(random_state=0) # Parameter C 최적화를 위해 GridSearchCV 를 이용. params = { &#39;C&#39;: [1 ,3.5, 4.5, 5.5, 10 ] } grid_cv = GridSearchCV(lg_clf , param_grid=params , cv=3 ,scoring=&#39;accuracy&#39;, verbose=1 ) grid_cv.fit(tfidf_matrix_train , train_df[&#39;label&#39;] ) print(grid_cv.best_params_ , round(grid_cv.best_score_,4)) . 이제 테스트 세트를 이용해 최종 감성 분석 예측을 수행한다. | 유의할 것은 재차 언급하지만 테스트 세트를 이용해 예측할 때는 적용한 TfifdVectorizer를 그대로 사용해야 한다. | 그래야만 학습 시 설정된 TfidfVectorizer의 피처 개수와 테스트 데이터를 TfidfVectorizer로 변환할 피처의 개수가 같아진다. | . from sklearn.metrics import accuracy_score # 학습 데이터를 적용한 TfidfVectorizer를 이용하여 테스트 데이터를 TF-IDF 값으로 Feature 변환함. tfidf_matrix_test = tfidf_vect.transform(test_df[&#39;document&#39;]) # classifier 는 GridSearchCV에서 최적 파라미터로 학습된 classifier를 그대로 이용 best_estimator = grid_cv.best_estimator_ preds = best_estimator.predict(tfidf_matrix_test) print(&#39;Logistic Regression 정확도: &#39;,accuracy_score(test_df[&#39;label&#39;],preds)) . Logistic Regression 정확도: 0.85612 . 텍스트 분석 실습, 캐글 Mercari Price Suggestion Challenge 파트는 7zip 압축 파일 문제로 업로드하지 않겠음 | . &#52628;&#52380; &#49884;&#49828;&#53596; . 콘텐츠 기반 필터링(Content based filtering) 방식과 협업 필터링(Collaborative Filtering) 방식으로 나뉜다. 그리고 협업 필터링 방식은 다시 최근접 이웃 협업 필터링과 잠재 요인 협업 필터링으로 나뉜다. . 콘텐츠 기반 필터링 추천 시스템 . 사용자가 특정한 아이템을 매우 선호하는 경우, 그 아이템과 비슷한 콘텐츠를 가진 다른 아이템을 추천하는 방식이다. 예를 들어 사용자가 특정 영화에 높은 평점을 줬다면 그 영화의 장르, 출연 배우, 감독, 영화 키워드 등의 콘텐츠와 유사한 다른 영화를 추천해주는 방식이다. . | 최근접 이웃 협업 필터링 . 우리가 신작 영화가 나왔을 때 평소 취향이 비슷한 친구에게 그 신작 영화의 평가를 요구하듯 사용자가 아이템에 매긴 평점 정보나 상품 구매 이력과 같은 사용자 행동 양식만을 기반으로 추천을 수행하는 것이 협업 필터링 방식이다. 협업 필터링의 주요 목표는 사용자-아이템 평점 매트릭스와 같은 축적된 사용자 행동 데이터를 기반으로 사용자가 아직 평가하지 않은 아이템을 예측 평가하는 것이다. 협업 필터링은 사용자가 평가한 다른 아이템을 기반으로 사용자가 평가하지 않은 아이템의 예측 평가를 도출하는 방식이다. 최근접 이웃 방식과 잠재 요인 방식 둘 다 사용자-아이템 평점 행렬 데이터에만 의지해 추천을 수행한다. 협업 필터링 알고리즘에 사용되는 사용자-아이템 평점 행렬에서 행은 개별 사용자, 열은 개별 아이템으로 구성되며 사용자 아이디 행, 아이템 아이디 열 위치에 해당하는 값이 평점을 나타내는 형태가 돼야 한다. 판다스의 pivot_table()과 같은 메서드를 통해 이러한 행렬 형태로 적절히 변환할 수도 있겠다. 일반적으로 이러한 사용자-아이템 평점 행렬은 많은 아이템을 열로 가지는 다차원 행렬이며, 사용자가 아이템에 평점을 매기는 경우가 많지 않기 때문에 희소 행렬 특성을 가지고 있다. 최근접 이웃 협업 필터링은 메모리 협업 필터링이라고도 하며 일반적으로 사용자 기반과 아이템 기반으로 다시 나뉠 수 있다. 사용자 기반 : 당신과 비슷한 고객들이 다음 상품도 구매했습니다. 특정 사용자와 유사한 다른 사용자를 TOP-N으로 선정해 이 TOP-N 사용자가 좋아하는 아이템을 추천하는 방식이다. 즉 특정 사용자와 타 사용자 간의 유사도를 측정한 뒤 가장 유사도가 높은 TOP-N 사용자를 추출해 그들이 선호하는 아이템을 추천하는 것이다. 아이템 기반 : 이 상품을 선택한 다른 고객들은 다음 상품도 구매했습니다. 아이템이 가지는 속성과는 상관없이 사용자들이 그 아이템을 좋아하는지/싫어하는지의 평가 척도가 유사한 아이템을 추천하는 기준이 된다. 사용자 기반 최근접 이웃 데이터 세트와 행과 열이 서로 반대이다. 일반적으로 사용자 기반보다는 아이템 기반 협업 필터링이 정확도가 더 높다. . | 앞장의 텍스트 분석에서 소개된 유사도 측정 방법인 코사인 유사도는 추천 시스템의 유사도 측정에 가장 많이 적용된다. 추천 시스템에 사용되는 데이터는 피처 벡터화된 텍스트 데이터와 동일하게 다차원 희소 행렬이라는 특징이 있으므로 유사도 측정을 위해 주로 코사인 유사도를 이용한다. | . 잠재 요인 협업 필터링의 이해 사용자-아이템 평점 매트릭스 속에 숨어 있는 잠재 요인을 추출해 추천 예측을 할 수 있게 하는 기법이다. 대규모 다차원 행렬을 SVD와 같은 차원 감소 기법으로 분해하는 과정에서 잠재 요인을 추출하는데 이러한 기법을 행렬 분해라고 한다. 사용자-아이템 평점 행렬 데이터만을 이용해 말 그래도 잠재 요인을 끄집어 내는 것을 의미하는데 잠재 요인이 어떤 것인지는 명확이 정의할 수 없다. 하지만 이러한 잠재 요인을 기반으로 다차원 희소 행렬인 사용자-아이템 행렬 데이터를 저차원 밀집 행렬의 사용자-잠재 요인 행렬과 아이템-잠재 요인 행렬의 전치 행렬로 분해할 수 있으며, 이렇게 분해된 두 행렬의 내적을 통해 새로운 예측 사용자-아이템 평점 행렬 데이터를 만들어서 사용자가 아직 평점을 부여하지 않는 아이템에 대한 예측 평점을 생성하는 것이 잠재 요인 협력 필터링 알고리즘의 골자이다. 행렬 분해에 의해 추출되는 잠재 요인이 정확히 어떤 것인지는 알 수 없지만, 가령 영화 평점 기반의 사용자-아이템 평점 행렬 데이터라면 영화가 가지는 장르별 특성 선호도로 가정할 수 있다. 즉 사용자-잠재요인 행렬은 사용자의 영화 장르에 대한 선호도로 아이템 잠재 요인 행렬은 영화의 장르별 특성값으로 정의할 수 있다. 평점이란 사용자의 특정 영화 장르에 대한 선호도와 개별 영화의 그 장르적 특성값을 반영해 결정된다고 생각할 수 있다. 예를 들어 사용자가 액션 영화를 매우 좋아하고 특정 영화가 액션 영화의 특성이 매우 크다면 사용자가 해당 영화에 높은 평점을 줄 것이다. 따라서 평점은 사용자의 장르별 선호도 벡터와 영화의 장르별 특성 벡터를 서로 곱해서 만들 수 있다. 이처럼 잠재 요인 협업 필터링은 숨겨져 있는 잠재 요인을 기반으로 분해된 매트릭스를 이용해 사용자가 아직 평가하지 않은 아이템에 대한 예측 평가를 수행한다. 사용자-아이템 평점 행렬과 같이 다차원의 매트릭스를 저차원의 매트릭스로 분해하는 기법을 행렬 분해라고 한다. . | . 행렬 분해 행렬 분해는 다차원의 매트릭스를 저차원 매트릭스로 분해하는 기법으로서 대표적으로 SVD,NMF등이 있다. 인수 분해와 비슷하게 생각하면 된다. M개의 사용자 행과 N개의 아이템 열을 가진 평점 행렬 R은 MXN차원으로 구성되며 행렬 분해를 통해서 사용자-K차원 잠재 요인 행렬 P와 K차원 잠재 요인-아이템 행렬 Q.T로 분해될 수 있다. 사용자가 평가하지 않은 아이템에 대한 평점도 잠재 요인으로 분해된 P행렬과 Q행렬을 이용해 예측할 수 있다. 사용자-아이템 평점 행렬의 NaN 값을 포함한 모든 평점 값은 행렬 분해를 통해 얻어진 P행렬과 Q.T행렬의 내적을 통해 예측 평점으로 다시 계산할 수 있다. . | . 그렇다면 R행렬을 어떻게 P와 Q행렬로 분해할까. 행렬 분해는 주로 SVD 방식을 이용한다. 하지만 SVD는 NaN값이 없는 행렬에만 적용할 수 있다. NaN값이 포함된 경우에는 확률적 경사 하강법(SGD)이나 ALS방식을 이용해 SVD를 수행한다. . 확률적 경사 하강법을 이용한 행렬 분해 : 회귀에서 배운 경사 하강법의 한 종류이다. 확률적 경사 하강법을 이용한 행렬 분석을 요약하자면, P와 Q행렬로 계산된 예측 R 행렬 값이 실제 R 행렬 값과 가장 최소의 오류를 가질 수 있도록 반복적인 비용 함수 최적화를 통해 P와 Q를 유추해내는 것이다. | . P와 Q를 임의의 값을 가진 행렬로 설정한다. | P와 Q.T 값을 곱해 예측 R행렬을 계산하고 예측 R행렬과 실제 R행렬에 해당하는 오류 값을 계산한다. | 이 오류 값을 최소화할 수 있도록 P와 Q행렬을 적절한 값으로 각각 업데이트한다. | 만족할만한 오류 값을 가질 때까지 2,3번 작업을 반복하면서 P와 Q값을 업데이트해 근사화한다. | 일반적으로 사용자-아이템 평점 행렬의 경우 행렬 분해를 위해서 단순히 예측 오류값의 최소화와 학습 시 과적합을 피하기 위해서 규제를 반영한 비용 함수를 적용한다. | L2 규제를 반영해 실제 R 행렬 값과 예측 R 행렬 값의 차이를 최소화하는 방향성을 가지고 P행렬과 Q행렬에 업데이트 값을 반복적으로 수행하면서 최적화된 예측 R 행렬을 구하는 방식이 SGD기반의 행렬분해이다. | . SGD를 이용해 행렬 분해를 수행하는 예제를 파이썬으로 구현해보자. | 분해하려는 원본 행렬 R을 P와 Q로 분해한 뒤에 다시 P와 Q.T의 내적으로 예측 행렬을 만드는 예제이다. | 먼저 원본 행렬 R을 미정인 널 값을 포함해 생성하고 분해 행렬 P와 Q는 정규 분포를 가진 랜덤 값으로 초기화한다. 잠재 요인 차원을 3으로 설정한다. | . import numpy as np # 원본 행렬 R 생성, 분해 행렬 P와 Q 초기화, 잠재요인 차원 K는 3 설정. R = np.array([[4, np.NaN, np.NaN, 2, np.NaN ], [np.NaN, 5, np.NaN, 3, 1 ], [np.NaN, np.NaN, 3, 4, 4 ], [5, 2, 1, 2, np.NaN ]]) num_users, num_items = R.shape K=3 # P와 Q 매트릭스의 크기를 지정하고 정규분포를 가진 random한 값으로 입력합니다. np.random.seed(1) P = np.random.normal(scale=1./K, size=(num_users, K)) Q = np.random.normal(scale=1./K, size=(num_items, K)) print(&quot;P:&quot;,P) print(&quot;Q:&quot;,Q) . P: [[ 0.54144845 -0.2039188 -0.17605725] [-0.35765621 0.28846921 -0.76717957] [ 0.58160392 -0.25373563 0.10634637] [-0.08312346 0.48736931 -0.68671357]] Q: [[-0.1074724 -0.12801812 0.37792315] [-0.36663042 -0.05747607 -0.29261947] [ 0.01407125 0.19427174 -0.36687306] [ 0.38157457 0.30053024 0.16749811] [ 0.30028532 -0.22790929 -0.04096341]] . 다음으로 실제 R 행렬과 예측 행렬의 오차를 구하는 함수를 만들어보자 | 이 함수는 실제 R행렬의 널이 아닌 행렬 값의 위치 인덱스를 추출해 이 인덱스에 있는 실제 R행렬 값과 분해된 P,Q를 이용해 다시 조합된 예측 행렬 값의 RMSE 값을 반환한다. | . from sklearn.metrics import mean_squared_error def get_rmse(R, P, Q, non_zeros): error = 0 # 두개의 분해된 행렬 P와 Q.T의 내적으로 예측 R 행렬 생성 full_pred_matrix = np.dot(P, Q.T) # 실제 R 행렬에서 널이 아닌 값의 위치 인덱스 추출하여 실제 R 행렬과 예측 행렬의 RMSE 추출 x_non_zero_ind = [non_zero[0] for non_zero in non_zeros] y_non_zero_ind = [non_zero[1] for non_zero in non_zeros] R_non_zeros = R[x_non_zero_ind, y_non_zero_ind] full_pred_matrix_non_zeros = full_pred_matrix[x_non_zero_ind, y_non_zero_ind] mse = mean_squared_error(R_non_zeros, full_pred_matrix_non_zeros) rmse = np.sqrt(mse) return rmse . 이제 SGD를 기반으로 행렬 분해를 수행한다. | 먼저 R에서 널 값을 제외한 데이터의 행렬 인덱스를 추출한다. | steps는 SGD를 반복해서 업데이트할 횟수 | . non_zeros = [ (i, j, R[i,j]) for i in range(num_users) for j in range(num_items) if R[i,j] &gt; 0 ] steps=1000 learning_rate=0.01 r_lambda=0.01 # SGD 기법으로 P와 Q 매트릭스를 계속 업데이트. for step in range(steps): for i, j, r in non_zeros: # 실제 값과 예측 값의 차이인 오류 값 구함 eij = r - np.dot(P[i, :], Q[j, :].T) # Regularization을 반영한 SGD 업데이트 공식 적용 P[i,:] = P[i,:] + learning_rate*(eij * Q[j, :] - r_lambda*P[i,:]) Q[j,:] = Q[j,:] + learning_rate*(eij * P[i, :] - r_lambda*Q[j,:]) rmse = get_rmse(R, P, Q, non_zeros) if (step % 50) == 0 : print(&quot;### iteration step : &quot;, step,&quot; rmse : &quot;, rmse) . ### iteration step : 0 rmse : 3.2388050277987723 ### iteration step : 50 rmse : 0.4876723101369647 ### iteration step : 100 rmse : 0.15643403848192458 ### iteration step : 150 rmse : 0.07455141311978064 ### iteration step : 200 rmse : 0.043252267985793146 ### iteration step : 250 rmse : 0.029248328780879226 ### iteration step : 300 rmse : 0.022621116143829507 ### iteration step : 350 rmse : 0.019493636196525232 ### iteration step : 400 rmse : 0.018022719092132773 ### iteration step : 450 rmse : 0.01731968595344283 ### iteration step : 500 rmse : 0.016973657887570985 ### iteration step : 550 rmse : 0.01679680459589558 ### iteration step : 600 rmse : 0.016701322901884634 ### iteration step : 650 rmse : 0.016644736912476574 ### iteration step : 700 rmse : 0.016605910068210012 ### iteration step : 750 rmse : 0.01657420047570488 ### iteration step : 800 rmse : 0.01654431582921612 ### iteration step : 850 rmse : 0.016513751774735037 ### iteration step : 900 rmse : 0.01648146573819507 ### iteration step : 950 rmse : 0.01644717168347911 . 이제 분해된 P와 Q 함수를 P*Q.T로 예측 행렬을 만들어서 출력해보자 | . pred_matrix = np.dot(P, Q.T) print(&#39;예측 행렬: n&#39;, np.round(pred_matrix, 3)) . 예측 행렬: [[3.991 0.897 1.306 2.002 1.663] [6.696 4.978 0.979 2.981 1.003] [6.677 0.391 2.987 3.977 3.986] [4.968 2.005 1.006 2.017 1.14 ]] . 원본 행렬과 비교해 널이 아닌 값은 큰 차이가 나지 않으며, 널인 값은 새로운 예측값으로 채워졌다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/23/intro.html",
            "relUrl": "/2022/01/23/intro.html",
            "date": " • Jan 23, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "2022/01/22/SAT",
            "content": "&#53664;&#54589; &#47784;&#45944;&#47553; . 문서 집합에 숨어 있는 주제를 찾아내는 것. 토픽 모델링을 적용해 숨어 있는 중요 주제를 효과적으로 찾아낼 수 있다. 중심 단어를 함축적으로 추출. 머신러닝 기반의 토픽 모델링에 자주 사용되는 기법은 LSA와 LDA이다. LDA는 Count 기반의 벡터화만 사용한다. . from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import CountVectorizer from sklearn.decomposition import LatentDirichletAllocation # 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 전자공학, 의학 등 8개 주제를 추출. cats = [&#39;rec.motorcycles&#39;, &#39;rec.sport.baseball&#39;, &#39;comp.graphics&#39;, &#39;comp.windows.x&#39;, &#39;talk.politics.mideast&#39;, &#39;soc.religion.christian&#39;, &#39;sci.electronics&#39;, &#39;sci.med&#39; ] # categories 파라미터를 통해 필요한 주제만 필터링해 추출하고 추출된 텍스트를 Count 기반으로 벡터화 변환 # 위에서 cats 변수로 기재된 category만 추출. featch_20newsgroups( )의 categories에 cats 입력 news_df= fetch_20newsgroups(subset=&#39;all&#39;,remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;), categories=cats, random_state=0) #LDA 는 Count기반의 Vectorizer만 적용합니다. count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words=&#39;english&#39;, ngram_range=(1,2)) feat_vect = count_vect.fit_transform(news_df.data) print(&#39;CountVectorizer Shape:&#39;, feat_vect.shape) . CountVectorizer Shape: (7862, 1000) . 이렇게 feature 벡터화된 데이터 세트를 기반으로 LDA 토픽 모델링을 수행 | 토픽의 개수는 위의 뉴스그룹에서 추출한 주제와 동일한 8개로 정한다. | . lda = LatentDirichletAllocation(n_components=8, random_state=0) lda.fit(feat_vect) . LatentDirichletAllocation(n_components=8, random_state=0) . 위 객체는 components_ 속성값을 가지게 된다. 이 속성값은 개별 토픽별로 각 word feature가 얼마나 많이 그 토픽에 할당됐는지에 대한 수치를 가지고 있다. 높은 값일수록 해당 word feature는 그 토픽의 중심 word가 된다. | . print(lda.components_.shape) lda.components_ . (8, 1000) . array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ..., 3.02911688e+01, 8.66830093e+01, 6.79285199e+01], [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ..., 1.81506995e+02, 1.25097844e-01, 9.39593286e+01], [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ..., 1.25105772e-01, 3.63689741e+01, 1.25025218e-01], ..., [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ..., 1.45056650e+01, 8.33854413e+00, 1.55690009e+01], [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ..., 9.17278769e+01, 1.25177668e-01, 3.74575887e+01], [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ..., 4.87048440e+01, 1.25034678e-01, 1.25074632e-01]]) . 8개의 토픽별로 1000개의 word feature가 해당 토픽별로 연관도 값을 가지고 있다. | display_topics() 함수를 만들어서 각 토픽별로 연관도가 높은 순으로 Word를 나열해보자. | . def display_topic_words(model, feature_names, no_top_words): for topic_index, topic in enumerate(model.components_): print(&#39; nTopic #&#39;,topic_index) # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array index를 반환. topic_word_indexes = topic.argsort()[::-1] top_indexes=topic_word_indexes[:no_top_words] # top_indexes대상인 index별로 feature_names에 해당하는 word feature 추출 후 join으로 concat feature_concat = &#39; + &#39;.join([str(feature_names[i])+&#39;*&#39;+str(round(topic[i],1)) for i in top_indexes]) print(feature_concat) # CountVectorizer객체내의 전체 word들의 명칭을 get_features_names( )를 통해 추출 feature_names = count_vect.get_feature_names() # Topic별 가장 연관도가 높은 word를 15개만 추출 display_topic_words(lda, feature_names, 15) # 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 전자공학, 의학 등 8개 주제를 추출. . Topic # 0 year*703.2 + 10*563.6 + game*476.3 + medical*413.2 + health*377.4 + team*346.8 + 12*343.9 + 20*340.9 + disease*332.1 + cancer*319.9 + 1993*318.3 + games*317.0 + years*306.5 + patients*299.8 + good*286.3 Topic # 1 don*1454.3 + just*1392.8 + like*1190.8 + know*1178.1 + people*836.9 + said*802.5 + think*799.7 + time*754.2 + ve*676.3 + didn*675.9 + right*636.3 + going*625.4 + say*620.7 + ll*583.9 + way*570.3 Topic # 2 image*1047.7 + file*999.1 + jpeg*799.1 + program*495.6 + gif*466.0 + images*443.7 + output*442.3 + format*442.3 + files*438.5 + color*406.3 + entry*387.6 + 00*334.8 + use*308.5 + bit*308.4 + 03*258.7 Topic # 3 like*620.7 + know*591.7 + don*543.7 + think*528.4 + use*514.3 + does*510.2 + just*509.1 + good*425.8 + time*417.4 + book*410.7 + read*402.9 + information*395.2 + people*393.5 + used*388.2 + post*368.4 Topic # 4 armenian*960.6 + israel*815.9 + armenians*699.7 + jews*690.9 + turkish*686.1 + people*653.0 + israeli*476.1 + jewish*467.0 + government*464.4 + war*417.8 + dos dos*401.1 + turkey*393.5 + arab*386.1 + armenia*346.3 + 000*345.2 Topic # 5 edu*1613.5 + com*841.4 + available*761.5 + graphics*708.0 + ftp*668.1 + data*517.9 + pub*508.2 + motif*460.4 + mail*453.3 + widget*447.4 + software*427.6 + mit*421.5 + information*417.3 + version*413.7 + sun*402.4 Topic # 6 god*2013.0 + people*721.0 + jesus*688.7 + church*663.0 + believe*563.0 + christ*553.1 + does*500.1 + christian*474.8 + say*468.6 + think*446.0 + christians*443.5 + bible*422.9 + faith*420.1 + sin*396.5 + life*371.2 Topic # 7 use*685.8 + dos*635.0 + thanks*596.0 + windows*548.7 + using*486.5 + window*483.1 + does*456.2 + display*389.1 + help*385.2 + like*382.8 + problem*375.7 + server*370.2 + need*366.3 + know*355.5 + run*315.3 . 문서 군집화 소개와 실습 | . 비슷한 텍스트 구성의 문서를 군집화 하는 것. 동일한 군집에 속하는 문서를 같은 카테고리 소속으로 분류할 수 있으므로 앞에서 소개한 텍스트 분류 기반의 문서 분류와 유사하다. 하지만 텍스트 분류 기반의 문서 분류는 사전에 결정 카테고리 값을 가진 학습 데이터 세트가 필요한 데 반해, 문섭 군집화는 학습 데이터 세트가 필요없는 비지도학습 기반으로 동작한다. 이전 장에서 배웠던 군집화 기법을 활용해 텍스트 기반의 문서 군집화를 적용한다. . Opinion Review 데이터 세트를 이용한 문서 군집화 수행하기 해당 데이터 세트는 51개의 텍스트 파일로 구성돼 있으며 각 파일은 100개 정도의 문장을 가지고 있다. . | . import pandas as pd import glob ,os path = r&#39;C: Users ehfus Downloads OpinosisDataset1.0 OpinosisDataset1.0 topics&#39; # path로 지정한 디렉토리 밑에 있는 모든 .data 파일들의 파일명을 리스트로 취합 all_files = glob.glob(os.path.join(path, &quot;*.data&quot;)) filename_list = [] opinion_text = [] # 개별 파일들의 파일명은 filename_list 리스트로 취합, # 개별 파일들의 파일내용은 DataFrame로딩 후 다시 string으로 변환하여 opinion_text 리스트로 취합 for file_ in all_files: # 개별 파일을 읽어서 DataFrame으로 생성 df = pd.read_table(file_,index_col=None, header=0,encoding=&#39;latin1&#39;) # 절대경로로 주어진 file 명을 가공. 만일 Linux에서 수행시에는 아래 를 / 변경. 맨 마지막 .data 확장자도 제거 filename_ = file_.split(&#39; &#39;)[-1] filename = filename_.split(&#39;.&#39;)[0] #파일명 리스트와 파일내용 리스트에 파일명과 파일 내용을 추가. filename_list.append(filename) opinion_text.append(df.to_string()) # 파일명 리스트와 파일내용 리스트를 DataFrame으로 생성 document_df = pd.DataFrame({&#39;filename&#39;:filename_list, &#39;opinion_text&#39;:opinion_text}) document_df.head() . filename opinion_text . 0 accuracy_garmin_nuvi_255W_gps | ... | . 1 bathroom_bestwestern_hotel_sfo | ... | . 2 battery-life_amazon_kindle | ... | . 3 battery-life_ipod_nano_8gb | ... | . 4 battery-life_netbook_1005ha | ... | . 각 파일 이름 자체만으로 의견의 텍스트가 어떠한 제품/서비스에 대한 리뷰인지 알 수 있다. | 문서를 TF-IDF 형태로 feature 벡터화하겠다. | . from nltk.stem import WordNetLemmatizer import nltk import string # nltk는 remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation) lemmar = WordNetLemmatizer() def LemTokens(tokens): return [lemmar.lemmatize(token) for token in tokens] def LemNormalize(text): return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict))) from sklearn.feature_extraction.text import TfidfVectorizer tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words=&#39;english&#39; , ngram_range=(1,2), min_df=0.05, max_df=0.85 ) #opinion_text 컬럼값으로 feature vectorization 수행 feature_vect = tfidf_vect.fit_transform(document_df[&#39;opinion_text&#39;]) . 문서별 텍스트가 TF-IDF 변환된 feature 벡터화 행렬 데이터에 대해서 군집화를 수행해 어떤 문서끼리 군집되는지 확인해보자. 군집화 기법은 K-평균을 적용한다. | 먼저 5개의 중심(Centroid) 기반으로 어떻게 군집화되는지 확인해 보자. | . from sklearn.cluster import KMeans # 5개 집합으로 군집화 수행. 예제를 위해 동일한 클러스터링 결과 도출용 random_state=0 km_cluster = KMeans(n_clusters=5, max_iter=10000, random_state=0) km_cluster.fit(feature_vect) cluster_label = km_cluster.labels_ cluster_centers = km_cluster.cluster_centers_ . 각 데이터별로 할당된 군집의 레이블을 파일명과 파일 내용을 가지고 있는 document_df DataFrame에 &#39;cluster_label&#39; 칼럼을 추가해 저장한다. | 각 파일명은 의견 리뷰에 대한 주제를 나타낸다. | . document_df[&#39;cluster_label&#39;] = cluster_label document_df.head() . filename opinion_text cluster_label . 0 accuracy_garmin_nuvi_255W_gps | ... | 0 | . 1 bathroom_bestwestern_hotel_sfo | ... | 1 | . 2 battery-life_amazon_kindle | ... | 3 | . 3 battery-life_ipod_nano_8gb | ... | 3 | . 4 battery-life_netbook_1005ha | ... | 3 | . 판다스 DataFrame의 sort_values(by=지정칼럼명)를 수행하면 인자로 입력된 &#39;정렬칼럼명&#39;으로 데이터를 정렬할 수 있다. | 먼저 cluster_label=0인 데이터 세트를 살펴보자 | . document_df[document_df[&#39;cluster_label&#39;]==0].sort_values(by=&#39;filename&#39;) . filename opinion_text cluster_label . 0 accuracy_garmin_nuvi_255W_gps | ... | 0 | . 8 directions_garmin_nuvi_255W_gps | ... | 0 | . 9 display_garmin_nuvi_255W_gps | ... | 0 | . 33 satellite_garmin_nuvi_255W_gps | ... | 0 | . 34 screen_garmin_nuvi_255W_gps | ... | 0 | . 43 speed_garmin_nuvi_255W_gps | ... | 0 | . 47 transmission_toyota_camry_2007 | ... | 0 | . 48 updates_garmin_nuvi_255W_gps | ... | 0 | . document_df[document_df[&#39;cluster_label&#39;]==1].sort_values(by=&#39;filename&#39;) . filename opinion_text cluster_label . 1 bathroom_bestwestern_hotel_sfo | ... | 1 | . 13 food_holiday_inn_london | ... | 1 | . 14 food_swissotel_chicago | ... | 1 | . 15 free_bestwestern_hotel_sfo | ... | 1 | . 20 location_bestwestern_hotel_sfo | ... | 1 | . 21 location_holiday_inn_london | ... | 1 | . 24 parking_bestwestern_hotel_sfo | ... | 1 | . 28 price_holiday_inn_london | ... | 1 | . 32 room_holiday_inn_london | ... | 1 | . 30 rooms_bestwestern_hotel_sfo | ... | 1 | . 31 rooms_swissotel_chicago | ... | 1 | . 38 service_bestwestern_hotel_sfo | ... | 1 | . 39 service_holiday_inn_london | ... | 1 | . 40 service_swissotel_hotel_chicago | ... | 1 | . 45 staff_bestwestern_hotel_sfo | ... | 1 | . 46 staff_swissotel_chicago | ... | 1 | . 전반적으로 군집화된 결과를 살펴보면 군집 개수가 약간 많게 설정돼 있어서 세분화되어 군집화된 경향이 있다. 중심 개수를 5개에서 3개로 낮춰서 3개 그룹으로 군집화한 뒤 결과를 확인해 보자 | . from sklearn.cluster import KMeans # 3개의 집합으로 군집화 km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0) km_cluster.fit(feature_vect) cluster_label = km_cluster.labels_ # 소속 클러스터를 cluster_label 컬럼으로 할당하고 cluster_label 값으로 정렬 document_df[&#39;cluster_label&#39;] = cluster_label document_df.sort_values(by=&#39;cluster_label&#39;) . filename opinion_text cluster_label . 0 accuracy_garmin_nuvi_255W_gps | ... | 0 | . 48 updates_garmin_nuvi_255W_gps | ... | 0 | . 44 speed_windows7 | ... | 0 | . 43 speed_garmin_nuvi_255W_gps | ... | 0 | . 42 sound_ipod_nano_8gb | headphone jack i got a clear case for it a... | 0 | . 41 size_asus_netbook_1005ha | ... | 0 | . 36 screen_netbook_1005ha | ... | 0 | . 35 screen_ipod_nano_8gb | ... | 0 | . 34 screen_garmin_nuvi_255W_gps | ... | 0 | . 33 satellite_garmin_nuvi_255W_gps | ... | 0 | . 27 price_amazon_kindle | ... | 0 | . 26 performance_netbook_1005ha | ... | 0 | . 49 video_ipod_nano_8gb | ... | 0 | . 23 navigation_amazon_kindle | ... | 0 | . 19 keyboard_netbook_1005ha | ... | 0 | . 50 voice_garmin_nuvi_255W_gps | ... | 0 | . 9 display_garmin_nuvi_255W_gps | ... | 0 | . 4 battery-life_netbook_1005ha | ... | 0 | . 3 battery-life_ipod_nano_8gb | ... | 0 | . 2 battery-life_amazon_kindle | ... | 0 | . 8 directions_garmin_nuvi_255W_gps | ... | 0 | . 10 eyesight-issues_amazon_kindle | ... | 0 | . 11 features_windows7 | ... | 0 | . 12 fonts_amazon_kindle | ... | 0 | . 5 buttons_amazon_kindle | ... | 0 | . 13 food_holiday_inn_london | ... | 1 | . 39 service_holiday_inn_london | ... | 1 | . 38 service_bestwestern_hotel_sfo | ... | 1 | . 1 bathroom_bestwestern_hotel_sfo | ... | 1 | . 14 food_swissotel_chicago | ... | 1 | . 20 location_bestwestern_hotel_sfo | ... | 1 | . 24 parking_bestwestern_hotel_sfo | ... | 1 | . 15 free_bestwestern_hotel_sfo | ... | 1 | . 31 rooms_swissotel_chicago | ... | 1 | . 30 rooms_bestwestern_hotel_sfo | ... | 1 | . 45 staff_bestwestern_hotel_sfo | ... | 1 | . 40 service_swissotel_hotel_chicago | ... | 1 | . 21 location_holiday_inn_london | ... | 1 | . 46 staff_swissotel_chicago | ... | 1 | . 32 room_holiday_inn_london | ... | 1 | . 28 price_holiday_inn_london | ... | 1 | . 47 transmission_toyota_camry_2007 | ... | 2 | . 16 gas_mileage_toyota_camry_2007 | ... | 2 | . 6 comfort_honda_accord_2008 | ... | 2 | . 7 comfort_toyota_camry_2007 | ... | 2 | . 29 quality_toyota_camry_2007 | ... | 2 | . 22 mileage_honda_accord_2008 | ... | 2 | . 18 interior_toyota_camry_2007 | ... | 2 | . 17 interior_honda_accord_2008 | ... | 2 | . 37 seats_honda_accord_2008 | ... | 2 | . 25 performance_honda_accord_2008 | ... | 2 | . Cluster별로 잘 구성됐음을 확인할 수 있다. | . 군집별 핵심 단어 추출해보자 . 각 군집에 속한 문서는 핵심 단어를 주축으로 군집화돼 있을 것이다. 이번에는 각 군집을 구성하는 핵심 단어가 어떤 것이 있는지 확인해보자. KMeans 객체는 각 군집을 구성하는 단어 feature 가 군집의 중심을 기준으로 얼마나 가깝게 위치해 있는지 cluster_centers_라는 속성으로 제공한다. . cluster_centers = km_cluster.cluster_centers_ print(&#39;cluster_centers shape :&#39;,cluster_centers.shape) print(cluster_centers) . cluster_centers shape : (3, 4611) [[0.01005322 0. 0. ... 0.00706287 0. 0. ] [0. 0.00099499 0.00174637 ... 0. 0.00183397 0.00144581] [0. 0.00092551 0. ... 0. 0. 0. ]] . cluster_centers_는 (3,2409) 배열이다. 이는 군집이 3개, word 피처가 2409개로 구성되었음을 의미한다. | 각 행의 배열 값은 각 군집 내의 2409개 피처의 위치가 개별 중심과 얼마나 가까운가를 상대 값으로 나타낸 것이다. 0~1로 표현하며 1에 가까울수록 중심과 가까운 값을 의미한다. | 이제 clustercenters 속성값을 이용해 각 군집별 핵심 단어를 찾아보자. | ndarray의 argsort()[:,::-1]를 이용하면 cluster_centers 배열 내 값이 큰 순으로 정렬된 위치 인덱스 값을 반환한다. | 이 위치 인덱스 값을 통해 핵심 단어 피처의 이름을 출력한다. | 이를 새로운 함수를 만들어 처리해보자 | . def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num, top_n_features=10): cluster_details = {} # cluster_centers array 의 값이 큰 순으로 정렬된 index 값을 반환 # 군집 중심점(centroid)별 할당된 word 피처들의 거리값이 큰 순으로 값을 구하기 위함. centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:,::-1] #개별 군집별로 iteration하면서 핵심단어, 그 단어의 중심 위치 상대값, 대상 파일명 입력 for cluster_num in range(clusters_num): # 개별 군집별 정보를 담을 데이터 초기화. cluster_details[cluster_num] = {} cluster_details[cluster_num][&#39;cluster&#39;] = cluster_num # cluster_centers_.argsort()[:,::-1] 로 구한 index 를 이용하여 top n 피처 단어를 구함. top_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features] top_features = [ feature_names[ind] for ind in top_feature_indexes ] # top_feature_indexes를 이용해 해당 피처 단어의 중심 위치 상댓값 구함 top_feature_values = cluster_model.cluster_centers_[cluster_num, top_feature_indexes].tolist() # cluster_details 딕셔너리 객체에 개별 군집별 핵심 단어와 중심위치 상대값, 그리고 해당 파일명 입력 cluster_details[cluster_num][&#39;top_features&#39;] = top_features cluster_details[cluster_num][&#39;top_features_value&#39;] = top_feature_values filenames = cluster_data[cluster_data[&#39;cluster_label&#39;] == cluster_num][&#39;filename&#39;] filenames = filenames.values.tolist() cluster_details[cluster_num][&#39;filenames&#39;] = filenames return cluster_details . get_cluster_details()를 호출하면 dictionary를 원소로 가지는 리스트인 cluster_details를 반환한다. 이 리스트에는 개별 군집번호, 핵심 단어, 핵심단어 중심 위치 상댓값, 파일명 속성 값 정보등이 있는데 이를 좀 더 보기 좋게 표현하기 위해서 별도의 함수를 만들자 | . def print_cluster_details(cluster_details): for cluster_num, cluster_detail in cluster_details.items(): print(&#39;####### Cluster {0}&#39;.format(cluster_num)) print(&#39;Top features:&#39;, cluster_detail[&#39;top_features&#39;]) print(&#39;Reviews 파일명 :&#39;,cluster_detail[&#39;filenames&#39;][:7]) print(&#39;==================================================&#39;) . feature_names = tfidf_vect.get_feature_names() cluster_details = get_cluster_details(cluster_model=km_cluster, cluster_data=document_df, feature_names=feature_names, clusters_num=3, top_n_features=10 ) print_cluster_details(cluster_details) . ####### Cluster 0 Top features: [&#39;screen&#39;, &#39;battery&#39;, &#39;keyboard&#39;, &#39;battery life&#39;, &#39;life&#39;, &#39;kindle&#39;, &#39;direction&#39;, &#39;video&#39;, &#39;size&#39;, &#39;voice&#39;] Reviews 파일명 : [&#39;accuracy_garmin_nuvi_255W_gps&#39;, &#39;battery-life_amazon_kindle&#39;, &#39;battery-life_ipod_nano_8gb&#39;, &#39;battery-life_netbook_1005ha&#39;, &#39;buttons_amazon_kindle&#39;, &#39;directions_garmin_nuvi_255W_gps&#39;, &#39;display_garmin_nuvi_255W_gps&#39;] ================================================== ####### Cluster 1 Top features: [&#39;room&#39;, &#39;hotel&#39;, &#39;service&#39;, &#39;staff&#39;, &#39;food&#39;, &#39;location&#39;, &#39;bathroom&#39;, &#39;clean&#39;, &#39;price&#39;, &#39;parking&#39;] Reviews 파일명 : [&#39;bathroom_bestwestern_hotel_sfo&#39;, &#39;food_holiday_inn_london&#39;, &#39;food_swissotel_chicago&#39;, &#39;free_bestwestern_hotel_sfo&#39;, &#39;location_bestwestern_hotel_sfo&#39;, &#39;location_holiday_inn_london&#39;, &#39;parking_bestwestern_hotel_sfo&#39;] ================================================== ####### Cluster 2 Top features: [&#39;interior&#39;, &#39;seat&#39;, &#39;mileage&#39;, &#39;comfortable&#39;, &#39;gas&#39;, &#39;gas mileage&#39;, &#39;transmission&#39;, &#39;car&#39;, &#39;performance&#39;, &#39;quality&#39;] Reviews 파일명 : [&#39;comfort_honda_accord_2008&#39;, &#39;comfort_toyota_camry_2007&#39;, &#39;gas_mileage_toyota_camry_2007&#39;, &#39;interior_honda_accord_2008&#39;, &#39;interior_toyota_camry_2007&#39;, &#39;mileage_honda_accord_2008&#39;, &#39;performance_honda_accord_2008&#39;] ================================================== . &#47928;&#49436; &#50976;&#49324;&#46020; . 中 코사인 유사도를 알아보자. 문서와 문서 간의 유사도 비교는 일반적으로 코사인 유사도를 사용한다. 코사인 유사도는 벡터와 벡터 간의 유사도를 비교할 때 벡터의 크기보다는 벡터의 상호 방향성이 얼마나 유사한지에 기반한다. 즉, 코사인 유사도는 두 벡터 사이의 사잇각을 구해서 얼마나 유사한지 수치로 적용한 것이다. . 두 벡터의 사잇각은 유사하거나 관련이 없거나 아예 반대 관계가 될 수 있다. | 두 벡터 A와 B의 코사인 값은 다음 식으로 구할 수 있다. | $A * B = |A||B|cos theta$ | 따라서 유사도 $cos theta$는 다음과 같이 두 벡터의 내적을 총 벡터 크기의 합으로 나눈 것이다. | $similarity = cos theta = frac{A*B}{|A||B|}$ | . 코사인 유사도가 문서의 유사도 비교에 가장 많이 사용되는 이유가 있다. 문서를 피처 벡터화 변환하면 차원이 매우 많은 희소 행렬이 되기 쉽다. 이러한 희소 행렬 기반에서 문서와 문서 벡터간의 크기에 기반한 유사도 지표(예를 들어 유클리드 거리 기반 지표)는 정확도가 떨어지기 쉽다. 또한 문서가 매우 긴 경우 단어의 빈도수도 더 많을 것이기 때문에 이러한 빈도수에만 기반해서는 공정한 비교를 할 수 없다. 예를 들어 A문서에서 머신러닝이라는 단어가 5번 언급되고 B문서에서는 3번 언급됐을 때 A문서가 머신러닝과 더 밀접하게 관련된 문서라고 쉽게 판단해서는 안 된다. A문서가 B문서보다 10이상 크다면 오히려 B문서가 머신러닝이라는 토픽과 더 밀접하게 관련된 문서라고 판단할 수 있다. . 간단한 문서에 대해서 서로 간의 문서 유사도를 코사인 유사도 기반으로 구해보자. 먼저 두개의 넘파이 배열에 대한 코사인 유사도를 구하는 cos_similarity() 함수를 작성해보자 . import numpy as np def cos_similarity(v1, v2): dot_product = np.dot(v1, v2) l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2)))) similarity = dot_product / l2_norm return similarity . doc_list로 정의된 3개의 간단한 문서의 유사도를 비교하기 위해 이 문서를 TF-IDF로 벡터화된 행렬로 변환한다. | . from sklearn.feature_extraction.text import TfidfVectorizer doc_list = [&#39;if you take the blue pill, the story ends&#39; , &#39;if you take the red pill, you stay in Wonderland&#39;, &#39;if you take the red pill, I show you how deep the rabbit hole goes&#39;] tfidf_vect_simple = TfidfVectorizer() feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list) print(feature_vect_simple.shape) . (3, 18) . 반환된 행렬은 희소 행렬이므로 앞에서 작성한 cos_similarity() 함수의 인자인 array로 만들기 위해 밀집 행렬로 변환한 뒤 다시 각각을 배열로 변환한다. | . feature_vect_dense = feature_vect_simple.todense() #첫번째 문장과 두번째 문장의 feature vector 추출 vect1 = np.array(feature_vect_dense[0]).reshape(-1,) vect2 = np.array(feature_vect_dense[1]).reshape(-1,) #첫번째 문장과 두번째 문장의 feature vector로 두개 문장의 Cosine 유사도 추출 similarity_simple = cos_similarity(vect1, vect2 ) print(&#39;문장 1, 문장 2 Cosine 유사도: {0:.3f}&#39;.format(similarity_simple)) . 문장 1, 문장 2 Cosine 유사도: 0.402 . 첫 번째 문장과 두 번째 문장의 코사인 유사도는 0.402이다. 다음으로 첫 번째 문장과 세 번째 문장, 그리고 두 번째 문장과 세 번째 문장의 유사도로 측정해보자 | . vect1 = np.array(feature_vect_dense[0]).reshape(-1,) vect3 = np.array(feature_vect_dense[2]).reshape(-1,) similarity_simple = cos_similarity(vect1, vect3 ) print(&#39;문장 1, 문장 3 Cosine 유사도: {0:.3f}&#39;.format(similarity_simple)) vect2 = np.array(feature_vect_dense[1]).reshape(-1,) vect3 = np.array(feature_vect_dense[2]).reshape(-1,) similarity_simple = cos_similarity(vect2, vect3 ) print(&#39;문장 2, 문장 3 Cosine 유사도: {0:.3f}&#39;.format(similarity_simple)) . 문장 1, 문장 3 Cosine 유사도: 0.404 문장 2, 문장 3 Cosine 유사도: 0.456 . 사이킷런은 코사인 유사도를 측정하기 위해 sklearn.metrics.pairwise.cosine_similarity API를 제공한다.ㅏ | 이번에는 이를 이용해 앞 예제의 문서 유사도를 측정해보자. | cosine_similarity()는 희소 행렬, 밀집 행렬 모두가 가능하며, 행렬 또는 배열 모두 가능하다. | 앞서 만든 cos_similarity() 함수와 같이 별도의 변환 작업이 필요하지 않다. | 첫 번째 문서와 비교해 바로 자신 문서인 첫 번째 문서, 그리고 두 번째, 세 번째 문서의 유사도를 측정해보자 | . from sklearn.metrics.pairwise import cosine_similarity similarity_simple_pair = cosine_similarity(feature_vect_simple[0] , feature_vect_simple) print(similarity_simple_pair) . [[1. 0.40207758 0.40425045]] . 여기서 값 1은 비교 기준은 첫 번째 문서 자신에 대한 유사도 측정이다.(feature_vect[1:]로 무시할 수 있다.) | cosine_similarity()는 쌍으로 코사인 유사도 값을 제공할 수 있다. | . similarity_simple_pair = cosine_similarity(feature_vect_simple , feature_vect_simple) print(similarity_simple_pair) print(&#39;shape:&#39;,similarity_simple_pair.shape) . [[1. 0.40207758 0.40425045] [0.40207758 1. 0.45647296] [0.40425045 0.45647296 1. ]] shape: (3, 3) . Opinion Review 데이터 세트를 이용한 문서 유사도 측정 | 앞 절의 문서 군집화에서 사용한 Opinion Review 데이터 세트를 이용해 이들 문서 간의 유사도를 측정해보자. | 다시 데이터 세트를 새롭게 DataFrame으로 로드하고 문서 군집화를 적용해보자 | . from nltk.stem import WordNetLemmatizer import nltk import string remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation) lemmar = WordNetLemmatizer() def LemTokens(tokens): return [lemmar.lemmatize(token) for token in tokens] def LemNormalize(text): return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict))) . import pandas as pd import glob ,os from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.cluster import KMeans path = r&#39;C: Users ehfus Downloads OpinosisDataset1.0 OpinosisDataset1.0 topics&#39; all_files = glob.glob(os.path.join(path, &quot;*.data&quot;)) filename_list = [] opinion_text = [] for file_ in all_files: df = pd.read_table(file_,index_col=None, header=0,encoding=&#39;latin1&#39;) filename_ = file_.split(&#39; &#39;)[-1] filename = filename_.split(&#39;.&#39;)[0] filename_list.append(filename) opinion_text.append(df.to_string()) document_df = pd.DataFrame({&#39;filename&#39;:filename_list, &#39;opinion_text&#39;:opinion_text}) tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words=&#39;english&#39; , ngram_range=(1,2), min_df=0.05, max_df=0.85 ) feature_vect = tfidf_vect.fit_transform(document_df[&#39;opinion_text&#39;]) km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0) km_cluster.fit(feature_vect) cluster_label = km_cluster.labels_ cluster_centers = km_cluster.cluster_centers_ document_df[&#39;cluster_label&#39;] = cluster_label . C: Users ehfus Anaconda3 envs dv2021 lib site-packages sklearn feature_extraction text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [&#39;ha&#39;, &#39;le&#39;, &#39;u&#39;, &#39;wa&#39;] not in stop_words. warnings.warn(&#39;Your stop_words may be inconsistent with &#39; . 이전 절에서 해당 문서의 군집화는 전자제품, 호텔, 자동차를 주제로 군집화됐다. 이 중 호텔을 주제로 군집화된 문서를 이용해 특정 문서와 다른 문서 간의 유사도를 알아보자. | . from sklearn.metrics.pairwise import cosine_similarity # cluster_label=1인 데이터는 호텔로 클러스터링된 데이터임. DataFrame에서 해당 Index를 추출 hotel_indexes = document_df[document_df[&#39;cluster_label&#39;]==1].index print(&#39;호텔로 클러스터링 된 문서들의 DataFrame Index:&#39;, hotel_indexes) # 호텔로 클러스터링된 데이터 중 첫번째 문서를 추출하여 파일명 표시. comparison_docname = document_df.iloc[hotel_indexes[0]][&#39;filename&#39;] print(&#39;##### 비교 기준 문서명 &#39;,comparison_docname,&#39; 와 타 문서 유사도######&#39;) &#39;&#39;&#39; document_df에서 추출한 Index 객체를 feature_vect로 입력하여 호텔 클러스터링된 feature_vect 추출 이를 이용하여 호텔로 클러스터링된 문서 중 첫번째 문서와 다른 문서간의 코사인 유사도 측정.&#39;&#39;&#39; similarity_pair = cosine_similarity(feature_vect[hotel_indexes[0]] , feature_vect[hotel_indexes]) print(similarity_pair) . 호텔로 클러스터링 된 문서들의 DataFrame Index: Int64Index([1, 13, 14, 15, 20, 21, 24, 28, 30, 31, 32, 38, 39, 40, 45, 46], dtype=&#39;int64&#39;) ##### 비교 기준 문서명 bathroom_bestwestern_hotel_sfo 와 타 문서 유사도###### [[1. 0.0430688 0.05221059 0.06189595 0.05846178 0.06193118 0.03638665 0.11742762 0.38038865 0.32619948 0.51442299 0.11282857 0.13989623 0.1386783 0.09518068 0.07049362]] . import seaborn as sns import numpy as np import matplotlib.pyplot as plt %matplotlib inline # argsort()를 이용하여 앞예제의 첫번째 문서와 타 문서간 유사도가 큰 순으로 정렬한 인덱스 반환하되 자기 자신은 제외. sorted_index = similarity_pair.argsort()[:,::-1] sorted_index = sorted_index[:, 1:] print(sorted_index) # 유사도가 큰 순으로 hotel_indexes를 추출하여 재 정렬. print(hotel_indexes) hotel_sorted_indexes = hotel_indexes[sorted_index.reshape(-1,)] # 유사도가 큰 순으로 유사도 값을 재정렬하되 자기 자신은 제외 hotel_1_sim_value = np.sort(similarity_pair.reshape(-1,))[::-1] hotel_1_sim_value = hotel_1_sim_value[1:] # 유사도가 큰 순으로 정렬된 Index와 유사도값을 이용하여 파일명과 유사도값을 Seaborn 막대 그래프로 시각화 hotel_1_sim_df = pd.DataFrame() hotel_1_sim_df[&#39;filename&#39;] = document_df.iloc[hotel_sorted_indexes][&#39;filename&#39;] hotel_1_sim_df[&#39;similarity&#39;] = hotel_1_sim_value sns.barplot(x=&#39;similarity&#39;, y=&#39;filename&#39;,data=hotel_1_sim_df) plt.title(comparison_docname) . [[10 8 9 12 13 7 11 14 15 5 3 4 2 1 6]] Int64Index([1, 13, 14, 15, 20, 21, 24, 28, 30, 31, 32, 38, 39, 40, 45, 46], dtype=&#39;int64&#39;) . Text(0.5, 1.0, &#39;bathroom_bestwestern_hotel_sfo&#39;) .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/22/intro.html",
            "relUrl": "/2022/01/22/intro.html",
            "date": " • Jan 22, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "2022/01/21/FRI",
            "content": "&#53581;&#49828;&#53944; &#48516;&#47448; &#49892;&#49845; . 사이킷런이 내부에 가지고 있는 예제 데이터인 20 뉴스그룹 데이터 세트를 이용해 텍스트 분류를 적용해 보자. 텍스트 분류는 특정 문서의 분류를 학습 데이터를 통해 학습해 모델을 생성한 뒤 이 학습 모델을 이용해 다른 문서의 분류를 예측하는 것이다. 텍스트를 feature 벡터화로 변환하면 일반적으로 희소 행렬 형태가 된다. 그리고 이러한 희소 행렬에 분류를 효과적으로 잘 처리할 수 있는 알고리즘은 로지스틱 회구, 선형 서포트 벡터 머신, 나이브 베이즈 등이다. 이 중 로지스틱 회귀를 이용해 분류를 수행해보자. 텍스트를 기반으로 분류를 수행할 때는 먼저 텍스트를 정규화한 뒤 feature 벡터화를 적용한다. 그리고 그 이후에 적합한 머신러닝 알고리즘을 적용해 분류를 학습/예측/평가합니다. . 텍스트 정규화 | . from sklearn.datasets import fetch_20newsgroups news_data = fetch_20newsgroups(subset=&#39;all&#39;,random_state=156) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy _distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs: C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy .libs libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll C: Users ehfus Anaconda3 envs dv2021 lib site-packages numpy .libs libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll warnings.warn(&#34;loaded more than 1 DLL from .libs:&#34; . 어떠한 key 값을 가지고 있는지 확인해 보자. | . print(news_data.keys()) . dict_keys([&#39;data&#39;, &#39;filenames&#39;, &#39;target_names&#39;, &#39;target&#39;, &#39;DESCR&#39;]) . 다음으로 Target 클래스가 어떻게 구성돼 있는지 확인해 보자 | . import pandas as pd print(&#39;target 클래스의 값과 분포도 n&#39;,pd.Series(news_data.target).value_counts().sort_index()) print(&#39;target 클래스의 이름들 n&#39;,news_data.target_names) len(news_data.target_names), pd.Series(news_data.target).shape . target 클래스의 값과 분포도 0 799 1 973 2 985 3 982 4 963 5 988 6 975 7 990 8 996 9 994 10 999 11 991 12 984 13 990 14 987 15 997 16 910 17 940 18 775 19 628 dtype: int64 target 클래스의 이름들 [&#39;alt.atheism&#39;, &#39;comp.graphics&#39;, &#39;comp.os.ms-windows.misc&#39;, &#39;comp.sys.ibm.pc.hardware&#39;, &#39;comp.sys.mac.hardware&#39;, &#39;comp.windows.x&#39;, &#39;misc.forsale&#39;, &#39;rec.autos&#39;, &#39;rec.motorcycles&#39;, &#39;rec.sport.baseball&#39;, &#39;rec.sport.hockey&#39;, &#39;sci.crypt&#39;, &#39;sci.electronics&#39;, &#39;sci.med&#39;, &#39;sci.space&#39;, &#39;soc.religion.christian&#39;, &#39;talk.politics.guns&#39;, &#39;talk.politics.mideast&#39;, &#39;talk.politics.misc&#39;, &#39;talk.religion.misc&#39;] . (20, (18846,)) . 개별 데이터가 텍스트로 어떻게 구성돼 있는지 데이터를 한 개만 추출해 값을 확인해 보자 | . print(news_data.data[0]) . From: egreen@east.sun.com (Ed Green - Pixel Cruncher) Subject: Re: Observation re: helmets Organization: Sun Microsystems, RTP, NC Lines: 21 Distribution: world Reply-To: egreen@east.sun.com NNTP-Posting-Host: laser.east.sun.com In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes: &gt; &gt; The question for the day is re: passenger helmets, if you don&#39;t know for &gt;certain who&#39;s gonna ride with you (like say you meet them at a .... church &gt;meeting, yeah, that&#39;s the ticket)... What are some guidelines? Should I just &gt;pick up another shoei in my size to have a backup helmet (XL), or should I &gt;maybe get an inexpensive one of a smaller size to accomodate my likely &gt;passenger? If your primary concern is protecting the passenger in the event of a crash, have him or her fitted for a helmet that is their size. If your primary concern is complying with stupid helmet laws, carry a real big spare (you can put a big or small head in a big helmet, but not in a small one). Ed Green, former Ninjaite |I was drinking last night with a biker, Ed.Green@East.Sun.COM |and I showed him a picture of you. I said, DoD #0111 (919)460-8302 |&#34;Go on, get to know her, you&#39;ll like her!&#34; (The Grateful Dead) --&gt; |It seemed like the least I could do... . 텍스트 데이터를 확인해 보면 뉴스그룹 기사의 내용뿐만 아니라 뉴스그룹 제목, 작성자, 소속, 이메일 등의 다양한 정보를 가지고 있다. 이 중에서 내용을 제외하고 제목 등의 다른 정보는 제거하자. | remove 파라미터를 이용하면 뉴스그룹 기사의 헤더, 푸터 정보등을 제거할 수 있다. | 또한 fetch_20newsgroup()는 subset 파라미터를 이용해 학습 데이터 세트와 테스트 데이터 세트를 분리해 내려받을 수 있다. | . from sklearn.datasets import fetch_20newsgroups # subset=&#39;train&#39;으로 학습용(Train) 데이터만 추출, remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)로 내용만 추출 train_news= fetch_20newsgroups(subset=&#39;train&#39;, remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;), random_state=156) X_train = train_news.data y_train = train_news.target print(type(X_train)) # subset=&#39;test&#39;으로 테스트(Test) 데이터만 추출, remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)로 내용만 추출 test_news= fetch_20newsgroups(subset=&#39;test&#39;,remove=(&#39;headers&#39;, &#39;footers&#39;,&#39;quotes&#39;),random_state=156) X_test = test_news.data y_test = test_news.target print(&#39;학습 데이터 크기 {0} , 테스트 데이터 크기 {1}&#39;.format(len(train_news.data) , len(test_news.data))) . &lt;class &#39;list&#39;&gt; 학습 데이터 크기 11314 , 테스트 데이터 크기 7532 . feature 벡터화 변환과 머신러닝 모델 학습/예측/평가 학습 데이터는 11314개의 뉴스 그룹 문서가 리스트 형태로 주어지고, 테스트 데이터는 7532개의 문서가 역시 리스트 형태로 주어졌다. CountVectorizer를 이용해 학습 데이터의 텍스트를 feature 벡터화하겠다. 테스트 데이터 역시 feature 벡터화를 수행하는데 한 가지 반드시 유의해야할 것은 바로 테스트 데이터에서 CountVectorizer를 적용할 때는 반드시 학습 데이터를 이용해 fit()이 수행된 CountVertorizer 객체를 이용해 테스트 데이터를 변환해야 한다는 것이다. 그래야만 학습 시 설정된 CountVectorizer의 feature 개수와 테스트 데이터를 CountVectorizer로 변환할 feature 개수가 같아진다. 테스트 데이터의 feature 벡터화는 학습 데이터에 사용된 CountVectorizer 객체 변수인 cnt_vect.transform()을 이용해 변환한다. . | . from sklearn.feature_extraction.text import CountVectorizer # Count Vectorization으로 feature extraction 변환 수행. cnt_vect = CountVectorizer() cnt_vect.fit(X_train) X_train_cnt_vect = cnt_vect.transform(X_train) # 학습 데이터로 fit( )된 CountVectorizer를 이용하여 테스트 데이터를 feature extraction 변환 수행. X_test_cnt_vect = cnt_vect.transform(X_test) print(&#39;학습 데이터 Text의 CountVectorizer Shape:&#39;,X_train_cnt_vect.shape, X_test_cnt_vect.shape) . 학습 데이터 Text의 CountVectorizer Shape: (11314, 101631) (7532, 101631) . 학습 데이터를 CountVectorizer로 feature를 추출한 결과 11341개의 문서에서 feature 즉, 단어가 101631개로 만들어졌다. 이렇게 feature 벡터화된 데이터에 로지스틱 회귀를 적용해 뉴스 그룹에 대한 분류를 예측해보자 | . from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # LogisticRegression을 이용하여 학습/예측/평가 수행. lr_clf = LogisticRegression() lr_clf.fit(X_train_cnt_vect , y_train) pred = lr_clf.predict(X_test_cnt_vect) print(&#39;CountVectorized Logistic Regression 의 예측 정확도는 {0:.3f}&#39;.format(accuracy_score(y_test,pred))) . Count 기반으로 feature 벡터화가 적용된 데이터 세트에 대한 로지스틱 회귀의 예측 정확도는 0.617이다. | TF-IDF 기반으로 벡터화를 변경해 예측 모델을 수행해보자 | . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, roc_auc_score # TF-IDF Vectorization 적용하여 학습 데이터셋과 테스트 데이터 셋 변환. tfidf_vect = TfidfVectorizer() tfidf_vect.fit(X_train) X_train_tfidf_vect = tfidf_vect.transform(X_train) X_test_tfidf_vect = tfidf_vect.transform(X_test) # LogisticRegression을 이용하여 학습/예측/평가 수행. lr_clf = LogisticRegression() lr_clf.fit(X_train_tfidf_vect , y_train) pred = lr_clf.predict(X_test_tfidf_vect) print(&#39;TF-IDF Logistic Regression 의 예측 정확도는 {0:.3f}&#39;.format(accuracy_score(y_test ,pred))) . TF-IDF Logistic Regression 의 예측 정확도는 0.674 . TF-IDF가 단순 카운트 기반보다 훨씬 높은 예측 정확도를 제공한다. | 일반적으로 문서 내에 텍스트가 많고 많은 문서를 가지는 텍스틑 분석에서 카운트 벡터화보다는 TF-IDF 벡터화가 좋은 예측 결과를 도출한다. | 텍스트 분석에서 머신러닝 모델의 성능을 향상시키는 중요한 2가지 방법은 최적의 ML 알고리즘을 선택하는 것과 최상의 feature 전처리를 수행하는 것이다. | TF-IDF 벡터화에서 좀 더 다양한 파라미터를 적용해보자 | . # stop words 필터링을 추가하고 ngram을 기본(1,1)에서 (1,2)로 변경하여 Feature Vectorization 적용. tfidf_vect = TfidfVectorizer(stop_words=&#39;english&#39;, ngram_range=(1,2), max_df=300 ) tfidf_vect.fit(X_train) X_train_tfidf_vect = tfidf_vect.transform(X_train) X_test_tfidf_vect = tfidf_vect.transform(X_test) lr_clf = LogisticRegression() lr_clf.fit(X_train_tfidf_vect , y_train) pred = lr_clf.predict(X_test_tfidf_vect) print(&#39;TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}&#39;.format(accuracy_score(y_test ,pred))) . 이번에는 GridSearchCV를 이용해 로지스틱 회귀의 하이퍼 파라미터 최적화를 수행해보자. | 로지스틱 회귀의 C파라미터만 변경하면서 최적의 C값을 찾은 뒤 이 C값으로 학습된 모델에서 테스트 데이터로 예측해 성능을 평가해보자 | . from sklearn.model_selection import GridSearchCV # 최적 C 값 도출 튜닝 수행. CV는 3 Fold셋으로 설정. params = { &#39;C&#39;:[0.01, 0.1, 1, 5, 10]} grid_cv_lr = GridSearchCV(lr_clf ,param_grid=params , cv=3 , scoring=&#39;accuracy&#39; , verbose=1 ) grid_cv_lr.fit(X_train_tfidf_vect , y_train) print(&#39;Logistic Regression best C parameter :&#39;,grid_cv_lr.best_params_ ) # 최적 C 값으로 학습된 grid_cv로 예측 수행하고 정확도 평가. pred = grid_cv_lr.predict(X_test_tfidf_vect) print(&#39;TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}&#39;.format(accuracy_score(y_test ,pred))) . 로지스틱 회귀의 C가 10일 때 GridSearchCV의 교차 검증 테스트 세트에서 가장 좋은 예측 성능을 나타냈으며, 이를 테스트 데이터 세트에 적용해 약 0.703으로 이전보다 약간 향상된 성능 수치가 됐다. | . &#49324;&#51060;&#53431;&#47088; &#54028;&#51060;&#54532;&#46972;&#51064; &#49324;&#50857; &#48143; GridSearchCV&#50752;&#51032; &#44208;&#54633; . 사이킷런의 Pipeline 클래스를 이용하면 feature 벡터화와 ML 알고리즘 학습/예측을 위한 코드 작성을 한 번에 진행할 수 있다. 일반적으로 머신러닝에서 Pipeline이란 데이터의 가공, 변환 등의 전처리와 알고리즘 적용을 마치 &#39;수도관(Pipe)에서 물이 흐르듯&#39; 한꺼번에 스트림 기반으로 처리한다는 의미이다. 이렇게 Pipeline을 이용하면 데이터 전처리와 머신러닝 학습 과정을 통일된 API 기반에서 처리할 수 있어 더 직관적인 ML 모델 코드를 생성할 수 있다. 또한 대용량 데이터의 feature 벡터화 결과를 별도 데이터로 저장하지 않고 스트림 기반에서 바로 머신러닝 알고리즘의 데이터로 입력할 수 있기 때문에 수행시간을 절약할 수 있다. 일반적으로 사이킷런 파이프라인은 텍스트 기반의 feature 벡터화뿐만 아니라 모든 데이터 전처리 작업과 Estimator를 결합할 수 있다. 예를 들어 스케일링 또는 벡터 정규화, PCA 등의 변환 작업과 분류, 회귀 등의 Estimator를 한 번에 결합하는 것이다. . 다음은 위에서 텍스트 분류 예제 코드를 Pipeline을 이용해 다시 작성한 코드이다 Pipeline 객체는 다음과 같이 선언한다. . TfidfVectorizer 객체를 tfidf_vect라는 객체 변수 명으로, LogisticRegression 객체를 lr_clf라는 객체 변수 명으로 생성한 뒤 이 두 개의 객체를 파이프라인으로 연결하는 Pipeline 객체 pipeline을 생성한다는 의미이다. | 또한 다음 코드를 보면 기존 TfidfVectorizer의 학습 데이터와 테스트 데이터에 대한 fit()과 transform() 수행을 통한 feature 벡터화와 LogisticRegressor의 fit()과 predict() 수행을 통한 머신러닝 모델의 학습과 예측이 Pipeline의 fit()과 predict()로 통일돼 수행됨을 알 수 있다. 이렇게 Pipeline 방식을 적용하면 머신러닝 코드를 더 직관적이고 쉽게 작성할 수 있다. | . from sklearn.pipeline import Pipeline # TfidfVectorizer 객체를 tfidf_vect 객체명으로, LogisticRegression객체를 lr_clf 객체명으로 생성하는 Pipeline생성 pipeline = Pipeline([ (&#39;tfidf_vect&#39;, TfidfVectorizer(stop_words=&#39;english&#39;, ngram_range=(1,2), max_df=300)), (&#39;lr_clf&#39;, LogisticRegression(C=10)) ]) # 별도의 TfidfVectorizer객체의 fit_transform( )과 LogisticRegression의 fit(), predict( )가 필요 없음. # pipeline의 fit( ) 과 predict( ) 만으로 한꺼번에 Feature Vectorization과 ML 학습/예측이 가능. pipeline.fit(X_train, y_train) pred = pipeline.predict(X_test) print(&#39;Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}&#39;.format(accuracy_score(y_test ,pred))) . 사이킷런은 GridSearchCV 클래스의 생성 파라미터로 Pipeline을 입력해 Pipeline 기반에서도 하이퍼 파라미터 튜닝을 GridSearchCV 방식으로 진행할 수 있게 지원한다. 이렇게 하면 feature 벡터화를 위한 파라미터와 ML 알고리즘의 하이퍼 파라미터를 모두 한 번에 GridSearchCV를 이용해 최적화할 수 있다. | 다음 예제는 GridSearchCV에 Pipeline을 입력하면서 TfidfVectorizer의 파라미터와 LogisticRegressor의 하이퍼 파라미터를 함께 최적화한다. | GridSearchCV에 Estimator가 아닌 Pipeline을 입력할 경우에는 param_grid의 입력 값 설정이 기존과 약간 다르다. 딕셔너리 형태로 들어간다. | Pipeline+GridSearchCV를 적용할 때 유의할 점을 모두의 파라미터를 최적화하려면 너무 많은 튜닝 시간이 소모된다는 점이다. | 다음 코드는 수행시 너무 많은 시간이 소모되므로 Markdown 형식으로 남겨놓겠다. | . from sklearn.pipeline import Pipeline pipeline = Pipeline([ (&#39;tfidf_vect&#39;, TfidfVectorizer(stop_words=&#39;english&#39;)), (&#39;lr_clf&#39;, LogisticRegression()) ]) # Pipeline에 기술된 각각의 객체 변수에 언더바(_)2개를 연달아 붙여 GridSearchCV에 사용될 # 파라미터/하이퍼 파라미터 이름과 값을 설정. . params = { &#39;tfidf_vect__ngram_range&#39;: [(1,1), (1,2), (1,3)], &#39;tfidf_vect__max_df&#39;: [100, 300, 700], &#39;lr_clf__C&#39;: [1,5,10] } # GridSearchCV의 생성자에 Estimator가 아닌 Pipeline 객체 입력 grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3 , scoring=&#39;accuracy&#39;,verbose=1) grid_cv_pipe.fit(X_train , y_train) print(grid_cv_pipe.best_params_ , grid_cv_pipe.best_score_) pred = grid_cv_pipe.predict(X_test) print(&#39;Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}&#39;.format(accuracy_score(y_test ,pred))) . &#44048;&#49457; &#48516;&#49437; . 문서의 주고나적인 감성,의견,감정,기분 등을 파악하기 위한 방법으로 소셜 미디어, 여론 조사, 온라인 리뷰, 피드백 등 다양한 분야에서 활용되고 있다. 감성 분석은 문서 내 텍스트가 나타내는 여러 가지 주관적인 단어와 문맥을 기바으로 감성(Sentiment) 수치를 계산하는 방법을 이용한다. 이러한 감성 지수는 긍정 감성 지수와 부정 감성 지수로 구성되며 이들 지수를 합산해 긍정 감성 또는 부정 감성을 결정한다. 이러한 감성 분석은 머신러닝 관점에서 지도학습과 비지도학습 방식으로 나눌 수 있다. . 지도학습 :학습 데이터와 타킷 레이블 값을 기반으로 감성 분석 학습을 수행한 뒤 이를 기반으로 다른 데이터의 감성 분석을 예측하는 방법으로 일반적인 텍스트 기반의 분류와 거의 동일하다. 비지도학습 : &#39;Lexicon&#39;이라는 일종의 감성 어휘 사전을 이용한다. Lexicon은 감성 분석을 위한 용어와 문맥에 대한 다양한 정보를 가지고 있으며, 이를 이용해 문서의 긍정적, 부정적 감성 여부를 판단한다. . 지도학습 기반 감성 분석 실습 $ to$ IMDB 영화평 | 먼저 지도학습 기반으로 감성 분석을 수행하겠다. 유명한 IMBD의 영화 사이트의 영화평을 이용하겠다. (저자는 감성 분석이라는 타이틀이 붙었지만 지도학습 기반 감성 분석은 텍스트 기반의 이진 분류라고 표현하고 싶다고 했다.) | 영화평의 텍스트를 분석해 감성 분석 결과가 긍정 또는 부정인지를 예측하는 모델을 만들어보자. | . import pandas as pd review_df = pd.read_csv(&#39;./labeledTrainData.tsv&#39;, header=0, sep=&quot; t&quot;, quoting=3) review_df.head(3) . id sentiment review . 0 &quot;5814_8&quot; | 1 | &quot;With all this stuff going down at the moment ... | . 1 &quot;2381_9&quot; | 1 | &quot; &quot;The Classic War of the Worlds &quot; by Timothy ... | . 2 &quot;7759_3&quot; | 0 | &quot;The film starts with a manager (Nicholas Bell... | . Sentiment 칼럼의 1은 긍정적 평가, 0은 부정적 평가를 의미한다. | 이번에는 텍스트가 어떻게 구성돼 있는지 review 칼럼의 텍스트 값을 하나만 살펴보자 | . print(review_df[&#39;review&#39;][0]) . &#34;With all this stuff going down at the moment with MJ i&#39;ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ&#39;s feeling towards the press and also the obvious message of drugs are bad m&#39;kay.&lt;br /&gt;&lt;br /&gt;Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.&lt;br /&gt;&lt;br /&gt;The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci&#39;s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ&#39;s music.&lt;br /&gt;&lt;br /&gt;Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.&lt;br /&gt;&lt;br /&gt;Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ&#39;s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i&#39;ve gave this subject....hmmm well i don&#39;t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.&#34; . HTML 형식에서 추출해 태그가 여전히 존재한다. 이 문자열은 feature로 만들 필요가 없으니 삭제. | 판다스의 DataFrame/Series는 문자열 연산을 지원하기 위해 str 속성을 이용한다. DataFrame/Seires 객체에서 str을 적용하면 다양한 문자열 연산을 수행할 수 있다. | 태그를 공백으로 모두 바꿔주자 | 영어가 아닌 숫자와 특수문자 역시 Sentiment를 위한 feature로는 별 의미가 없어 보이므로 이들 모두 공란으로 변경하자. 이를 찾고 공란으로 변환하는 과정은 정규 표현식을 이용하겠다. | 정규 표현식은 텍스트를 처리하는 데 있어서 매우 유용하므로 익혀둘 필요가 있다. | . import re # &lt;br&gt; html 태그는 replace 함수로 공백으로 변환 review_df[&#39;review&#39;] = review_df[&#39;review&#39;].str.replace(&#39;&lt;br /&gt;&#39;,&#39; &#39;) # 파이썬의 정규 표현식 모듈인 re를 이용하여 영어 문자열이 아닌 문자는 모두 공백으로 변환 review_df[&#39;review&#39;] = review_df[&#39;review&#39;].apply( lambda x : re.sub(&quot;[^a-zA-Z]&quot;, &quot; &quot;, x) ) . 결정 값 클래스인 sentiment 칼럼을 별도로 추출해 결정 값 데이터 세트를 만들고, 원본 데이터 세트에서 id와 sentiment 칼럼을 삭제해 feature 데이터 세트를 생성한다. | 그리고 train_test_split()을 이용해 학습용과 테스트용 데이터 세트로 분리한다. | . from sklearn.model_selection import train_test_split class_df = review_df[&#39;sentiment&#39;] feature_df = review_df.drop([&#39;id&#39;,&#39;sentiment&#39;], axis=1, inplace=False) X_train, X_test, y_train, y_test= train_test_split(feature_df, class_df, test_size=0.3, random_state=156) X_train.shape, X_test.shape . ((17500, 1), (7500, 1)) . 이제 review 텍스트를 feature 벡터화한 후에 ML 분류 알고리즘을 적용해 예측 성능을 측정하자. | 앞 절에서 설명한 Pipeline 객체를 이용해 이 두 가지를 한꺼번에 수행해보자. | . from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, roc_auc_score . # 스톱 워드는 English, filtering, ngram은 (1,2)로 설정해 CountVectorization수행. # LogisticRegression의 C는 10으로 설정. pipeline = Pipeline([ (&#39;cnt_vect&#39;, CountVectorizer(stop_words=&#39;english&#39;, ngram_range=(1,2) )), (&#39;lr_clf&#39;, LogisticRegression(C=10))]) # Pipeline 객체를 이용하여 fit(), predict()로 학습/예측 수행. predict_proba()는 roc_auc때문에 수행. pipeline.fit(X_train[&#39;review&#39;], y_train) pred = pipeline.predict(X_test[&#39;review&#39;]) pred_probs = pipeline.predict_proba(X_test[&#39;review&#39;])[:,1] print(&#39;예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}&#39;.format(accuracy_score(y_test ,pred), roc_auc_score(y_test, pred_probs))) . 이번에는 TF-IDF 벡터화를 적용해 다시 예측 성능을 특정해 보자. | . # LogisticRegression의 C는 10으로 설정. pipeline = Pipeline([ (&#39;tfidf_vect&#39;, TfidfVectorizer(stop_words=&#39;english&#39;, ngram_range=(1,2) )), (&#39;lr_clf&#39;, LogisticRegression(C=10))]) pipeline.fit(X_train[&#39;review&#39;], y_train) pred = pipeline.predict(X_test[&#39;review&#39;]) pred_probs = pipeline.predict_proba(X_test[&#39;review&#39;])[:,1] print(&#39;예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}&#39;.format(accuracy_score(y_test ,pred), roc_auc_score(y_test, pred_probs))) . 예측 정확도는 0.8936, ROC-AUC는 0.9598 . TF-IDF 기반 feature 벡터화의 예측 성능이 조금 더 나아졌다. | . 비지도학습 기반 감성 분석 . Lexicon을 기반으로 이루어짐. 위에선 데이터 세트가 레이블 값을 가지고 있었지만 많은 감성 분석용 데이터는 이러한 결정된 레이블 값을 가지고 있지 않다. 이러한 경우에 Lexicon은 유용하게 사용된다. Lexicon은 긍정 감성 또는 부정 감성의 정도를 의미하는 수치를 가지고 있으며 이를 감성 지수라고 한다. 이 감성 지수는 단어의 위치나 주변 단어, 문맥, POS(Part of Speech)등을 참고해 결정된다. . import nltk . 502p 참고 | SentiWordNet과 VADER 감성 사전을 이용해 감성 분석을 수행한 뒤 예측 성능을 지도학습 기반의 분류와 비교해보자 | &#39;present&#39; 단어에 대한 Synset을 추출하자. | . from nltk.corpus import wordnet as wn term = &#39;present&#39; # &#39;present&#39;라는 단어로 wordnet의 synsets 생성. synsets = wn.synsets(term) print(&#39;synsets() 반환 type :&#39;, type(synsets)) print(&#39;synsets() 반환 값 갯수:&#39;, len(synsets)) print(&#39;synsets() 반환 값 :&#39;, synsets) . synsets() 반환 type : &lt;class &#39;list&#39;&gt; synsets() 반환 값 갯수: 18 synsets() 반환 값 : [Synset(&#39;present.n.01&#39;), Synset(&#39;present.n.02&#39;), Synset(&#39;present.n.03&#39;), Synset(&#39;show.v.01&#39;), Synset(&#39;present.v.02&#39;), Synset(&#39;stage.v.01&#39;), Synset(&#39;present.v.04&#39;), Synset(&#39;present.v.05&#39;), Synset(&#39;award.v.01&#39;), Synset(&#39;give.v.08&#39;), Synset(&#39;deliver.v.01&#39;), Synset(&#39;introduce.v.01&#39;), Synset(&#39;portray.v.04&#39;), Synset(&#39;confront.v.03&#39;), Synset(&#39;present.v.12&#39;), Synset(&#39;salute.v.06&#39;), Synset(&#39;present.a.01&#39;), Synset(&#39;present.a.02&#39;)] . 총 18개의 서로 다른 semantic(문맥상의)을 가지는 synset 객체가 반환됐다. | synset 객체가 가지는 여러 가지 속성을 살펴보면, POS(품사), 정의, 부명제 등으로 시맨틱적인 요소를 표현할 수 있다. | . for synset in synsets : print(&#39;##### Synset name : &#39;, synset.name(),&#39;#####&#39;) print(&#39;POS :&#39;,synset.lexname()) print(&#39;Definition:&#39;,synset.definition()) print(&#39;Lemmas:&#39;,synset.lemma_names()) . 이처럼 synset은 하나의 단어가 가질 수 있는 여러 가지 시맨틱 정보를 개별 클래스로 나타낸 것이다. | WordNet은 어떤 어휘와 다른 어휘 간의 관계를 유사도로 나타낼 수 있다. synset 객체는 단어 간의 유사도를 나타내기 위해서 path_similarity() 메서드를 제공한다. | 해당 메서드를 통해 &#39;tree&#39;,&#39;lion&#39;,&#39;tiger&#39;,&#39;cat&#39;,&#39;dog&#39;라는 단어의 상호 유사도를 살펴보자 | . tree = wn.synset(&#39;tree.n.01&#39;) lion = wn.synset(&#39;lion.n.01&#39;) tiger = wn.synset(&#39;tiger.n.02&#39;) cat = wn.synset(&#39;cat.n.01&#39;) dog = wn.synset(&#39;dog.n.01&#39;) entities = [tree , lion , tiger , cat , dog] similarities = [] entity_names = [ entity.name().split(&#39;.&#39;)[0] for entity in entities] # 단어별 synset 들을 iteration 하면서 다른 단어들의 synset과 유사도를 측정합니다. for entity in entities: similarity = [ round(entity.path_similarity(compared_entity), 2) for compared_entity in entities ] similarities.append(similarity) # 개별 단어별 synset과 다른 단어의 synset과의 유사도를 DataFrame형태로 저장합니다. similarity_df = pd.DataFrame(similarities , columns=entity_names,index=entity_names) similarity_df . tree lion tiger cat dog . tree 1.00 | 0.07 | 0.07 | 0.08 | 0.12 | . lion 0.07 | 1.00 | 0.33 | 0.25 | 0.17 | . tiger 0.07 | 0.33 | 1.00 | 0.25 | 0.17 | . cat 0.08 | 0.25 | 0.25 | 1.00 | 0.20 | . dog 0.12 | 0.17 | 0.17 | 0.20 | 1.00 | . SentiWordNet은 WordNet의 Synset과 유사한 Senti_Synset 클래스를 가지고 있다. SentiWordNet 모듈의 senti_synsets()는 WordNet 모듈이라서 synsets()와 비슷하게 Senti_Synset 클래스를 리스트 형태로 반환한다. | . import nltk from nltk.corpus import sentiwordnet as swn senti_synsets = list(swn.senti_synsets(&#39;slow&#39;)) print(&#39;senti_synsets() 반환 type :&#39;, type(senti_synsets)) print(&#39;senti_synsets() 반환 값 갯수:&#39;, len(senti_synsets)) print(&#39;senti_synsets() 반환 값 :&#39;, senti_synsets) . senti_synsets() 반환 type : &lt;class &#39;list&#39;&gt; senti_synsets() 반환 값 갯수: 11 senti_synsets() 반환 값 : [SentiSynset(&#39;decelerate.v.01&#39;), SentiSynset(&#39;slow.v.02&#39;), SentiSynset(&#39;slow.v.03&#39;), SentiSynset(&#39;slow.a.01&#39;), SentiSynset(&#39;slow.a.02&#39;), SentiSynset(&#39;dense.s.04&#39;), SentiSynset(&#39;slow.a.04&#39;), SentiSynset(&#39;boring.s.01&#39;), SentiSynset(&#39;dull.s.08&#39;), SentiSynset(&#39;slowly.r.01&#39;), SentiSynset(&#39;behind.r.03&#39;)] . SentiSynset 객체는 단어의 감성을 나타내는 감성 지수와 객관성을 나타내는 객관성 지수를 가지고 있다. 감성 지수는 다시 긍정 감성 지수와 부정 감성 지수로 나뉜다. 어떤 단어가 전혀 감성적이지 않으면 객관성 지수는 1이 되고 감성 지수는 모두 0이 된다. 다음은 father라는 단어와 fabulous라는 두 개의 단어의 감성 지수와 객관성 지수를 나타낸다. | . import nltk from nltk.corpus import sentiwordnet as swn father = swn.senti_synset(&#39;father.n.01&#39;) print(&#39;father 긍정감성 지수: &#39;, father.pos_score()) print(&#39;father 부정감성 지수: &#39;, father.neg_score()) print(&#39;father 객관성 지수: &#39;, father.obj_score()) print(&#39; n&#39;) fabulous = swn.senti_synset(&#39;fabulous.a.01&#39;) print(&#39;fabulous 긍정감성 지수: &#39;,fabulous .pos_score()) print(&#39;fabulous 부정감성 지수: &#39;,fabulous .neg_score()) . father 긍정감성 지수: 0.0 father 부정감성 지수: 0.0 father 객관성 지수: 1.0 fabulous 긍정감성 지수: 0.875 fabulous 부정감성 지수: 0.125 . father는 객관적인 단어로 객관성 지수가 1이고 긍정 감성/부정 감성 지수 모두 0이다. 반면에 fabulous는 감성 단어로서 긍정 감성 지수가 0.9에 육박한다. | 이제 IMBD 영화 감상평 감성 분석을 SentiWordNet Lexicon 기반으로 수행해 보자. | . 문서를 문장 단위로 분해 | 다시 문장을 단어 단위로 토큰화하고 품사 태깅 | 품사 태깅된 단어 기반으로 synset 객체와 senti_synset 객체를 생성 | Senti_synset에서 긍정 감성/부정 감성 지수를 구하고 이를 모두 합산해 특정 임계치 값 이상일 때 긍정 감성으로, 그렇지 않을 때는 부정 감성으로 결정 | SentiWordNet을 이용하기 위해서 WordNet을 이용해 문서를 다시 단어로 토큰화한 뒤 어근 추출과 품사 태깅을 적용한다. | 먼저 품사 태킹을 수행하는 내부 함수를 생성한다. | . from nltk.corpus import wordnet as wn # 간단한 NTLK PennTreebank Tag를 기반으로 WordNet기반의 품사 Tag로 변환 def penn_to_wn(tag): if tag.startswith(&#39;J&#39;): return wn.ADJ elif tag.startswith(&#39;N&#39;): return wn.NOUN elif tag.startswith(&#39;R&#39;): return wn.ADV elif tag.startswith(&#39;V&#39;): return wn.VERB return . 이제 문서를 문장 $ to$ 단어 토큰 $ to$ 품사 태깅 후에 SentiSynset 클래스를 생성하고 Polarity Score를 합산하는 함수를 생성한다. | 각 단어의 긍정 감성 지수와 부정 감성 지수를 모두 합한 총 감성 지수가 0 이상일 경우 긍정 감성, 그렇지 않을 경우 부정 감성으로 예측한다. | . from nltk.stem import WordNetLemmatizer from nltk.corpus import sentiwordnet as swn from nltk import sent_tokenize, word_tokenize, pos_tag def swn_polarity(text): # 감성 지수 초기화 sentiment = 0.0 tokens_count = 0 lemmatizer = WordNetLemmatizer() raw_sentences = sent_tokenize(text) # 분해된 문장별로 단어 토큰 -&gt; 품사 태깅 후에 SentiSynset 생성 -&gt; 감성 지수 합산 for raw_sentence in raw_sentences: # NTLK 기반의 품사 태깅 문장 추출 tagged_sentence = pos_tag(word_tokenize(raw_sentence)) for word , tag in tagged_sentence: # WordNet 기반 품사 태깅과 어근 추출 wn_tag = penn_to_wn(tag) if wn_tag not in (wn.NOUN , wn.ADJ, wn.ADV): continue lemma = lemmatizer.lemmatize(word, pos=wn_tag) if not lemma: continue # 어근을 추출한 단어와 WordNet 기반 품사 태깅을 입력해 Synset 객체를 생성. synsets = wn.synsets(lemma , pos=wn_tag) if not synsets: continue # sentiwordnet의 감성 단어 분석으로 감성 synset 추출 # 모든 단어에 대해 긍정 감성 지수는 +로 부정 감성 지수는 -로 합산해 감성 지수 계산. synset = synsets[0] swn_synset = swn.senti_synset(synset.name()) sentiment += (swn_synset.pos_score() - swn_synset.neg_score()) tokens_count += 1 if not tokens_count: return 0 # 총 score가 0 이상일 경우 긍정(Positive) 1, 그렇지 않을 경우 부정(Negative) 0 반환 if sentiment &gt;= 0 : return 1 return 0 . review_df[&#39;preds&#39;] = review_df[&#39;review&#39;].apply( lambda x : swn_polarity(x) ) y_target = review_df[&#39;sentiment&#39;].values preds = review_df[&#39;preds&#39;].values from sklearn.metrics import accuracy_score, confusion_matrix, precision_score from sklearn.metrics import recall_score, f1_score, roc_auc_score print(confusion_matrix( y_target, preds)) print(&quot;정확도:&quot;, accuracy_score(y_target , preds)) print(&quot;정밀도:&quot;, precision_score(y_target , preds)) print(&quot;재현율:&quot;, recall_score(y_target, preds)) . [[7668 4832] [3636 8864]] 정확도: 0.66128 정밀도: 0.647196261682243 재현율: 0.70912 . 전반적인 성능 평가 지표가 만족스러울 만한 수치는 아니다. | 이번에는 좀 더 성능 평가 지표가 잘 나올 것으로 예상되는 VADER를 이용해 감성 분석을 수행해보자. | . VADER는 소셜 미디어의 감성 분석 용도로 만들어진 룰 기반의 Lexicon이다. | 간략하게 IMDB의 감상평 한 개만 감성 분석을 수행해 결과를 살펴보자 | VADER의 경우 지속적으로 버전이 업데이트 되므로 설치한 VADER 버전에 따라 다음 결과는 상이할 수 있다. | . from nltk.sentiment.vader import SentimentIntensityAnalyzer senti_analyzer = SentimentIntensityAnalyzer() senti_scores = senti_analyzer.polarity_scores(review_df[&#39;review&#39;][0]) print(senti_scores) . {&#39;neg&#39;: 0.13, &#39;neu&#39;: 0.743, &#39;pos&#39;: 0.127, &#39;compound&#39;: -0.7943} . 이처럼 VADER를 이용하면 매우 쉽게 감성 분석을 수행할 수 있다. 먼저 위 셀의 두번 째 줄처럼 객체를 생성한 뒤 문서별로 polarity_scores() 메서드를 호출해 감성 점수를 구한 뒤, 해당 문서의 감성 점수가 특정 임계값 이상이면 긍정, 그렇지 않으면 부정으로 판단한다. | VADER를 이용해 IMDB의 감성 분석을 수행해보자. | . def vader_polarity(review,threshold=0.1): analyzer = SentimentIntensityAnalyzer() scores = analyzer.polarity_scores(review) # compound 값에 기반하여 threshold 입력값보다 크면 1, 그렇지 않으면 0을 반환 agg_score = scores[&#39;compound&#39;] final_sentiment = 1 if agg_score &gt;= threshold else 0 return final_sentiment # apply lambda 식을 이용하여 레코드별로 vader_polarity( )를 수행하고 결과를 &#39;vader_preds&#39;에 저장 review_df[&#39;vader_preds&#39;] = review_df[&#39;review&#39;].apply( lambda x : vader_polarity(x, 0.1) ) y_target = review_df[&#39;sentiment&#39;].values vader_preds = review_df[&#39;vader_preds&#39;].values print(&#39;#### VADER 예측 성능 평가 ####&#39;) from sklearn.metrics import accuracy_score, confusion_matrix, precision_score from sklearn.metrics import recall_score, f1_score, roc_auc_score print(confusion_matrix( y_target, vader_preds)) print(&quot;정확도:&quot;, accuracy_score(y_target , vader_preds)) print(&quot;정밀도:&quot;, precision_score(y_target , vader_preds)) print(&quot;재현율:&quot;, recall_score(y_target, vader_preds)) . #### VADER 예측 성능 평가 #### [[ 6747 5753] [ 1858 10642]] 정확도: 0.69556 정밀도: 0.6491003354681305 재현율: 0.85136 . 정확도가 SentiWordNet보다 향상됐고, 특히 재현율은 매우 크게 향상됐다. 이외에도 pattern이라는 감성 사전 패키지가 있으나 파이썬 3버전에서는 지원 X | 감성 사전을 이용한 감성 분석 예측 성능은 지도학습 분류 기반의 예측 성능에 비해 아직은 낮은 수준이지만 결정 클래스 값이 없는 상황을 고려한다면 예측 성능에 일정 수준 만족할 수 있을 것이다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/21/intro.html",
            "relUrl": "/2022/01/21/intro.html",
            "date": " • Jan 21, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "2022/01/20/THU",
            "content": "&#53581;&#49828;&#53944; &#49324;&#51204; &#51456;&#48708; &#51089;&#50629;(&#53581;&#49828;&#53944; &#51204;&#52376;&#47532;) - &#53581;&#49828;&#53944; &#51221;&#44508;&#54868; . 텍스트 자체를 feature로 만들 순 없다. 이를 위해 사전에 텍스트를 가공하는 준비 작업이 필요하다. 텍스트 정규화는 텍스트를 머신러닝 알고리즘이나 NLP 애플리케이션에 입력 데이터로 사용하기 위해 클렌징, 정제, 토큰화, 어근화 등의 다양한 텍스트 데이터의 사전 작업을 수행하는 것을 의미한다. 텍스트 분석은 이러한 텍스트 정규화 작업이 매우 중요하다. 텍스트 정규화 작업에는 클렌징, 토큰화, 필터링, 스톱 워드 제거, 철자 수정, Stemming, Lemmatization 등이 있다. . 클렌징 : 텍스트에서 분석에 오히려 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업니다. 예를 들어 HTML, XML 태그나 특정 기호 등을 사전에 제거한다. | 텍스트 토큰화 : 토큰화의 유형은 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화로 나눌 수 있다. NLTK는 이를 위해 다양한 API를 제공한다. 문장 토큰화 : 문장의 마침표(.), 개행문자( n), 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적이다. 또한 정규 표현식에 따른 문장 토큰화도 가능하다. | . | . 다음은 3개의 문장으로 이루어진 텍스트 문서를 문장으로 각각 분리하는 예제이다. | . from nltk import sent_tokenize import nltk nltk.download(&#39;punkt&#39;) text_sample = &#39;The Matrix is everywhere its all around us, here even in this room. You can see it out your window or on your television. You feel it when you go to work, or go to church or pay your taxes.&#39; sentences = sent_tokenize(text=text_sample) print(type(sentences),len(sentences)) print(sentences) . &lt;class &#39;list&#39;&gt; 3 [&#39;The Matrix is everywhere its all around us, here even in this room.&#39;, &#39;You can see it out your window or on your television.&#39;, &#39;You feel it when you go to work, or go to church or pay your taxes.&#39;] . [nltk_data] Downloading package punkt to [nltk_data] C: Users ehfus AppData Roaming nltk_data... [nltk_data] Package punkt is already up-to-date! . sent_tokenize()가 반환하는 것은 각각의 문장으로 구성된 list 객체이다. 반환된 list 객체가 3개의 문장으로 된 문자열을 가지고 있는 것을 알 수 있다. | . 단어 토큰화 : 문장을 단어로 토큰화하는 것이다. 기본적으로 공백, 콤마, 마침표, 개행문자( n) 등으로 단어를 분리하지만, 정규 표현식을 이용해 다양한 유형으로 토큰화를 수행할 수 있다. | 마침표나 개행문자와 같이 문장을 분리하는 구분자를 이용해 단어를 토큰화할 수 있으므로 Bag Of Word와 같이 단어의 순서가 중요하지 않은 경우 문장 토큰화를 사용하지 않고 단어 토큰화만 사용해도 충분하다. 일반적으로 문장 토큰화는 각 문장이 가지는 시맨틱(Ssemantic : 의미의, 의미론적인)적인 의미가 중요한 요소로 사용될 때 사용한다. | 단어 토큰화를 수행해보자 | . from nltk import word_tokenize sentence = &quot;The Matrix is everywhere its all around us, here even in this room.&quot; words = word_tokenize(sentence) print(type(words), len(words)) print(words) . &lt;class &#39;list&#39;&gt; 15 [&#39;The&#39;, &#39;Matrix&#39;, &#39;is&#39;, &#39;everywhere&#39;, &#39;its&#39;, &#39;all&#39;, &#39;around&#39;, &#39;us&#39;, &#39;,&#39;, &#39;here&#39;, &#39;even&#39;, &#39;in&#39;, &#39;this&#39;, &#39;room&#39;, &#39;.&#39;] . 이번에는 sent_tokenize와 word_tokenize를 조합해 문서에 대해서 모든 단어를 토큰화해 보자. 이전 예제에서 선언된 3개의 문장으로 된 text_sample을 문장별로 단어 토큰화를 적용한다. 이를 위해 먼저 문장으로 나누고, 개별 문장을 다시 단어로 토큰화하는 함수를 생성해보자 | . from nltk import word_tokenize, sent_tokenize #여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성 def tokenize_text(text): # 문장별로 분리 토큰 sentences = sent_tokenize(text) # 분리된 문장별 단어 토큰화 word_tokens = [word_tokenize(sentence) for sentence in sentences] return word_tokens #여러 문장들에 대해 문장별 단어 토큰화 수행. word_tokens = tokenize_text(text_sample) print(type(word_tokens),len(word_tokens)) print(word_tokens) . &lt;class &#39;list&#39;&gt; 3 [[&#39;The&#39;, &#39;Matrix&#39;, &#39;is&#39;, &#39;everywhere&#39;, &#39;its&#39;, &#39;all&#39;, &#39;around&#39;, &#39;us&#39;, &#39;,&#39;, &#39;here&#39;, &#39;even&#39;, &#39;in&#39;, &#39;this&#39;, &#39;room&#39;, &#39;.&#39;], [&#39;You&#39;, &#39;can&#39;, &#39;see&#39;, &#39;it&#39;, &#39;out&#39;, &#39;your&#39;, &#39;window&#39;, &#39;or&#39;, &#39;on&#39;, &#39;your&#39;, &#39;television&#39;, &#39;.&#39;], [&#39;You&#39;, &#39;feel&#39;, &#39;it&#39;, &#39;when&#39;, &#39;you&#39;, &#39;go&#39;, &#39;to&#39;, &#39;work&#39;, &#39;,&#39;, &#39;or&#39;, &#39;go&#39;, &#39;to&#39;, &#39;church&#39;, &#39;or&#39;, &#39;pay&#39;, &#39;your&#39;, &#39;taxes&#39;, &#39;.&#39;]] . 3개 문장은 문장별로 먼저 토큰화했기 때문에 word_tokens 변수는 3개의 리스트 객체를 내포하는 리스트이다. 그리고 내포된 이 3개의 개별 리스트 객체는 각각 문장별로 토큰화된 단어를 요소로 가지고 있다. | 문장을 단어별로 하나씩 토큰화 할 경우 문맥적인 의미는 무시될 수밖에 없다. 이러한 문제를 조금이라도 해결해 보고자 도입 된 것이 n-gram이다. n-gram은 연속된 n개의 단어를 하나의 토큰화 단위로 분리해 내는 것이다. n개 단어 크기 윈도우를 만들어 문장의 처음부터 오른쪽으로 움직이면서 토큰화를 수행한다. | 예를 들어 &quot;Agent Smith knocks the door&quot;를 2-gram(bigram)으로 만들면 (Agent,Smith),(knocks,the),(the,door)와 같이 연속적으로 2개의 단어들을 순차적으로 이동하면서 단어들을 토큰화 한다. | . from nltk import ngrams sentence = &quot;The Matrix is everywhere its all around us, here even in this room.&quot; words = word_tokenize(sentence) all_ngrams = ngrams(words, 2) ngrams = [ngram for ngram in all_ngrams] print(ngrams) . [(&#39;The&#39;, &#39;Matrix&#39;), (&#39;Matrix&#39;, &#39;is&#39;), (&#39;is&#39;, &#39;everywhere&#39;), (&#39;everywhere&#39;, &#39;its&#39;), (&#39;its&#39;, &#39;all&#39;), (&#39;all&#39;, &#39;around&#39;), (&#39;around&#39;, &#39;us&#39;), (&#39;us&#39;, &#39;,&#39;), (&#39;,&#39;, &#39;here&#39;), (&#39;here&#39;, &#39;even&#39;), (&#39;even&#39;, &#39;in&#39;), (&#39;in&#39;, &#39;this&#39;), (&#39;this&#39;, &#39;room&#39;), (&#39;room&#39;, &#39;.&#39;)] . 스톱 워드 제거 : 스톱 워드는 분석에 큰 의미가 없는 단어를 지칭한다. 가령 영어에서 is,the,a,will 등 문장을 구성하는 필수 문법요소이지만 문맥적으로 큰 의미가 없는 단어가 이에 해당된다. 이 단어의 경우 문법적인 특성으로 인해 특히 빈번하게 텍스트에 나타나므로 이것들을 사전에 제거하지 않으면 그 빈번함으로 인해 오히려 중요한 단어로 인지될 수 있다. 따라서 이 의미 없는 단어를 제거하는 것이 중요한 전처리 작업이다. 언어별로 이러한 스톱 워드가 목록화 돼 있다. 스톱 워드 목록을 내려받아보자 | . import nltk nltk.download(&#39;stopwords&#39;) . [nltk_data] Downloading package stopwords to [nltk_data] C: Users ehfus AppData Roaming nltk_data... [nltk_data] Package stopwords is already up-to-date! . True . 그 중 20개만 확인해 보자 | . print(&#39;영어 stop words 갯수:&#39;,len(nltk.corpus.stopwords.words(&#39;english&#39;))) print(nltk.corpus.stopwords.words(&#39;english&#39;)[:20]) . 영어 stop words 갯수: 179 [&#39;i&#39;, &#39;me&#39;, &#39;my&#39;, &#39;myself&#39;, &#39;we&#39;, &#39;our&#39;, &#39;ours&#39;, &#39;ourselves&#39;, &#39;you&#39;, &#34;you&#39;re&#34;, &#34;you&#39;ve&#34;, &#34;you&#39;ll&#34;, &#34;you&#39;d&#34;, &#39;your&#39;, &#39;yours&#39;, &#39;yourself&#39;, &#39;yourselves&#39;, &#39;he&#39;, &#39;him&#39;, &#39;his&#39;] . 위 예제에서 3개의 문장별로 단어를 토큰화해 생성된 word_tokens 리스트에 대해서 stopwords를 필터링으로 제거해 분석을 위한 의미 있는 단어만 추출해보자 | . import nltk stopwords = nltk.corpus.stopwords.words(&#39;english&#39;) all_tokens = [] # 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop for sentence in word_tokens: filtered_words=[] # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop for word in sentence: #소문자로 모두 변환합니다. word = word.lower() # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가 if word not in stopwords: filtered_words.append(word) all_tokens.append(filtered_words) print(all_tokens) . [[&#39;matrix&#39;, &#39;everywhere&#39;, &#39;around&#39;, &#39;us&#39;, &#39;,&#39;, &#39;even&#39;, &#39;room&#39;, &#39;.&#39;], [&#39;see&#39;, &#39;window&#39;, &#39;television&#39;, &#39;.&#39;], [&#39;feel&#39;, &#39;go&#39;, &#39;work&#39;, &#39;,&#39;, &#39;go&#39;, &#39;church&#39;, &#39;pay&#39;, &#39;taxes&#39;, &#39;.&#39;]] . is, this와 같은 스톱 워드가 필터링을 통해 제거됐음을 알 수 있다. | . Stemming과 Lemmatization : 영어의 경우 과거/현재, 3인칭 단수 여부, 진행형 등 매우 많은 조건에 따라 원래 단어가 변화한다. 이런 경우에 대해서 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 과정을 일컫는다. 두 기능 모두 원형 단어를 찾는다는 목적은 유사하지만, Lemmatization이 Stemming보다 정교하며 의미론적인 기반에서 단어의 원형을 찾는다. 따라서 시간이 더 소요된다. | . from nltk.stem import LancasterStemmer stemmer = LancasterStemmer() print(stemmer.stem(&#39;working&#39;),stemmer.stem(&#39;works&#39;),stemmer.stem(&#39;worked&#39;)) print(stemmer.stem(&#39;amusing&#39;),stemmer.stem(&#39;amuses&#39;),stemmer.stem(&#39;amused&#39;)) print(stemmer.stem(&#39;happier&#39;),stemmer.stem(&#39;happiest&#39;)) print(stemmer.stem(&#39;fancier&#39;),stemmer.stem(&#39;fanciest&#39;)) . work work work amus amus amus happy happiest fant fanciest . 비교적 간단한 형태인 work를 제외하고는 나머지 3가지 경우는 오류를 범하고 있음 | 이번에는 WordNetLemmatizer를 이용해 Lemmatization을 수행해보자. 일반적으로 Lemmatization은 보다 정확한 원형 단어 추출을 위해 단어의 &#39;품사&#39;를 입력해줘야 한다. 동사의 경우 v 형용사의 경우 a를 입력해준다. | . from nltk.stem import WordNetLemmatizer import nltk nltk.download(&#39;wordnet&#39;) lemma = WordNetLemmatizer() print(lemma.lemmatize(&#39;amusing&#39;,&#39;v&#39;),lemma.lemmatize(&#39;amuses&#39;,&#39;v&#39;),lemma.lemmatize(&#39;amused&#39;,&#39;v&#39;)) print(lemma.lemmatize(&#39;happier&#39;,&#39;a&#39;),lemma.lemmatize(&#39;happiest&#39;,&#39;a&#39;)) print(lemma.lemmatize(&#39;fancier&#39;,&#39;a&#39;),lemma.lemmatize(&#39;fanciest&#39;,&#39;a&#39;)) . Stemmer보다 정확하게 원형 단어를 추출해줌을 알 수 있다. | . Bag of Words - BOW : 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 feature 값을 추출하는 모델이다. 이 모델의 장점을 쉽고 빠른 구축에 있다. 단순히 단어의 발생 횟수에 기반하고 있지만, 예상보다 문서의 특징을 잘 나타낼 수 있는 모델이어서 전통적으로 여러 분야에서 활용도가 높다. 하지만 문맥 의미 반영 부족이라던가 희소 행렬 문제등의 단점 등이 있다. (희소 행렬 : 대규모의 칼럼으로 구성된 행렬에서 대부분의 값이 0으로 채워지는 행렬을 희소 행렬 (Spare Matrix)이라고 한다. 이와는 반대로 대부분의 값이 0이 아닌 의미 있는 값으로 채워져 있는 행렬을 밀집 행렬(Dense Matirix)이라고 한다) 일반적으로 희소 행렬은 ML 알고리즘의 수행 시간과 예측 성능을 떨어뜨리기 때문에 희소 행렬을 위한 특별한 기법이 마련돼 있다. | . 머신러닝 알고리즘은 일반적으로 숫자형 feature를 데이터로 입력받아 동작하기 때문에 텍스트와 같은 데이터는 머신러닝 알고리즘에 바로 입력할 수 없다. 따라서 텍스트는 특정 의미를 가지는 숫자형 값인 벡터 값으로 변환해야 하는데, 이러한 변환을 feature 벡터화라고 한다. 예를 들어 각 문서의 텍스트를 단어로 추출해 feature로 할당하고 각 단어의 발생 빈도와 같은 값을 이 feature에 값으로 부여해 각 문서를 이 단어 feature의 발생 빈도 값으로 구성된 벡터로 만드는 기법이다. | BOW 모델에서 feature 벡터화를 수행한다는 것은 모든 문서에서 모든 단어를 칼럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것이다. 예를 들어 M 개의 텍스트 문서가 있고, 이 문서에서 모든 단어를 추출해 나열했을 때 N개의 단어가 있다고 가정하면 문ㅅ머의 feature 벡터화를 수행하면 M개의 문서는 각각 N개의 값이 할당된 feature 벡터 세트가 된다. 결과적으로는 M x N 개의 단어 feature로 이뤄진 행렬을 구성하게 된다. | 일반적으로 BOW의 feature 벡터화는 두 가지 방식이 있다. 카운트(count) 기반의 벡터화 : 단어 feature에 값을 부여할 때 각 문서에서 해당 단어가 나타나는 횟수, 즉, Count를 부여하는 경우를 카운트 벡터화라고 한다. 카운트 벡터화에서는 카운트 값이 높을수록 중요한 단어로 인식된다. 하지만 언어의 특성상 문장에서 자주 사용될 수 밖에 없는 단어까지 높은 값을 부여하게 되는데 이러한 문제를 보완하기 위해 아래와 같은 벡터화를 사용하게 된다. | TF-IDF(Term Frequency - Inverse Document Frequency) 기반의 벡터화 : 개별 문서에서 자주 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 나타나는 단어에 대해서는 페널티를 주는 방식으로 값을 부여한다. | . | . 보통 사이킷런의 CountVectorizer 클래스를 이용해 카운트 기반의 feature 여러 개의 문서로 구성된 텍스트의 feature 벡터화 방법은 다음과 같다. | . 영어의 경우 모든 문자를 소문자로 변경하는 등의 전처리 작업을 수행한다. | default로 단어 기준으로 n_gram_range를 반영해 각 단어를 토큰화한다. | 텍스트 정규화를 수행한다. | BOW 벡터화를 위한 희소 행렬 모든 문서에 있는 단어를 추춯해 이를 feature로 벡터화하는 방법은 필연적으로 많은 feature 칼럼을 만들 수 밖에 없다. 모든 문서에 있는 단어를 중복을 제거하고 feature로 만들면 일반적으로 수만 개에서 수십만 개의 단어가 만들어진다. 만일 n-gram을 (1,2)나 (1,3)으로 증가시키면 칼럼의 수는 더욱 증가할 수 밖에 없다. 그런데 이러한 대규모의 행렬이 생성되더라도 레코드의 각 문서가 가지는 단어의 수는 제한적이기 때문에이 행렬의 값은 대부분이 0을 차지할 수밖에 없다. 이처럼 대규모 행렬의 대부분의 값을 0이 차지하는 행렬을 가리켜 희소 행렬이라고 한다. BOW 형태를 가진 언어 모델의 feature 벡터화는 대부분 희소 행렬이다. | 이 희소 행렬은 너무 많은 불필요한 0 값이 메모리 공간에 할당되어 메모리 공간이 많이 필요하며, 행렬의 크기가 커서 연산 시에도 데이터 액세스를 위한 시간이 많이 소모된다. 따라서 이러한 희소 행렬을 물리적으로 적은 메모리 공간을 차지할 수 있도록 변환해야 하는데 대표적인 방법으로 COO 형식와 CSR 형식이 있다. | 일반적으로 큰 희소 행렬을 저장하고 계산을 수행하는 능력이 CSR 형식이 더 뛰어나기 때문에 CSR을 많이 사용한다. 먼저 COO 방식부터 보자 | . | . 희소 행렬 처리 中 COO 형식 COO(coordinate : 좌표) 형식을 0이 아닌 데이터만 별도의 데이터 배열에 저장하고 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식이다. 예를 들어 [[3,0,1],[0,2,0]]과 같은 2차원 데이터가 있다고 가정하자. 0이 아닌 데이터는 [3,1,2]이며 0이 아닌 데이터가 있는 위치를 (row,col) 형태로 표시하면 (0,0),(0,2),(1,1)가 된다. 로우와 칼럼을 별도의 배열로 저장하면 로우는 [0,0,1]이고 칼럼은 [0,2,1]이다. 파이썬 세계에서는 희소 행렬 변환을 위해서 주로 Scipy를 이용한다. 사이파이의 sparse 패키지는 희소 행렬 변환을 위한 다양한 모듈을 제공한다. . | . import numpy as np dense = np.array( [ [ 3, 0, 1 ], [0, 2, 0 ] ] ) from scipy import sparse # 0 이 아닌 데이터 추출 data = np.array([3,1,2]) # 행 위치와 열 위치를 각각 array로 생성 row_pos = np.array([0,0,1]) col_pos = np.array([0,2,1]) # sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성 sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos))) print(type(sparse_coo)) print(sparse_coo) dense01=sparse_coo.toarray() print(type(dense01),&quot; n&quot;, dense01) . &lt;class &#39;scipy.sparse.coo.coo_matrix&#39;&gt; (0, 0) 3 (0, 2) 1 (1, 1) 2 &lt;class &#39;numpy.ndarray&#39;&gt; [[3 0 1] [0 2 0]] . 희소 행렬 : CSR(Compressed Sparse Row)형식은 COO 형식이 행과 열의 위치를 나타내기 위해서 반복적인 위치 데이터를 사용해야 하는 문제점을 해결한 방식이다. 자세한 내용은 택의 484p를 참고. | . from scipy import sparse dense2 = np.array([[0,0,1,0,0,5], [1,4,0,3,2,5], [0,6,0,3,0,0], [2,0,0,0,0,0], [0,0,0,7,0,8], [1,0,0,0,0,0]]) # 0 이 아닌 데이터 추출 data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1]) # 행 위치와 열 위치를 각각 array로 생성 row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5]) col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0]) # COO 형식으로 변환 sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos))) # 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성 row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13]) # CSR 형식으로 변환 sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind)) print(&#39;COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인&#39;) print(sparse_coo.toarray()) print(&#39;CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인&#39;) print(sparse_csr.toarray()) . COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인 [[0 0 1 0 0 5] [1 4 0 3 2 5] [0 6 0 3 0 0] [2 0 0 0 0 0] [0 0 0 7 0 8] [1 0 0 0 0 0]] CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인 [[0 0 1 0 0 5] [1 4 0 3 2 5] [0 6 0 3 0 0] [2 0 0 0 0 0] [0 0 0 7 0 8] [1 0 0 0 0 0]] . 실제 사용 시에는 다음과 같이 밀집 행렬을 생성 파라미터로 입력하면 COO나 CSR 희소 행렬로 생성한다. | . dense3 = np.array([[0,0,1,0,0,5], [1,4,0,3,2,5], [0,6,0,3,0,0], [2,0,0,0,0,0], [0,0,0,7,0,8], [1,0,0,0,0,0]]) coo = sparse.coo_matrix(dense3) csr = sparse.csr_matrix(dense3) .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/20/intro.html",
            "relUrl": "/2022/01/20/intro.html",
            "date": " • Jan 20, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "2022/01/19/WED",
            "content": "&#44400;&#51665;&#54868; &#49892;&#49845; . 고객 세그먼테이션(Customer Segmentation) :다양한 기준으로 고객을 분류하는 기법. 따라서 어떤 상품을 얼마나 많은 비용을 써서 얼마나 자주 사용하는가에 기반한 정보로 분류하는 것이 보통이다. 기업 입장에서는 얼마나 많은 매출을 발생하느냐가 고객 기준을 정하는 중요한 요소일 것이다. 고객 세그먼테이션의 주요 목표는 타깃 마케팅이다. 타깃 마케팅이란 고객을 여러 특성에 맞게 세분화해서 그 유형에 따라 맞춤형 마케팅이나 서비스를 제공하는 것이다. 기업의 마케팅은 고객의 상품 구매 이력에서 출발한다. 고객 세그먼테이션은 고객의 어떤 요소를 기반으로 군집화할 것인가를 결정하는 것이 중요한데 여기서는 기본적인 고객 분석 요소인 RFM 기법을 이용해보겠다. (RFM - Recency : 가장 최근 상품 구입 일에서 오늘까지의 기간, Frequency : 상품 구매 횟수, Monetary Value : 총 구매 금액) . 데이터 세트 로딩과 데이터 클렌징 | . import pandas as pd import datetime import math import numpy as np import matplotlib.pyplot as plt %matplotlib inline retail_df = pd.read_excel(io=&#39;Online Retail.xlsx&#39;) retail_df.head(3) . InvoiceNo StockCode Description Quantity InvoiceDate UnitPrice CustomerID Country . 0 536365 | 85123A | WHITE HANGING HEART T-LIGHT HOLDER | 6 | 2010-12-01 08:26:00 | 2.55 | 17850.0 | United Kingdom | . 1 536365 | 71053 | WHITE METAL LANTERN | 6 | 2010-12-01 08:26:00 | 3.39 | 17850.0 | United Kingdom | . 2 536365 | 84406B | CREAM CUPID HEARTS COAT HANGER | 8 | 2010-12-01 08:26:00 | 2.75 | 17850.0 | United Kingdom | . 이 데이터 세트는 제품 주문 데이터 세트이다. Invoice(주문번호) + StockCode(제품코드)를 기반으로 주문량, 주문 일자, 제품 단가, 주문 고객 번호, 주문 고객 국가 등의 칼럼으로 구성돼 있다. | . retail_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 541909 entries, 0 to 541908 Data columns (total 8 columns): # Column Non-Null Count Dtype -- -- 0 InvoiceNo 541909 non-null object 1 StockCode 541909 non-null object 2 Description 540455 non-null object 3 Quantity 541909 non-null int64 4 InvoiceDate 541909 non-null datetime64[ns] 5 UnitPrice 541909 non-null float64 6 CustomerID 406829 non-null float64 7 Country 541909 non-null object dtypes: datetime64[ns](1), float64(2), int64(1), object(4) memory usage: 33.1+ MB . CustomerID의 Null 값이 너무 많다. 그 외에 다른 칼럼의 경우도 오류 데이터가 일부 존재한다. | 따라서 이 데이터 세트는 먼저 사정 정제 작업이 필요하다. | CustomerID는 Null 데이터가 많은데 고객 세그먼테이션에 고객 식별 번호는 굳이 필요 없으므로 삭제 처리한다. | 대표적인 오류 데이터는 Quantity 또는 UnitPrice가 0보다 작은 경우이다. 사실 Quantity가 0보다 작은 경우는 오류 데이터라기보다는 반환을 뜻하는 값이다. 이 경우 InvoiceNo의 앞자리는 &#39;C&#39;로 돼 있다. 분석의 효율성을 위해서 이 데이터는 모두 삭제한다. | . retail_df = retail_df[retail_df[&#39;Quantity&#39;] &gt; 0] retail_df = retail_df[retail_df[&#39;UnitPrice&#39;] &gt; 0] retail_df = retail_df[retail_df[&#39;CustomerID&#39;].notnull()] print(retail_df.shape) retail_df.isnull().sum() . (397884, 8) . InvoiceNo 0 StockCode 0 Description 0 Quantity 0 InvoiceDate 0 UnitPrice 0 CustomerID 0 Country 0 dtype: int64 . 이제 Null 값은 칼럼에 존재하지 않는다. | Country 칼럼은 주문 고객 국가이며, 주요 고객 국가는 영국이다. 이 외에도 EU의 여러 나라와 영연방 국가들이 포함돼 있다. | . retail_df[&#39;Country&#39;].value_counts()[:5] . United Kingdom 354321 Germany 9040 France 8341 EIRE 7236 Spain 2484 Name: Country, dtype: int64 . 영국이 대다수를 차지하므로 다른 국가의 데이터는 모두 제외한다. | . retail_df = retail_df[retail_df[&#39;Country&#39;]==&#39;United Kingdom&#39;] print(retail_df.shape) . (354321, 8) . 이제 사전 정제된 데이터 기반으로 고객 세그먼테이션 군집화를 RFM 기반으로 수행해보자. 이를 위해 필요한 데이터를 가공해보자. | 먼저 &#39;UnitPrice&#39;와 &#39;Quantity&#39;를 곱해서 주문 금액 데이터를 만든다. 그리고 CustomerNo도 더 편리한 식별성을 위해 float 형을 int형으로 변경한다. | . retail_df[&#39;sale_amount&#39;] = retail_df[&#39;Quantity&#39;] * retail_df[&#39;UnitPrice&#39;] retail_df[&#39;CustomerID&#39;] = retail_df[&#39;CustomerID&#39;].astype(int) . 해당 온라인 판매 세트는 주문 횟수와 주문 금액이 압도적으로 특정 고객에게 많은 특성을 가지고 있다. 개인 고객의 주문과 소매점의 주문이 함께 포함돼 있기 때문이다. Top-5 주문 건수와 주문 금액을 가진 고객 데이터를 추출해보자 | . print(retail_df[&#39;CustomerID&#39;].value_counts().head(5)) print(retail_df.groupby(&#39;CustomerID&#39;)[&#39;sale_amount&#39;].sum().sort_values(ascending=False)[:5]) . 17841 7847 14096 5111 12748 4595 14606 2700 15311 2379 Name: CustomerID, dtype: int64 CustomerID 18102 259657.30 17450 194550.79 16446 168472.50 17511 91062.38 16029 81024.84 Name: sale_amount, dtype: float64 . 보다시피 몇몇 특정 고객이 많은 주문 건수와 주문 금액을 가지고 있다. | 주어진 온라인 판매 데이터 세트는 전형적인 판매 데이터 세트와 같이 주문번호 + 상품코드 레벨의 식별자로 돼 있다. | InvoiceNo + StockCode로 Group by를 수행하면 거의 1에 가깝게 유일한 식별자 레벨이 됨을 알 수 있다. | . retail_df.groupby([&#39;InvoiceNo&#39;,&#39;StockCode&#39;])[&#39;InvoiceNo&#39;].count().mean() . 1.028702077315023 . 그런데 지금 수행하려는 RFM기반의 고객 세그먼테이션은 고객 레벨로 주문 기간, 주문 횟수, 주문 금액 데이터를 기반으로 해 세그먼테이션을 수행하는 것이다. 이에 주문번호 + 상품코드 기준의 데이터를 고객 기준의 Recency, Frequency, Monetary value 데이터로 변경하자. 이를 위해서는 주문번호 기준의 데이터를 개별 고객 기준의 데이터로 Group by를 해야한다. | . # Recency는 InvoiceDate 컬럼의 max() 에서 데이터 가공 # Frequency는 InvoiceNo 컬럼의 count() , Monetary value는 sale_amount 컬럼의 sum() aggregations = { &#39;InvoiceDate&#39;: &#39;max&#39;, &#39;InvoiceNo&#39;: &#39;count&#39;, &#39;sale_amount&#39;:&#39;sum&#39; } cust_df = retail_df.groupby(&#39;CustomerID&#39;).agg(aggregations) # groupby된 결과 컬럼값을 Recency, Frequency, Monetary로 변경 cust_df = cust_df.rename(columns = {&#39;InvoiceDate&#39;:&#39;Recency&#39;, &#39;InvoiceNo&#39;:&#39;Frequency&#39;, &#39;sale_amount&#39;:&#39;Monetary&#39; } ) cust_df = cust_df.reset_index() cust_df.head(3) . CustomerID Recency Frequency Monetary . 0 12346 | 2011-01-18 10:01:00 | 1 | 77183.60 | . 1 12747 | 2011-12-07 14:34:00 | 103 | 4196.01 | . 2 12748 | 2011-12-09 12:20:00 | 4595 | 33719.73 | . Recency 칼럼은 개별 고객당 가장 최근의 주문인데, 데이터 값의 특성(DateTime)상 아직 데이터 가공이 추가로 필요하다. | 추가적 데이터 가공 원리는 459p 참고 | . import datetime as dt cust_df[&#39;Recency&#39;] = dt.datetime(2011,12,10) - cust_df[&#39;Recency&#39;] cust_df[&#39;Recency&#39;] = cust_df[&#39;Recency&#39;].apply(lambda x: x.days+1) print(&#39;cust_df 로우와 컬럼 건수는 &#39;,cust_df.shape) cust_df.head(3) . cust_df 로우와 컬럼 건수는 (3920, 4) . CustomerID Recency Frequency Monetary . 0 12346 | 326 | 1 | 77183.60 | . 1 12747 | 3 | 103 | 4196.01 | . 2 12748 | 1 | 4595 | 33719.73 | . 이제 고객별로 RFM 분석에 필요한 칼럼을 모두 생성했다. 다음으로는 생성된 RFM 기반 데이터 세트의 특성을 개괄적으로 알아보고 RFM 기반에서 고객 세그먼테이션을 수행해보자. | . 앞에서 언급했지만 온라인 판매 데이터 세트는 소매업체의 대규모 주문을 포함하고 있다. 이들은 주문 횟수와 주문 금액에서 개인 고객 주문과 매우 큰 차이를 나타내고 있으며, 이로 인해 매우 왜곡된 데이터 분포도를 가지게 되어 군집화가 한쪽 군집에만 집중되는 형상이 발생하게 된다. 먼저 온라인 판매 데이터 세트의 칼럼별 히스토그램을 확인하고 이처럼 왜곡된 데이터 분포도에서 군집화를 수행할 때 어떤 현상이 발생하는지 알아보자 . fig, (ax1,ax2,ax3) = plt.subplots(figsize=(12,4), nrows=1, ncols=3) ax1.set_title(&#39;Recency Histogram&#39;) ax1.hist(cust_df[&#39;Recency&#39;]) ax2.set_title(&#39;Frequency Histogram&#39;) ax2.hist(cust_df[&#39;Frequency&#39;]) ax3.set_title(&#39;Monetary Histogram&#39;) ax3.hist(cust_df[&#39;Monetary&#39;]) . (array([3.887e+03, 1.900e+01, 9.000e+00, 2.000e+00, 0.000e+00, 0.000e+00, 1.000e+00, 1.000e+00, 0.000e+00, 1.000e+00]), array([3.75000000e+00, 2.59691050e+04, 5.19344600e+04, 7.78998150e+04, 1.03865170e+05, 1.29830525e+05, 1.55795880e+05, 1.81761235e+05, 2.07726590e+05, 2.33691945e+05, 2.59657300e+05]), &lt;BarContainer object of 10 artists&gt;) . 세 칼럼 모두 왜곡된 데이터 값 분포도를 가지고 있으며 특히 Frequency,Monetary의 경우 특정 범위에 값이 몰려 있어서 왜곡 정도가 매우 심함을 알 수 있다. 각 칼럼의 데이터 값 백분위로 대략적으로 어떻게 값이 분포돼 있는지 확인해보자. | . cust_df[[&#39;Recency&#39;,&#39;Frequency&#39;,&#39;Monetary&#39;]].describe() . Recency Frequency Monetary . count 3920.000000 | 3920.000000 | 3920.000000 | . mean 92.742092 | 90.388010 | 1864.385601 | . std 99.533485 | 217.808385 | 7482.817477 | . min 1.000000 | 1.000000 | 3.750000 | . 25% 18.000000 | 17.000000 | 300.280000 | . 50% 51.000000 | 41.000000 | 652.280000 | . 75% 143.000000 | 99.250000 | 1576.585000 | . max 374.000000 | 7847.000000 | 259657.300000 | . 평균과 분위수를 비교하여 왜곡 정도를 비교해볼 수 있다. | 왜곡 정도가 매우 높은 데이터 세트에 K-평균 군집을 적용하면 중심의 개수를 증가시키더라도 변별력이 떨어지는 군집화가 수행된다. 먼저 데이터 세트를 StandardScaler로 평균과 표준편차를 재조정한 뒤에 K-평균을 수행해보자. | . from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score, silhouette_samples X_features = cust_df[[&#39;Recency&#39;,&#39;Frequency&#39;,&#39;Monetary&#39;]].values X_features_scaled = StandardScaler().fit_transform(X_features) kmeans = KMeans(n_clusters=3, random_state=0) labels = kmeans.fit_predict(X_features_scaled) cust_df[&#39;cluster_label&#39;] = labels print(&#39;실루엣 스코어는 : {0:.3f}&#39;.format(silhouette_score(X_features_scaled,labels))) . 실루엣 스코어는 : 0.592 . 군집을 3개로 구성할 경우 전체 군집의 평균 실루엣 계수인 실루엣 스코어는 0.592로 안정적인 수치가 나왔다. | 하지만 각 군집별 실루엣 계수 값도 살펴볼 필요가 있다. | . def visualize_silhouette(cluster_lists, X_features): from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score import matplotlib.pyplot as plt import matplotlib.cm as cm import math # 입력값으로 클러스터링 갯수들을 리스트로 받아서, 각 갯수별로 클러스터링을 적용하고 실루엣 개수를 구함 n_cols = len(cluster_lists) # plt.subplots()으로 리스트에 기재된 클러스터링 만큼의 sub figures를 가지는 axs 생성 fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols) # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 실루엣 개수 시각화 for ind, n_cluster in enumerate(cluster_lists): # KMeans 클러스터링 수행하고, 실루엣 스코어와 개별 데이터의 실루엣 값 계산. clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0) cluster_labels = clusterer.fit_predict(X_features) sil_avg = silhouette_score(X_features, cluster_labels) sil_values = silhouette_samples(X_features, cluster_labels) y_lower = 10 axs[ind].set_title(&#39;Number of Cluster : &#39;+ str(n_cluster)+&#39; n&#39; &#39;Silhouette Score :&#39; + str(round(sil_avg,3)) ) axs[ind].set_xlabel(&quot;The silhouette coefficient values&quot;) axs[ind].set_ylabel(&quot;Cluster label&quot;) axs[ind].set_xlim([-0.1, 1]) axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10]) axs[ind].set_yticks([]) # Clear the yaxis labels / ticks axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1]) # 클러스터링 갯수별로 fill_betweenx( )형태의 막대 그래프 표현. for i in range(n_cluster): ith_cluster_sil_values = sil_values[cluster_labels==i] ith_cluster_sil_values.sort() size_cluster_i = ith_cluster_sil_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i) / n_cluster) axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values, facecolor=color, edgecolor=color, alpha=0.7) axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) y_lower = y_upper + 10 axs[ind].axvline(x=sil_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;) ### 여러개의 클러스터링 갯수를 List로 입력 받아 각각의 클러스터링 결과를 시각화 def visualize_kmeans_plot_multi(cluster_lists, X_features): from sklearn.cluster import KMeans from sklearn.decomposition import PCA import pandas as pd import numpy as np # plt.subplots()으로 리스트에 기재된 클러스터링 만큼의 sub figures를 가지는 axs 생성 n_cols = len(cluster_lists) fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols) # 입력 데이터의 FEATURE가 여러개일 경우 2차원 데이터 시각화가 어려우므로 PCA 변환하여 2차원 시각화 pca = PCA(n_components=2) pca_transformed = pca.fit_transform(X_features) dataframe = pd.DataFrame(pca_transformed, columns=[&#39;PCA1&#39;,&#39;PCA2&#39;]) # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 KMeans 클러스터링 수행하고 시각화 for ind, n_cluster in enumerate(cluster_lists): # KMeans 클러스터링으로 클러스터링 결과를 dataframe에 저장. clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0) cluster_labels = clusterer.fit_predict(pca_transformed) dataframe[&#39;cluster&#39;]=cluster_labels unique_labels = np.unique(clusterer.labels_) markers=[&#39;o&#39;, &#39;s&#39;, &#39;^&#39;, &#39;x&#39;, &#39;*&#39;] # 클러스터링 결과값 별로 scatter plot 으로 시각화 for label in unique_labels: label_df = dataframe[dataframe[&#39;cluster&#39;]==label] if label == -1: cluster_legend = &#39;Noise&#39; else : cluster_legend = &#39;Cluster &#39;+str(label) axs[ind].scatter(x=label_df[&#39;PCA1&#39;], y=label_df[&#39;PCA2&#39;], s=70, edgecolor=&#39;k&#39;, marker=markers[label], label=cluster_legend) axs[ind].set_title(&#39;Number of Cluster : &#39;+ str(n_cluster)) axs[ind].legend(loc=&#39;upper right&#39;) plt.show() visualize_silhouette([2,3,4,5],X_features_scaled) visualize_kmeans_plot_multi([2,3,4,5],X_features_scaled) . C: Users ehfus AppData Local Temp/ipykernel_6896/46435488.py:88: UserWarning: You passed a edgecolor/edgecolors (&#39;k&#39;) for an unfilled marker (&#39;x&#39;). Matplotlib is ignoring the edgecolor in favor of the facecolor. This behavior may change in the future. axs[ind].scatter(x=label_df[&#39;PCA1&#39;], y=label_df[&#39;PCA2&#39;], s=70, C: Users ehfus AppData Local Temp/ipykernel_6896/46435488.py:88: UserWarning: You passed a edgecolor/edgecolors (&#39;k&#39;) for an unfilled marker (&#39;x&#39;). Matplotlib is ignoring the edgecolor in favor of the facecolor. This behavior may change in the future. axs[ind].scatter(x=label_df[&#39;PCA1&#39;], y=label_df[&#39;PCA2&#39;], s=70, . 군집이 2개일 경우 0번 군집과 1번 군집이 너무 개괄적으로 군집화 됐다. 군집 수를 증가시키면 개선이 가능할 것으로 예상 됐는데, 군집이 3개 이상일 때부터는 데이터 세트의 개수가 너무 작은 군집이 만들어진다. 이 군집에 속한 데이터는 개수가 작을뿐더러 실루엣 계수 역시 상대적으로 매우 작다. 또한 군집 내부에서도 데이터가 광범위하게 퍼져있다. 바로 앞에서 왜곡된 데이터 값인 특정 소매점의 대량 주문 구매 데이터이다. 이 데이터 세트의 경우 데이터 값이 거리 기반으로 광범위하게 퍼져 있어서 군집 수를 계쏙 늘려봐야 이 군집만 지속적으로 분리하게 되기에 의미 없는 군집화 결과로 이어지게 된다. | 이정도로 크게 왜곡된 데이터 세트의 도출은 굳이 군집화를 이용하지 않고도 간단한 데이터 분석만으로도 충분히 가능하다. 이처럼 지나치게 왜곡된 데이터 세트는 K-평균과 같은 거리 기반 군집화 알고리즘에서 지나치게 일반적인 군집화 결과를 도출하게 된다. | 비지도학습 알고리즘의 하나인 군집화의 기능적 의미는 숨어 있는 새로운 집단을 발견하는 것이다. 새로운 군집 내의 데이터 값을 분석하고 이해함으로써 이 집단에 새로운 의미를 부여할 수 있다. 이를 통해 전체 데이터를 다른 각도로 바라볼 수 있게 만들어준다. | 데이터 세트의 왜곡 정도를 낮추기 위해 가장 자주 사용되는 방법은 데이터 값에 log를 적용한 로그 변환이다. 온라인 판매 데이터 세트의 왜곡정도를 낮추기 위해 전체 데이터를 로그 변환한 뒤에 K-평균 알고리즘을 적용하고 결과를 비교해보자 | . from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score, silhouette_samples # Recency, Frequecny, Monetary 컬럼에 np.log1p() 로 Log Transformation cust_df[&#39;Recency_log&#39;] = np.log1p(cust_df[&#39;Recency&#39;]) cust_df[&#39;Frequency_log&#39;] = np.log1p(cust_df[&#39;Frequency&#39;]) cust_df[&#39;Monetary_log&#39;] = np.log1p(cust_df[&#39;Monetary&#39;]) # Log Transformation 데이터에 StandardScaler 적용 X_features = cust_df[[&#39;Recency_log&#39;,&#39;Frequency_log&#39;,&#39;Monetary_log&#39;]].values X_features_scaled = StandardScaler().fit_transform(X_features) kmeans = KMeans(n_clusters=3, random_state=0) labels = kmeans.fit_predict(X_features_scaled) cust_df[&#39;cluster_label&#39;] = labels print(&#39;실루엣 스코어는 : {0:.3f}&#39;.format(silhouette_score(X_features_scaled,labels))) . 실루엣 스코어는 : 0.303 . visualize_silhouette([2,3,4,5],X_features_scaled) visualize_kmeans_plot_multi([2,3,4,5],X_features_scaled) . C: Users ehfus AppData Local Temp/ipykernel_6896/46435488.py:88: UserWarning: You passed a edgecolor/edgecolors (&#39;k&#39;) for an unfilled marker (&#39;x&#39;). Matplotlib is ignoring the edgecolor in favor of the facecolor. This behavior may change in the future. axs[ind].scatter(x=label_df[&#39;PCA1&#39;], y=label_df[&#39;PCA2&#39;], s=70, C: Users ehfus AppData Local Temp/ipykernel_6896/46435488.py:88: UserWarning: You passed a edgecolor/edgecolors (&#39;k&#39;) for an unfilled marker (&#39;x&#39;). Matplotlib is ignoring the edgecolor in favor of the facecolor. This behavior may change in the future. axs[ind].scatter(x=label_df[&#39;PCA1&#39;], y=label_df[&#39;PCA2&#39;], s=70, . 실루엣 스코어는 로그 변환하기 전보다 떨어진다. 하지만 실루엣 스코어의 절대치가 중요한 것이 아니다. 어떻게 개별 군집이 더 균일하게 나뉠 수 있는지가 더 중요하다. | 이처럼 왜곡된 데이터 세트에 대해서는 로그 변환으로 데이터를 일차 변환한 후에 군집화를 수행하면 더 나은 결과를 도출할 수 있다. | . 정리 :각 군집화 기법은 나름의 장단점을 가지고 있으며, 군집화하려는 데이터의 특성에 맞게 선택해야 한다. . . &#53581;&#49828;&#53944; &#48516;&#49437; . 머신러닝이 보편화되면서 NLP(National Language Processing)와 텍스트 분석(Text Analytics,TA)을 구분하는 것이 큰 의미는 없지만 NLP는 머신이 인간의 언어를 이해하고 해석하는 데 더 중점을 두고 기술이 발전해 왔으며, 텍스트 마이닝이라고도 불리는 텍스트 분석은 비정형 텍스트에서 의미 있는 정보를 추출하는 것에 좀 더 중점을 두고 기술이 발전해왔다.NLP는 텍스트 분석을 향상하게 하는 기반 기술이라고 볼 수도 있다. NLP 기술이 발전함에 따라 텍스트 분석도 더욱 정교하게 발전할 수 있었다. NLP와 텍스트 분석의 발전 근간에는 머신러닝이 존재한다. 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용해 모델을 수립하고 정보를 추출해 비즈니스 인텔리전스나 예측 분석 등의 분석 작업을 주로 수행한다. . 텍스트 분류, 감성 분석, 텍스트 요약, 텍스트 군집화와 유사도 측정과 같은 기술 영역이 있다. . 텍스트 분석은 비정형 데이터인 텍스트를 분석하는 것이다. 지금까지 ML 모델은 주어진 정형 데이터 기반에서 모델을 수립하고 예측을 수행했다. 그리고 머신러닝 알고리즘은 숫자형의 feature 기반 데이터만 입력받을 수 있기 때문에 텍스트를 머신러닝에 적용하기 위해서는 비정형 텍스트 데이터를 어떻게 feature 형태로 추출하고 추출된 feature에 의미 있는 값을 부여하는가 하는 것이 매우 중요한 요소이다. . 텍스트를 word 기반의 다수의 feature로 추출하고 이 feature에 단어 빈도수와 같은 숫자 값을 부여하면 텍스트는 단어의 조합인 벡터값으로 표현될 수 있는데, 이렇게 텍스트를 변환하는 것을 feature vectorization 피처 벡터화 또는 feature extraction 피처 추출이라고 한다. . 대표적으로 feature vectorization 방법에는 BOW(Bag Of Words)와 Word2Vec 방법이 있다. 텍스트를 벡터값을 가지는 feature로 변환하는 것은 머신러닝 모델을 적용하기 전에 수행해야 할 매우 중요한 요소이다. . 텍스트 분석 수행 프로세스는 468p 참고 텍스트 사전 준비작업(텍스트 전처리) | feature vectorization/extraction | ML 모델 수립 및 학습/예측/평가 | | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/19/intro.html",
            "relUrl": "/2022/01/19/intro.html",
            "date": " • Jan 19, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "2022/01/18/TUE",
            "content": "GMM(Gaussian Mixture Model) . 군집화를 적용하고자 하는 데이터가 여러 개의 가우시안 분포를 가진 데이터 집합들이 섞여서 생성된 것이라는 가정하에 군집화를 수행하는 방식이다. 정규 분포로도 알려진 가우시안 분포는 좌우 대칭형의 종 형태를 가진 통계학에서 가장 잘 알려진 연속 확률 함수이다. 정규 분포는 평균을 중심으로 높은 데이터 분포도를 가지고 있으며 봐우 표준편차 1에 전체 데이터의 68.27%, 좌우 표준편차 2에 전체 데이터의 95.45%를 가지고 있다. 평균이 0이고 표준편차가 1인 정규 분포를 표준 정규 분포라고 한다. GMM은 데이터를 여러 개의 가우시안 분포가 섞인 것으로 간주한다. 섞인 데이터 분포에서 개별 유형의 가우시안 분포를 추출한다. . 전체 데이터 세트는 서로 다른 정규 분포 형태를 가진 여러 가지 확률 분포 곡선으로 구성될 수 있으며, 이러한 서로 다른 정규 분포에 기반해 군집화를 구행하는 것이 GMM 군집화 방식이다. 가령 1000개의 데이터 세트가 있다면 이를 구성하는 여러 개의 정규 분포 곡선을 추출하고, 개별 데이터가 이 중 어떤 정규 분포에 속하는지 결정하는 방식이다. 이와 같은 방식은 GMM에서는 모수 추정이라고 하는데 모수 추정은 대표적으로 2가지를 추정하는 것이다. . 개별 정규 분포의 평균과 분산 | 각 데이터가 어떤 정규 분포에 해당되는지의 확률 | 이러한 모수 추정을 위해 GMM은 EM(Expectation and Maximization)방법을 적용한다. . GMM을 이용한 붓꽃 데이터 세트 군집화 | GMM은 확률 기반 군집화이고 K-평균은 거리 기반 군집화이다. 이 둘을 비교해보자 | . from sklearn.datasets import load_iris from sklearn.cluster import KMeans import matplotlib.pyplot as plt import numpy as np import pandas as pd %matplotlib inline iris = load_iris() feature_names = [&#39;sepal_length&#39;,&#39;sepal_width&#39;,&#39;petal_length&#39;,&#39;petal_width&#39;] # 보다 편리한 데이타 Handling을 위해 DataFrame으로 변환 irisDF = pd.DataFrame(data=iris.data, columns=feature_names) irisDF[&#39;target&#39;] = iris.target . from sklearn.mixture import GaussianMixture gmm = GaussianMixture(n_components=3, random_state=0).fit(iris.data) gmm_cluster_labels = gmm.predict(iris.data) # 클러스터링 결과를 irisDF 의 &#39;gmm_cluster&#39; 컬럼명으로 저장 irisDF[&#39;gmm_cluster&#39;] = gmm_cluster_labels # target 값에 따라서 gmm_cluster 값이 어떻게 매핑되었는지 확인. iris_result = irisDF.groupby([&#39;target&#39;])[&#39;gmm_cluster&#39;].value_counts() print(iris_result) . target gmm_cluster 0 0 50 1 1 45 2 5 2 2 50 Name: gmm_cluster, dtype: int64 . K-평균 군집화 결과보다 더 효과적인 분류 결과가 도출됐다. | 붓꽃 데이터 세트의 K-평균 군집화를 수행한 결과를 보자. | . kmeans = KMeans(n_clusters=3, init=&#39;k-means++&#39;, max_iter=300,random_state=0).fit(iris.data) kmeans_cluster_labels = kmeans.predict(iris.data) irisDF[&#39;kmeans_cluster&#39;] = kmeans_cluster_labels iris_result = irisDF.groupby([&#39;target&#39;])[&#39;kmeans_cluster&#39;].value_counts() print(iris_result) . target kmeans_cluster 0 1 50 1 2 48 0 2 2 0 36 2 14 Name: kmeans_cluster, dtype: int64 . 이는 어떤 알고리즘이 더 뛰어나다는 의미가 아니라 붓꽃 데이터 세트가 GMM 군집화에 더 효과적이라는 의미이다. | K-평균은 평균 거리 중심으로 중심을 이동하면서 군집화를 수행하는 방식이므로 개별 군집 내의 데이터가 원형으로 흩어져 있는 경우에 매우 효과적으로 군집화가 수행될 수 있다. | K-평균은 원형의 범위에서 군집화를 수행한다. 데이터 세트가 원형의 범위를 가질수록 K-평균의 군집화 효율은 더욱 높아진다. | 다음은 make_blobs()의 군집의 수를 3개로 하되, cluster_std를 0.5로 설정해 군집 내의 데이터를 뭉치게 유도한 데이터 세트에 K-평균을 적용해보자 | 이렇게 cluster_std를 작게 설정하면 데이터가 원형 형태로 분산될 수 있다. | . def visualize_cluster_plot(clusterobj, dataframe, label_name, iscenter=True): if iscenter : centers = clusterobj.cluster_centers_ unique_labels = np.unique(dataframe[label_name].values) markers=[&#39;o&#39;, &#39;s&#39;, &#39;^&#39;, &#39;x&#39;, &#39;*&#39;] isNoise=False for label in unique_labels: label_cluster = dataframe[dataframe[label_name]==label] if label == -1: cluster_legend = &#39;Noise&#39; isNoise=True else : cluster_legend = &#39;Cluster &#39;+str(label) plt.scatter(x=label_cluster[&#39;ftr1&#39;], y=label_cluster[&#39;ftr2&#39;], s=70, edgecolor=&#39;k&#39;, marker=markers[label], label=cluster_legend) if iscenter: center_x_y = centers[label] plt.scatter(x=center_x_y[0], y=center_x_y[1], s=250, color=&#39;white&#39;, alpha=0.9, edgecolor=&#39;k&#39;, marker=markers[label]) plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color=&#39;k&#39;, edgecolor=&#39;k&#39;, marker=&#39;$%d$&#39; % label) if isNoise: legend_loc=&#39;upper center&#39; else: legend_loc=&#39;upper right&#39; plt.legend(loc=legend_loc) plt.show() . 하지만 데이터가 원형의 범위에서 벗어나 길쭉한 타원형으로 늘어선 경우같을 땐 군집화를 잘 수행하지 못한다. | GMM군집화와 K-Means군집화를 비교하기 위해 타원형으로 늘어선 임의의 데이터 세트를 생성 | . from sklearn.datasets import make_blobs # make_blobs() 로 300개의 데이터 셋, 3개의 cluster 셋, cluster_std=0.5 을 만듬. X, y = make_blobs(n_samples=300, n_features=2, centers=3, cluster_std=0.5, random_state=0) # 길게 늘어난 타원형의 데이터 셋을 생성하기 위해 변환함. transformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]] X_aniso = np.dot(X, transformation) # feature 데이터 셋과 make_blobs( ) 의 y 결과 값을 DataFrame으로 저장 clusterDF = pd.DataFrame(data=X_aniso, columns=[&#39;ftr1&#39;, &#39;ftr2&#39;]) clusterDF[&#39;target&#39;] = y # 생성된 데이터 셋을 target 별로 다른 marker 로 표시하여 시각화 함. visualize_cluster_plot(None, clusterDF, &#39;target&#39;, iscenter=False) . 위와 같이 만들어진 타원형의 길쭉한 데이터 세트에서는 K-평균의 군집화 정확성이 떨어지게 된다. | K-평균 군집화 방식이 위 데이터 세트를 어떻게 군집화하는지 확인해 보자. | . kmeans = KMeans(3, random_state=0) kmeans_label = kmeans.fit_predict(X_aniso) clusterDF[&#39;kmeans_label&#39;] = kmeans_label visualize_cluster_plot(kmeans, clusterDF, &#39;kmeans_label&#39;,iscenter=True) . K-평균으로 군집화를 수행할 경우, 주로 원형 영역 위치로 개별 군집화가 되면서 원하는 방향으로 구성되지 않음을 알 수 있다. K-평균이 평균 거리 기반으로 군집화를 수행하므로 같은 거리상 원형으로 군집을 구성하면서 위와 같이 길쭉한 방향으로 데이터가 밀집해 있을 경우에는 최적의 군집화가 어렵다. 이번에는 GMM으로 군집화를 수행해보자 | . gmm = GaussianMixture(n_components=3, random_state=0) gmm_label = gmm.fit(X_aniso).predict(X_aniso) clusterDF[&#39;gmm_label&#39;] = gmm_label # GaussianMixture는 cluster_centers_ 속성이 없으므로 iscenter를 False로 설정. visualize_cluster_plot(gmm, clusterDF, &#39;gmm_label&#39;,iscenter=False) . 데이터가 분포된 방향에 따라 정확하게 군집화됐음을 알 수 있다. | GMM은 K-평균과 다르게 군집의 중심 좌표를 구할 수 없다. | make_blobs()의 target 값과 KMeans,GMM의 군집 Label 값을 서로 비교해 효율 차이를 보자 | . print(&#39;### KMeans Clustering ###&#39;) print(clusterDF.groupby(&#39;target&#39;)[&#39;kmeans_label&#39;].value_counts()) print(&#39; n### Gaussian Mixture Clustering ###&#39;) print(clusterDF.groupby(&#39;target&#39;)[&#39;gmm_label&#39;].value_counts()) . ### KMeans Clustering ### target kmeans_label 0 2 73 0 27 1 1 100 2 0 86 2 14 Name: kmeans_label, dtype: int64 ### Gaussian Mixture Clustering ### target gmm_label 0 2 100 1 1 100 2 0 100 Name: gmm_label, dtype: int64 . 이처럼 GMM의 경우는 KMeans보다 유연하게 다양한 데이터 세트에 잘 적용될 수 있다는 장점이 있다. 하지만 군집화를 위한 수행시간이 오래 걸린다는 단점이 있다. | . DBSCAN . 밀도 기반 군집화의 대표적인 알고리즘이다. 데이터의 분포가 기하학적으로 복잡한 데이터 세트에도 효과적인 군집화가 가능하다. DBSCAN을 구성하는 가장 중요한 파라미터는 입실론으로 표기하는 주변 영역과 이 입실론 주변 영역에 포함되는 최소 데이터의 개수 min points이다. . 입실론 주변 영역(epsilon) :개별 데이터를 중심으로 입실론 반경을 가지는 원형의 영역 최소 데이터 개수(min points) : 개별 데이터의 입실론 주변 영역에 포함되는 타 데이터의 개수 . 입실론 주변 영역 내에 포함되는 최소 데이터 개수를 충족시키는가 아닌가에 따라 데이터 포인트를 다음과 같이 정의한다. | 핵심 포인트, 이웃 포인트, 경계 포인트, 잡음 포인트 | . DBSCAN의 자세한 과정에 대해선 443P를 참고하자. 이처럼 DBSCAN은 입실론 주변 영역의 최소 데이터 개수를 포함하는 밀도 기준을 충족시키는 데이터인 핵심 포인트를 연결하면서 군집화를 구성하는 방식이다. . 붓꽃 데이터 세트에 이를 적용해보자 | . from sklearn.datasets import load_iris import matplotlib.pyplot as plt import numpy as np import pandas as pd %matplotlib inline iris = load_iris() feature_names = [&#39;sepal_length&#39;,&#39;sepal_width&#39;,&#39;petal_length&#39;,&#39;petal_width&#39;] # 보다 편리한 데이타 Handling을 위해 DataFrame으로 변환 irisDF = pd.DataFrame(data=iris.data, columns=feature_names) irisDF[&#39;target&#39;] = iris.target irisDF.head() . sepal_length sepal_width petal_length petal_width target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . from sklearn.cluster import DBSCAN dbscan = DBSCAN(eps=0.6, min_samples=8, metric=&#39;euclidean&#39;) dbscan_labels = dbscan.fit_predict(iris.data) irisDF[&#39;dbscan_cluster&#39;] = dbscan_labels iris_result = irisDF.groupby([&#39;target&#39;])[&#39;dbscan_cluster&#39;].value_counts() print(iris_result) . target dbscan_cluster 0 0 49 -1 1 1 1 46 -1 4 2 1 42 -1 8 Name: dbscan_cluster, dtype: int64 . -1은 노이즈에 속하는 군집을 의미한다.따라서 위 붓꽃 데이터 세트는 DBSCAN에서 0과 1 두 개의 군집으로 군집화 됐음을 알 수 있다. Target 값의 유형이 3가지인데 군집이 2개가 됐다고 군집화 효율이 떨어지는 것은 아니다. DBSCAN은 군집의 개수를 알고리즘에 따라 자동으로 지정하므로 DBSCAN에서 군집의 개수를 지정하는 것은 무의미하다고 할 수 있다. 특히 붓꽃 데이터 세트에서는 군집을 3개로 하는 것보다는 2개로 하는 것이 군집화의 효율로서 더 좋은 면이 있다. | . from sklearn.decomposition import PCA # 2차원으로 시각화하기 위해 PCA n_componets=2로 피처 데이터 세트 변환 pca = PCA(n_components=2, random_state=0) pca_transformed = pca.fit_transform(iris.data) # visualize_cluster_2d( ) 함수는 ftr1, ftr2 컬럼을 좌표에 표현하므로 PCA 변환값을 해당 컬럼으로 생성 irisDF[&#39;ftr1&#39;] = pca_transformed[:,0] irisDF[&#39;ftr2&#39;] = pca_transformed[:,1] visualize_cluster_plot(dbscan, irisDF, &#39;dbscan_cluster&#39;, iscenter=False) . 별로 표현된 값은 모두 노이즈이다. PCA로 2차원으로 표현하면 이상치인 노이즈 데이터가 명확히 드러난다. DBSCAN을 적용할 때는 특정 군집 개수로 군집을 강제하지 않는 것이 좋다. eps와 min_samples 파라미터를 통해 최적의 군집을 찾는 게 중요하다. 일반적으로 eps의 값을 크게 하면 반경이 커져 포함하는 데이터가 많아지므로 노이즈 데이터 개수가 작아진다. min_samples를 크게 하면 주어진 반경 내에서 더 많은 데이터를 포함시켜야 하므로 노이즈 데이터 개수가 커지게 된다. 데이터 밀도가 더 커져야 하는데, 매우 촘촘한 데이터 분포가 아닌 경우 노이즈로 인식하기 때문이다. | . from sklearn.cluster import DBSCAN dbscan = DBSCAN(eps=0.8, min_samples=8, metric=&#39;euclidean&#39;) dbscan_labels = dbscan.fit_predict(iris.data) irisDF[&#39;dbscan_cluster&#39;] = dbscan_labels irisDF[&#39;target&#39;] = iris.target iris_result = irisDF.groupby([&#39;target&#39;])[&#39;dbscan_cluster&#39;].value_counts() print(iris_result) visualize_cluster_plot(dbscan, irisDF, &#39;dbscan_cluster&#39;, iscenter=False) . target dbscan_cluster 0 0 50 1 1 50 2 1 47 -1 3 Name: dbscan_cluster, dtype: int64 . dbscan = DBSCAN(eps=0.6, min_samples=16, metric=&#39;euclidean&#39;) dbscan_labels = dbscan.fit_predict(iris.data) irisDF[&#39;dbscan_cluster&#39;] = dbscan_labels irisDF[&#39;target&#39;] = iris.target iris_result = irisDF.groupby([&#39;target&#39;])[&#39;dbscan_cluster&#39;].value_counts() print(iris_result) visualize_cluster_plot(dbscan, irisDF, &#39;dbscan_cluster&#39;, iscenter=False) . target dbscan_cluster 0 0 48 -1 2 1 1 44 -1 6 2 1 36 -1 14 Name: dbscan_cluster, dtype: int64 . 이번에는 복합한 기하학적 분포를 가지는 데이터 세트에서 DBSCAN과 타 알고리즘을 비교해보자. | 먼저 make_circles() 함수를 이용해 내부 원과 외부 원 형태로 돼 있는 2차원 데이터 세트를 만들어보자 | . from sklearn.datasets import make_circles X, y = make_circles(n_samples=1000, shuffle=True, noise=0.05, random_state=0, factor=0.5) clusterDF = pd.DataFrame(data=X, columns=[&#39;ftr1&#39;, &#39;ftr2&#39;]) clusterDF[&#39;target&#39;] = y visualize_cluster_plot(None, clusterDF, &#39;target&#39;, iscenter=False) . 먼저 K-평균과 GMM이 어떻게 이 데이터 세트를 군집화하는지 확인해보자 | . from sklearn.cluster import KMeans kmeans = KMeans(n_clusters=2, max_iter=1000, random_state=0) kmeans_labels = kmeans.fit_predict(X) clusterDF[&#39;kmeans_cluster&#39;] = kmeans_labels visualize_cluster_plot(kmeans, clusterDF, &#39;kmeans_cluster&#39;, iscenter=True) . 거리 기반 군집화로는 위와 같이 데이터가 특정한 형태로 지속해서 이어지는 부분을 찾아내기 어렵다. | . from sklearn.mixture import GaussianMixture gmm = GaussianMixture(n_components=2, random_state=0) gmm_label = gmm.fit(X).predict(X) clusterDF[&#39;gmm_cluster&#39;] = gmm_label visualize_cluster_plot(gmm, clusterDF, &#39;gmm_cluster&#39;, iscenter=False) . from sklearn.cluster import DBSCAN dbscan = DBSCAN(eps=0.2, min_samples=10, metric=&#39;euclidean&#39;) dbscan_labels = dbscan.fit_predict(X) clusterDF[&#39;dbscan_cluster&#39;] = dbscan_labels visualize_cluster_plot(dbscan, clusterDF, &#39;dbscan_cluster&#39;, iscenter=False) . DBSCAN시에 정확한 예측이 가능함을 알 수 있다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/18/intro.html",
            "relUrl": "/2022/01/18/intro.html",
            "date": " • Jan 18, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "2022/01/17/MON",
            "content": "&#44400;&#51665; &#54217;&#44032; . 군집화는 분류와 유사해 보일 수 있으나 성격이 많이 다르다. 데이터 내에 숨어 있는 별도의 그룹을 찾아서 의미를 부여하거나 동일한 분류 값에 속하더라도 그 안에서 더 세분화된 군집화를 추구하거나 서로 다른 분류 값의 데이터도 더 넓은 군집화 레벨화 등의 영역을 가지고 있다. 군집화는 비지도 학습 특성상 어떠한 지표라도 정확하게 성능을 평가하기는 어렵다. 그럼에도 불구하고 군집화의 성능을 평가하는 대표적인 방법으로 실루엣 분석이 있다. . 실루엣 분석 : 각 군집 간의 거리가 얼마나 효율적으로 분리돼 있는지를 나타낸다. 효율적으로 잘 분리됐다는 것은 다른 군집과의 거리는 떨어져 있고 동일 군집기리의 데이터는 서로 가깝게 잘 뭉쳐 있다는 의미이다. 군집화가 잘 될수록 개별 군집은 비슷한 정도의 여유공간을 가지고 떨어져 있을 것이다. 실루엣 분석은 실루엣 계수를 기반으로 한다. 실루엣 계수는 개별 데이터가 각각이 가지는 군집화 지표이다. 개별 데이터가 가지는 실루엣 계수는 해당 데이터가 같은 군집 내의 데이터와 얼마나 가깝게 군집화돼 있고, 다른 군집에 있는 데이터와는 얼마나 멀리 분리돼 있는지를 나타내는 지표이다. | 특정 데이터 포인트의 실루엣 계수 값은 a(i),b(i)를 기반으로 계산된다. 두 군집 간의 거리가 얼마나 떨어져 있는가의 값은 b(i)-a(i)이며 이 값을 정규화하기 위해 MAX(a(i),b(i))값으로 나눈다. | 실루엣 계수는 -1에서 1 사이의 값을 가지며, 1로 가까워질수록 근처의 군집과 더 멀리 떨어져 있다는 것이고 0에 가까울수록 근처의 군집과 가까워진다는 것이다. - 값은 아예 다른 군집에 데이터 포인트가 할당됐음을 뜻한다. | . 좋은 군집화가 되기 위한 조건 . 전체 실루엣 계수의 평균값, 즉 사이킷런의 silhouette_score() 값은 0~1사이의 값을 가지며, 1에 가까울수록 좋다. | 전체 실루엣 계수의 평균값과 더불어 개별 군집의 평균값의 편차가 크진 않아야 한다. 즉, 개별 군집의 실루엣 계수 평균값이 전체 실루엣 계수 평균값에서 크게 벗어나지 않는 것이 중요하다. | . 앞의 붓꽃 데이터 세트의 군집화 결과를 실루엣 분석으로 평가해보자 . from sklearn.preprocessing import scale from sklearn.datasets import load_iris from sklearn.cluster import KMeans # 실루엣 분석 metric 값을 구하기 위한 API 추가 from sklearn.metrics import silhouette_samples, silhouette_score import matplotlib.pyplot as plt import numpy as np import pandas as pd %matplotlib inline iris = load_iris() feature_names = [&#39;sepal_length&#39;,&#39;sepal_width&#39;,&#39;petal_length&#39;,&#39;petal_width&#39;] irisDF = pd.DataFrame(data=iris.data, columns=feature_names) kmeans = KMeans(n_clusters=3, init=&#39;k-means++&#39;, max_iter=300,random_state=0).fit(irisDF) irisDF[&#39;cluster&#39;] = kmeans.labels_ # iris 의 모든 개별 데이터에 실루엣 계수값을 구함. score_samples = silhouette_samples(iris.data, irisDF[&#39;cluster&#39;]) print(&#39;silhouette_samples( ) return 값의 shape&#39; , score_samples.shape) # irisDF에 실루엣 계수 컬럼 추가 irisDF[&#39;silhouette_coeff&#39;] = score_samples # 모든 데이터의 평균 실루엣 계수값을 구함. average_score = silhouette_score(iris.data, irisDF[&#39;cluster&#39;]) print(&#39;붓꽃 데이터셋 Silhouette Analysis Score:{0:.3f}&#39;.format(average_score)) irisDF.head(3) . silhouette_samples( ) return 값의 shape (150,) 붓꽃 데이터셋 Silhouette Analysis Score:0.553 . sepal_length sepal_width petal_length petal_width cluster silhouette_coeff . 0 5.1 | 3.5 | 1.4 | 0.2 | 1 | 0.852955 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 1 | 0.815495 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 1 | 0.829315 | . 평균 실루엣 계수 값이 약 0.553이다. 1번 군집의 경우 0.8 이상의 높은 실루엣 계수 값을 나타내고 있다. 이는 1번 군집이 아닌 다른 군집의 경우 실루엣 계수 값이 평균 0.553 보다 낮기 때문일 것이다. | 군집별 평균 실루엣 계수 값을 알아보자 | . irisDF.groupby(&#39;cluster&#39;)[&#39;silhouette_coeff&#39;].mean() . cluster 0 0.451105 1 0.798140 2 0.417320 Name: silhouette_coeff, dtype: float64 . 1번 군집은 실루엣 계수 평균 값이 약 0.79인 반면, 다른 군집의 평균 실루엣 계수 값은 낮다. | . &#44400;&#51665;&#48324; &#54217;&#44512; &#49892;&#47336;&#50659; &#44228;&#49688;&#51032; &#49884;&#44033;&#54868;&#47484; &#53685;&#54620; &#44400;&#51665; &#44060;&#49688; &#52572;&#51201;&#54868; &#48169;&#48277; . 전체 데이터의 평균 실루엣 계수 값이 높다고 해서 반드시 최적의 군집 개수로 군집화가 잘 됐다고 볼 수 없다. 특정 군집 내의 실루엣 계수 값만 너무 높고, 다른 군집은 데이터끼리의 거리가 너무 떨어져 있어 실루엣 계수 값이 낮아져도 평균적으로 높은 값을 가질 수 있다. 개별 군집별로 적당히 분리된 거리를 유지하면서도 군집 내의 데이터가 서로 뭉쳐 있는 경우에 K-평균의 적절한 군집 개수가 설정됐다고 할 수 있다. . def visualize_silhouette(cluster_lists, X_features): from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from sklearn.metrics import silhouette_samples, silhouette_score import matplotlib.pyplot as plt import matplotlib.cm as cm import math # 입력값으로 클러스터링 갯수들을 리스트로 받아서, 각 갯수별로 클러스터링을 적용하고 실루엣 개수를 구함 n_cols = len(cluster_lists) # plt.subplots()으로 리스트에 기재된 클러스터링 수만큼의 sub figures를 가지는 axs 생성 fig, axs = plt.subplots(figsize=(4*n_cols, 4), nrows=1, ncols=n_cols) # 리스트에 기재된 클러스터링 갯수들을 차례로 iteration 수행하면서 실루엣 개수 시각화 for ind, n_cluster in enumerate(cluster_lists): # KMeans 클러스터링 수행하고, 실루엣 스코어와 개별 데이터의 실루엣 값 계산. clusterer = KMeans(n_clusters = n_cluster, max_iter=500, random_state=0) cluster_labels = clusterer.fit_predict(X_features) sil_avg = silhouette_score(X_features, cluster_labels) sil_values = silhouette_samples(X_features, cluster_labels) y_lower = 10 axs[ind].set_title(&#39;Number of Cluster : &#39;+ str(n_cluster)+&#39; n&#39; &#39;Silhouette Score :&#39; + str(round(sil_avg,3)) ) axs[ind].set_xlabel(&quot;The silhouette coefficient values&quot;) axs[ind].set_ylabel(&quot;Cluster label&quot;) axs[ind].set_xlim([-0.1, 1]) axs[ind].set_ylim([0, len(X_features) + (n_cluster + 1) * 10]) axs[ind].set_yticks([]) # Clear the yaxis labels / ticks axs[ind].set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1]) # 클러스터링 갯수별로 fill_betweenx( )형태의 막대 그래프 표현. for i in range(n_cluster): ith_cluster_sil_values = sil_values[cluster_labels==i] ith_cluster_sil_values.sort() size_cluster_i = ith_cluster_sil_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i) / n_cluster) axs[ind].fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_sil_values, facecolor=color, edgecolor=color, alpha=0.7) axs[ind].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) y_lower = y_upper + 10 axs[ind].axvline(x=sil_avg, color=&quot;red&quot;, linestyle=&quot;--&quot;) . from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=1, center_box=(-10.0, 10.0), shuffle=True, random_state=1) # cluster 개수를 2개, 3개, 4개, 5개 일때의 클러스터별 실루엣 계수 평균값을 시각화 visualize_silhouette([ 2, 3, 4, 5], X) . 위 그림에서 첫 번째 그림을 살펴보자. 해당 그림은 개별 군집에 속하는 데이터의 실루엣 계수를 2차원으로 나타낸 것이다. X축은 실루엣 계수 값이고, Y축은 개별 군집과 이에 속하는 데이터이다. 개별 군집은 Y축에 숫자값으로 0,1로 표시돼 있으며 점선으로 표시된 선은 전체 평균 실루엣 계수 값을 나타낸다. 이로 판단해 볼 때 해당 그림의 1번 군집의 모든 데이터는 평균 실루엣 계수 값 이상이지만, 2번 군집의 경우는 평균보다 적은 데이터 값이 매우 많다. 그에 반해 세 번째 그림을 살펴보자. 첫 번째 그림의 평균 실루엣 계수 값보다는 낮은 값이지만 개별 군집의 평균 실루엣 계수 값이 비교적 균일하게 위치하고 있다. 즉 군집이 2개인 경우보다는 평균 실루엣 계수 값이 작지만 4개인 경우가 가장 이상적인 군집화 개수로 판단할 수 있다. | . 이번에는 붓꽃 데이터를 이용해 K-평균 수행 시 최적의 군집 개수를 알아보자 | . from sklearn.datasets import load_iris iris=load_iris() visualize_silhouette([ 2, 3, 4,5 ], iris.data) . 붓꽃 데이터를 K-평균으로 군집화할 경우에는 군집 개수를 2개로 하는 것이 가장 좋아 보인다. | . 실루엣 계수를 통한 K-평균 군집 평가 방법은 직관적으로 이해하기 쉽지만, 각 데이터별로 다른 데이터와의 거리를 반복적으로 계산해야 하므로 데이터의 양이 늘어나면 수행 시간이 크게 늘어난다. 군집별로 임의의 데이터를 샘플링해 실루엣 계수를 평가하는 방안을 고민하여 대용량의 데이터에 대해 대응할 수 있어야 한다. . &#54217;&#44512; &#51060;&#46041; (Mean Shift) . K-평균과 유사하게 중심을 군집의 중심으로 지속적으로 움직이면서 군집화를 수행하지만 K-평균이 중심에 소속된 데이터의 평균 거리 중심으로 이동하는 데 반해, 평균 이동은 중심을 데이터가 모여 있는 밀도가 가장 높은 곳으로 이동시킨다. 평균 이동 군집화는 데이터의 분포도를 이용해 군집 중심점을 찾는다. 확률 밀도 함수를 이용하며 가장 집중적으로 데이터가 모여있어 확률 밀도 함수가 피크인 점을 군집 중심점으로 선정하며, 일반적으로 주어진 모델의 확률 밀도 함수를 찾기 위해서 KDE를 이용한다. 평균 이동 군집화는 특정 데이터를 반경 내의 데이터 분포 확률 밀도가 가장 높은 곳으로 이동하기 위해 주변 데이터와의 거리 값을 KDE 함수 값으로 입력한 뒤 그 반환 값을 현재 위치에서 업데이트하면서 이동하는 방식을 취한다. 이러한 방식을 전체 데이터에 반복적으로 적용하면서 데이터의 군집 중심점을 찾아낸다. . KDE는 커널 함수를 통해 어떤 변수의 확률 밀도 함수를 측정하는 대표적인 방법이다. 관측된 데이터 각각에 커널 함수를 적용한 값을 모두 더한 뒤 데이터 건수로 나눠 확률 밀도 함수를 추정한다. 확률 밀도 함수는 확률 변수의 분포를 나타내는 함수로 널리 알려진 정규 분포 함수를 포함해 감마 분포, t-분포 등이 있다. 확률 밀도 함수를 알면 특정 변수가 어떤 값을 갖게 될지에 대한 확률을 알게 되므로 이를 통해 변수의 특성, 확률 분포 등 변수의 많은 요소를 알 수 있다. 대표적인 커널 함수로서 가우시안 분포 함수가 사용된다. . KDE에서 대역폭은 KDE의 형태를 부드럽게 하거나 평활화하는 데 적용되며, 이 대역폭을 어떻게 설정하느냐에 따라 확률 밀도 추정 성능을 크게 좌우할 수 있다. 작은 대역폭 값은 좁고 뾰족한 KDE를 가지게 되며, 이는 변동성이 큰 방식으로 확률 밀도 함수를 추정하므로 과적합하기 쉽다. 따라서 적절한 KDE의 대역폭을 계산하는 것인 KDE 기반의 평균 이동 군집화에서 매우 중요하다. . 일반적으로 평균 이동 군집화는 대역폭이 클수록 평활화된 KDE로 인해 적은 수의 군집 중심점을 가지며 대역폭이 작을 수록 많은 수의 군집 중심점을 갖는다. 또한 평균 이동 군집화는 군집의 개수를 지정하지 않으며 오직 대역폭의 크기에 따라 군집화를 수행한다. 대역폭 크기 설정이 군집화의 품질에 큰 영향을 미치기 때문에 사이킷런은 최적의 대역폭 계산을 위해 estimate_bandwidth() 함수를 제공한다. . import numpy as np from sklearn.datasets import make_blobs from sklearn.cluster import MeanShift X, y = make_blobs(n_samples=200, n_features=2, centers=3, cluster_std=0.7, random_state=0) meanshift= MeanShift(bandwidth=0.8) cluster_labels = meanshift.fit_predict(X) print(&#39;cluster labels 유형:&#39;, np.unique(cluster_labels)) . cluster labels 유형: [0 1 2 3 4 5] . 군집이 6개로 분류됐다. 지나치게 세분화돼 군집화됐다. 일반적으로 bandwidth 값을 작게 할수록 군집 개수가 많아진다. 이번에 bandwidth를 살짝 높인 1로 해서 수행해보자 | . meanshift= MeanShift(bandwidth=1) cluster_labels = meanshift.fit_predict(X) print(&#39;cluster labels 유형:&#39;, np.unique(cluster_labels)) . cluster labels 유형: [0 1 2] . 군집화가 적절히 잘 이루어진 것으로 보인다. | 사이킷런은 최적화된 bandwidth 값을 찾기 위해서 estimate_bandwidth() 함수를 제공한다. | . from sklearn.cluster import estimate_bandwidth bandwidth = estimate_bandwidth(X) print(&#39;bandwidth 값:&#39;, round(bandwidth,3)) . bandwidth 값: 1.816 . import pandas as pd clusterDF = pd.DataFrame(data=X, columns=[&#39;ftr1&#39;, &#39;ftr2&#39;]) clusterDF[&#39;target&#39;] = y # estimate_bandwidth()로 최적의 bandwidth 계산 best_bandwidth = estimate_bandwidth(X, quantile=0.25) meanshift= MeanShift(best_bandwidth) cluster_labels = meanshift.fit_predict(X) print(&#39;cluster labels 유형:&#39;,np.unique(cluster_labels)) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages sklearn utils validation.py:67: FutureWarning: Pass bandwidth=1.5182908825307768 as keyword args. From version 0.25 passing these as positional arguments will result in an error warnings.warn(&#34;Pass {} as keyword args. From version 0.25 &#34; . cluster labels 유형: [0 1 2] . 3개의 군집으로 구성됨을 알 수 있다. | 구성된 3개의 군집을 시각화해보자 | . import matplotlib.pyplot as plt %matplotlib inline clusterDF[&#39;meanshift_label&#39;] = cluster_labels centers = meanshift.cluster_centers_ unique_labels = np.unique(cluster_labels) markers=[&#39;o&#39;, &#39;s&#39;, &#39;^&#39;, &#39;x&#39;, &#39;*&#39;] for label in unique_labels: label_cluster = clusterDF[clusterDF[&#39;meanshift_label&#39;]==label] center_x_y = centers[label] # 군집별로 다른 marker로 scatter plot 적용 plt.scatter(x=label_cluster[&#39;ftr1&#39;], y=label_cluster[&#39;ftr2&#39;], edgecolor=&#39;k&#39;, marker=markers[label] ) # 군집별 중심 시각화 plt.scatter(x=center_x_y[0], y=center_x_y[1], s=200, color=&#39;white&#39;, edgecolor=&#39;k&#39;, alpha=0.9, marker=markers[label]) plt.scatter(x=center_x_y[0], y=center_x_y[1], s=70, color=&#39;k&#39;, edgecolor=&#39;k&#39;, marker=&#39;$%d$&#39; % label) plt.show() . target값과 군집 label값을 비교해보자. | 1:1로 잘 매칭됐음을 알 수 있다. | . print(clusterDF.groupby(&#39;target&#39;)[&#39;meanshift_label&#39;].value_counts()) . target meanshift_label 0 2 67 1 0 67 2 1 66 Name: meanshift_label, dtype: int64 . 평균 이동의 장점은 데이터 세트의 형태를 특정 형태로 가정한다든가, 특정 분포도 기반의 모델로 가정하지 않기 때문에 좀 더 유연한 군집화가 가능한 것이다. 또한 이상치의 영향력도 크지 않으며, 미리 군집의 개수를 정할 필요도 없다. 하지만 알고리즘의 수행시간이 오래 걸리고 무엇보다도 band-width의 크기에 따른 군집화 영향도가 매우 크다. | 이 같은 특징 때문에 일반적으로 평균 이동 군집화 기법은 분석 업무 기반의 데이터 세트보다는 컴퓨터 비전 영역에서 더 많이 사용된다. 이미지나 영상 데이터에서 특정 개체를 구분하거나 움직임을 추적하는 데 뛰어난 역할을 수행하는 알고리즘이다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/17/intro.html",
            "relUrl": "/2022/01/17/intro.html",
            "date": " • Jan 17, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "2022/01/16/SUN",
            "content": "SVD . PCA와 유사한 행렬 분해 기법을 이용한다. SVD는 정방행렬뿐만 아니라 행과 열의 크기가 다른 행렬에도 적용할 수 있다. SVD는 특이값 분해로서 행렬 U와 V에 속한 벡터는 특이벡터 모든 특이벡터는 서로 직교하는 성질을 갖는다. 시그마는 대각행렬이며 시그마가 위치한 0이 아닌 값이 바로 행렬 A의 특이값이다. 일반적으로는 시그마의 비대각인 부분과 대각원소 중에 특이값이 0인 부분도 모두 제거하고 제거된 시그마에 대응되는 U와 V원소도 함께 제거해 차원을 줄인 형태로 SVD를 적용한다. SVD를 적용하면 A읭 차원이 MxN일때 U의 차원을 M x P, 시그마의 차원을 P x P 행렬 V의 차원을 P x N으로 분해한다. . Truncated SVD . 시그마의 대각원소 중에 상위 몇 개만 추출해서 여기에 대응되는 U와 V의 원소도 함께 제거해 더욱 차원을 줄인 형태로 분해하는 것이다. . import numpy as np from numpy.linalg import svd # 4X4 Random 행렬 a 생성 # 행렬의 개별 로우끼리의 의존성을 없애기 위해 랜덤행렬을 생선한다. np.random.seed(121) a = np.random.randn(4,4) print(np.round(a, 3)) . [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.014 0.63 1.71 -1.327] [ 0.402 -0.191 1.404 -1.969]] . 이렇게 생성된 a행렬에 SVD를 적용해 U,Sigma,Vt를 도출해보자. 시그마 행렬의 경우 0이 아닌 값의 경우만 1차원 행렬로 표현한다. | . U, Sigma, Vt = svd(a) print(U.shape, Sigma.shape, Vt.shape) print(&#39;U matrix: n&#39;,np.round(U, 3)) print(&#39;Sigma Value: n&#39;,np.round(Sigma, 3)) print(&#39;V transpose matrix: n&#39;,np.round(Vt, 3)) . (4, 4) (4,) (4, 4) U matrix: [[-0.079 -0.318 0.867 0.376] [ 0.383 0.787 0.12 0.469] [ 0.656 0.022 0.357 -0.664] [ 0.645 -0.529 -0.328 0.444]] Sigma Value: [3.423 2.023 0.463 0.079] V transpose matrix: [[ 0.041 0.224 0.786 -0.574] [-0.2 0.562 0.37 0.712] [-0.778 0.395 -0.333 -0.357] [-0.593 -0.692 0.366 0.189]] . 분해된 이 U,Sigma,Vt를 이용해 다시 원본 행렬로 정확히 복원되는지 확인해보자. U,Sigma,Vt를 내적하면 된다. 한 가지 유희할 것은 Sigma의 경우 0이 아닌 값만 1차원으로 추출했었으므로 다시 0을 포함한 대칭행렬로 변환한 뒤에 내적을 수행해야 한다는 점이다. | . Sigma_mat = np.diag(Sigma) a_ = np.dot(np.dot(U, Sigma_mat), Vt) print(np.round(a_, 3)) . [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.014 0.63 1.71 -1.327] [ 0.402 -0.191 1.404 -1.969]] . 지금까진 로우간 의존성이 없는 경우였으며 로우간 의존성이 있을 경우에 대해 살펴보자 | 의존성을 부여하기 위해 일부 조작해주자 | . a[2] = a[0] + a[1] a[3] = a[0] print(np.round(a,3)) . [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.542 0.899 1.041 -0.073] [-0.212 -0.285 -0.574 -0.44 ]] . U, Sigma, Vt = svd(a) print(U.shape, Sigma.shape, Vt.shape) print(&#39;Sigma Value: n&#39;,np.round(Sigma,3)) . (4, 4) (4,) (4, 4) Sigma Value: [2.663 0.807 0. 0. ] . 이전과 차원은 같지만 Sigma 값 중 2개가 0으로 변했다. 즉, 선형 독립인 로우 벡터의 개수가 2개라는 의미이다. 즉 행렬의 Rank가 2개이다. 이번에는 U,Sigma,Vt의 전체 데이터를 이용하지 않고 Sigma의 0에 대응되는 U,Sigma,Vt의 데이터를 제외하고 복원해보자. | . U_ = U[:, :2] Sigma_ = np.diag(Sigma[:2]) # V 전치 행렬의 경우는 앞 2행만 추출 Vt_ = Vt[:2] print(U_.shape, Sigma_.shape, Vt_.shape) # U, Sigma, Vt의 내적을 수행하며, 다시 원본 행렬 복원 a_ = np.dot(np.dot(U_,Sigma_), Vt_) print(np.round(a_, 3)) . (4, 2) (2, 2) (2, 4) [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.542 0.899 1.041 -0.073] [-0.212 -0.285 -0.574 -0.44 ]] . Truncated SVD를 이용해 행렬을 분해해보자. 인위적으로 더 작은 차원으로 분해하기 때문에 원본 행렬을 정확하기 다시 원상복구할 순 없다. 하지만 데이터 정보가 압축되어 분해됨에도 불구하고 상당한 수준으로 원본행렬을 근사할 수 있다. 당연한 얘기일테지만. 원래 차원의 차수에 가깝게 잘라낼수록(Truncate) 원본 행렬에 더 가깝게 복원할 수 있다. | Truncated SVD는 희소 행렬로만 지원돼서 scipy.sparse.linalg.svds를 이용해야 한다. | 임의의 행렬 6x6을 Normal SVD로, Truncated SVD로 분해해본뒤 결과들을 비교해보자 | . import numpy as np from scipy.sparse.linalg import svds from scipy.linalg import svd # 원본 행렬을 출력하고, SVD를 적용할 경우 U, Sigma, Vt 의 차원 확인 np.random.seed(121) matrix = np.random.random((6, 6)) print(&#39;원본 행렬: n&#39;,matrix) U, Sigma, Vt = svd(matrix, full_matrices=False) print(&#39; n분해 행렬 차원:&#39;,U.shape, Sigma.shape, Vt.shape) print(&#39; nSigma값 행렬:&#39;, Sigma) # Truncated SVD로 Sigma 행렬의 특이값을 4개로 하여 Truncated SVD 수행. num_components = 5 U_tr, Sigma_tr, Vt_tr = svds(matrix, k=num_components) print(&#39; nTruncated SVD 분해 행렬 차원:&#39;,U_tr.shape, Sigma_tr.shape, Vt_tr.shape) print(&#39; nTruncated SVD Sigma값 행렬:&#39;, Sigma_tr) matrix_tr = np.dot(np.dot(U_tr,np.diag(Sigma_tr)), Vt_tr) # output of TruncatedSVD print(&#39; nTruncated SVD로 분해 후 복원 행렬: n&#39;, matrix_tr) . 원본 행렬: [[0.11133083 0.21076757 0.23296249 0.15194456 0.83017814 0.40791941] [0.5557906 0.74552394 0.24849976 0.9686594 0.95268418 0.48984885] [0.01829731 0.85760612 0.40493829 0.62247394 0.29537149 0.92958852] [0.4056155 0.56730065 0.24575605 0.22573721 0.03827786 0.58098021] [0.82925331 0.77326256 0.94693849 0.73632338 0.67328275 0.74517176] [0.51161442 0.46920965 0.6439515 0.82081228 0.14548493 0.01806415]] 분해 행렬 차원: (6, 6) (6,) (6, 6) Sigma값 행렬: [3.2535007 0.88116505 0.83865238 0.55463089 0.35834824 0.0349925 ] Truncated SVD 분해 행렬 차원: (6, 5) (5,) (5, 6) Truncated SVD Sigma값 행렬: [0.35834824 0.55463089 0.83865238 0.88116505 3.2535007 ] Truncated SVD로 분해 후 복원 행렬: [[0.11368271 0.19721195 0.23106956 0.15961551 0.82758207 0.41695496] [0.55500167 0.75007112 0.24913473 0.96608621 0.95355502 0.48681791] [0.01789183 0.85994318 0.40526464 0.62115143 0.29581906 0.92803075] [0.40782587 0.55456069 0.24397702 0.23294659 0.035838 0.58947208] [0.82711496 0.78558742 0.94865955 0.7293489 0.67564311 0.73695659] [0.5136488 0.45748403 0.64231412 0.82744766 0.14323933 0.0258799 ]] . 사이킷런의 Truncated SVD클래스는 U,Sigma,Vt 행렬을 반환하진 않는다. 사이킷런의 Truncated SVD 클래스는 PCA와 유사하게 작동. 원본데이터를 Truncated SVD 방식으로 분해된 U*Sigma행렬에 선형 변환해 생성한다. | . from sklearn.decomposition import TruncatedSVD, PCA from sklearn.datasets import load_iris import matplotlib.pyplot as plt %matplotlib inline iris = load_iris() iris_ftrs = iris.data # 2개의 주요 component로 TruncatedSVD 변환 tsvd = TruncatedSVD(n_components=2) tsvd.fit(iris_ftrs) iris_tsvd = tsvd.transform(iris_ftrs) # Scatter plot 2차원으로 TruncatedSVD 변환 된 데이터 표현. 품종은 색깔로 구분 plt.scatter(x=iris_tsvd[:,0], y= iris_tsvd[:,1], c= iris.target) plt.xlabel(&#39;TruncatedSVD Component 1&#39;) plt.ylabel(&#39;TruncatedSVD Component 2&#39;) . Text(0, 0.5, &#39;TruncatedSVD Component 2&#39;) . 왼쪽에 있는 그림이 Truncated SVD로 변환된 붓꽃 데이터 세트. 오른쪽은 비교를 위해 PCA로 변환된 붓꽃 데이터 세트이다. | 사이킷런의 Truncated SVD와 PCA 클래스 구현을 조금 더 자세히 들여다보면 두 개 클래스 모두 SVD를 이용해 행렬을 분해한다. 붓꽃 데이터 세트를 스케일링을 정규분포 변환한 뒤에 Truncated SVD와 PCA 클래스 변환을 해보면 두 개가 거의 동일함을 알 수 있다. | . from sklearn.preprocessing import StandardScaler # iris 데이터를 StandardScaler로 변환 scaler = StandardScaler() iris_scaled = scaler.fit_transform(iris_ftrs) # 스케일링된 데이터를 기반으로 TruncatedSVD 변환 수행 tsvd = TruncatedSVD(n_components=2) tsvd.fit(iris_scaled) iris_tsvd = tsvd.transform(iris_scaled) # 스케일링된 데이터를 기반으로 PCA 변환 수행 pca = PCA(n_components=2) pca.fit(iris_scaled) iris_pca = pca.transform(iris_scaled) # TruncatedSVD 변환 데이터를 왼쪽에, PCA변환 데이터를 오른쪽에 표현 fig, (ax1, ax2) = plt.subplots(figsize=(9,4), ncols=2) ax1.scatter(x=iris_tsvd[:,0], y= iris_tsvd[:,1], c= iris.target) ax2.scatter(x=iris_pca[:,0], y= iris_pca[:,1], c= iris.target) ax1.set_title(&#39;Truncated SVD Transformed&#39;) ax2.set_title(&#39;PCA Transformed&#39;) . Text(0.5, 1.0, &#39;PCA Transformed&#39;) . 두 개의 변환 행렬 값과 원본 속성별 컴포넌트 비율값을 실제로 서로 비교해 보면 거의 같음을 알 수 있다. | . print((iris_pca - iris_tsvd).mean()) print((pca.components_ - tsvd.components_).mean()) . 2.3278716917059703e-15 5.2909066017292616e-17 . 모두 0에 가까운 값이므로 2개의 변환이 서로 동일함을 알 수 있다. 즉 데이터 세트가 스케일링으로 데이터 중심이 동일해지면 사이킷런의 SVD와 PCA는 동일한 변환을 수행한다. 이는 PCA가 SVD 알고리즘으로 구현됐음을 의미한다. 하지만 PCA는 밀집 행렬에 대한 변환만 가능하며 SVD는 희소행렬에 대한 변환도 가능하다 | 희소 행렬 : 행렬의 값 대부분이 0인 행렬 | . NMF(Non-Negative Matrix Factorization) . Truncated SVD와 같이 낮은 랭크를 통한 행렬 근사 방식의 변형이다. NMF는 원본 행렬 내의 모든 원소 값이 모두 양수라는 게 보장되면 두 개의 기반 양수 행렬로 분해될 수 있는 기법을 지칭한다. 행렬 분해는 일반적으로 SVD롸 같은 행렬 분해 기법을 통칭하는 것이다. W행렬과 H행렬은 일반적으로 길고 가는 행렬인 W와 작고 넓은 행렬인 H로 분해된다. 이렇게 분해된 행렬은 잠재 요소를 특성으로 갖는다. 분해 행렬 W는 원본 행에 대해서 이 잠재 요소의 값이 얼마나 되는지에 대응하며 분해 행렬 H는 이 잠재 요소가 원본 열로 어떻게 구성됐는지를 나타내는 행렬이다. . from sklearn.decomposition import NMF from sklearn.datasets import load_iris import matplotlib.pyplot as plt %matplotlib inline iris = load_iris() iris_ftrs = iris.data nmf = NMF(n_components=2) nmf.fit(iris_ftrs) iris_nmf = nmf.transform(iris_ftrs) plt.scatter(x=iris_nmf[:,0], y= iris_nmf[:,1], c= iris.target) plt.xlabel(&#39;NMF Component 1&#39;) plt.ylabel(&#39;NMF Component 2&#39;) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages sklearn decomposition _nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence. warnings.warn(&#34;Maximum number of iterations %d reached. Increase it to&#34; . Text(0, 0.5, &#39;NMF Component 2&#39;) . Conclusion . 무엇보다도 차원 축소는 단순히 feature의 개수를 줄이는 개념보다는 이를 통해 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하는 데 큰 의미가 있다. 이 때문에 많은 차원을 가지는 이미지나 텍스트에서 PCA나 SVD 등의 차원 축소 알고리즘이 활발하게 사용된다. . | . &#44400;&#51665;&#54868; . K-평균 군집화(Clustering)에서 가장 일반적으로 사용되는 알고리즘이다. K-평균은 군집 중심점(centroid)이라는 특정한 임의의 지점을 선택해 해당 중심에 가장 가까운 포인트들을 선택하는 군집화 기번이다. . step - 2개의 군짐 중심점을 설정, 각 데이터는 가장 가까운 중심점에 소속, 중심점에 할당된 데이터들의 평균 중심으로 중심점 이동, 각 데이터는 이동된 중심점을 기분으로 가장 가까운 중심점에 소속, 다시 중심점에 할당딘 데이터들의 평균 중심으로 중심점 이동, 중심점을 이동하였지만 데이터들의 중심점 소속 변경이 없으면 군집화 완료 . 임의 위치에 군집 중심점을 가져다 놓으면 반복적인 이동 수행을 너무 많이 해서 수행 시간이 오래 걸리기 때문에 초기화 알고리즘으로 적합한 위치에 중심점을 가져다 놓지만, 여기서는 설명을 위해 임의 위치로 가정하겠다고 한 것 뿐 . K-평균의 장점 일반적인 군집화에서 가장 많이 활용되는 알고리즘이다. | 알고리즘이 쉽고 간결하다. | . | K-평균의 단점 거리 기반 알고리즘으로서 속성의 개수가 매우 많을 경우 군집화 정확도가 떨어진다. 이를 위해 PCA로 차원 감소를 적용해야 할 수도 있다. | 반복을 수행하는데, 횟수가 많을 경우 시간이 많이 소요 | 몇개의 군집을 선택해야 할지 가이드하기 어렵다. | . | . K-평균을 이용해 붓꽃 데이터 세트를 군집화해보자 | . from sklearn.datasets import load_iris from sklearn.cluster import KMeans import matplotlib.pyplot as plt import numpy as np import pandas as pd %matplotlib inline iris = load_iris() # 보다 편리한 데이터 Handling을 위해 DataFrame으로 변환 irisDF = pd.DataFrame(data=iris.data, columns=[&#39;sepal_length&#39;,&#39;sepal_width&#39;,&#39;petal_length&#39;,&#39;petal_width&#39;]) irisDF.head(3) . sepal_length sepal_width petal_length petal_width . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 붓꽃 데이터 세트를 3개 그룸으로 군집화해보자. | . kmeans = KMeans(n_clusters=3, init=&#39;k-means++&#39;, max_iter=300,random_state=0) kmeans.fit(irisDF) . KMeans(n_clusters=3, random_state=0) . print(kmeans.labels_) print(kmeans.predict(irisDF)) . [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0 0 2] [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 0 0 0 2 0 0 0 0 0 0 2 2 0 0 0 0 2 0 2 0 2 0 0 2 2 0 0 0 0 0 2 0 0 0 0 2 0 0 0 2 0 0 0 2 0 0 2] . irisDF[&#39;cluster&#39;]=kmeans.labels_ irisDF[&#39;target&#39;] = iris.target iris_result = irisDF.groupby([&#39;target&#39;,&#39;cluster&#39;])[&#39;sepal_length&#39;].count() print(iris_result) . target cluster 0 1 50 1 0 2 2 48 2 0 36 2 14 Name: sepal_length, dtype: int64 . 다른 건 양호하나 Target 2값 데이터는 0번 군집에 14개, 2번 군집에 36개로 분산돼 그루핑됐다. | 붓꽃 데이터 세트의 속성이 4개이므로 2차원 평면에 적합치 않아 PCA를 이용해 4개의 속성을 2개로 차원 축소한 뒤에 각 좌표로 개별 데이터를 표현하도록 해보자 | . from sklearn.decomposition import PCA pca = PCA(n_components=2) pca_transformed = pca.fit_transform(iris.data) irisDF[&#39;pca_x&#39;] = pca_transformed[:,0] irisDF[&#39;pca_y&#39;] = pca_transformed[:,1] irisDF.head(3) . sepal_length sepal_width petal_length petal_width cluster target pca_x pca_y . 0 5.1 | 3.5 | 1.4 | 0.2 | 1 | 0 | -2.684126 | 0.319397 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 1 | 0 | -2.714142 | -0.177001 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 1 | 0 | -2.888991 | -0.144949 | . marker0_ind = irisDF[irisDF[&#39;cluster&#39;]==0].index marker1_ind = irisDF[irisDF[&#39;cluster&#39;]==1].index marker2_ind = irisDF[irisDF[&#39;cluster&#39;]==2].index # cluster값 0, 1, 2에 해당하는 Index로 각 cluster 레벨의 pca_x, pca_y 값 추출. o, s, ^ 로 marker 표시 plt.scatter(x=irisDF.loc[marker0_ind,&#39;pca_x&#39;], y=irisDF.loc[marker0_ind,&#39;pca_y&#39;], marker=&#39;o&#39;) plt.scatter(x=irisDF.loc[marker1_ind,&#39;pca_x&#39;], y=irisDF.loc[marker1_ind,&#39;pca_y&#39;], marker=&#39;s&#39;) plt.scatter(x=irisDF.loc[marker2_ind,&#39;pca_x&#39;], y=irisDF.loc[marker2_ind,&#39;pca_y&#39;], marker=&#39;^&#39;) plt.xlabel(&#39;PCA 1&#39;) plt.ylabel(&#39;PCA 2&#39;) plt.title(&#39;3 Clusters Visualization by 2 PCA Components&#39;) plt.show() . Cluster0과 Cluster2는 상당 수준 분리돼 있지만 Cluster1만큼 명확하게 분리돼 있지 않음을 알 수 있다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/16/intro.html",
            "relUrl": "/2022/01/16/intro.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "2022/01/15/SAT",
            "content": "&#52264;&#50896; &#52629;&#49548; . 차원 축소 알고리즘 :PCA, LDA, SVD, NMF / 차원 축소는 매우 많은 feature로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것이다. 일반적으로 차원이 증가할수록 데이터 포인트 간의 거리가 기하급수적으로 멀어지게 되고 희소한 구조를 갖게 된다. 수백 개 이상의 feature로 구성된 데이터 세트의 경우 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도가 떨어진다. 또한 feature가 많을 경우 개별 feature간에 상관관계가 높을 가능성이 크다. 선형 회귀와 같은 선형 모델에서는 입력 변수 간의 상관관계가 높을 경우 이로 인한 다중 공선성 문제로 모델의 예측 성능이 저하된다. 이렇게 매우 많은 다차원이 feature를 차원 축소해 feature수를 죽이면 더 직관적으로 데이터를 해석할 수 있다. 또한 차원 축소를 할 경우 학습 데이터의 크기가 줄어들어서 학습에 필요한 처리 능력도 줄일 수 있다. . 일반적으로 차원 축소는 feature 선택과 feature 추출로 나눌 수 있다. feature selection : 특정 feature에 종속성이 강한 불필요한 feature는 아예 제거하고 데이터의 특징을 잘 나타내는 주요 feature만 선택. feature extraction : 기존 feature를 저차원의 중요 feature로 암축해서 추출하는 것. 따라서 기존 feature와는 완전히 상이한 값이 됨. 차원 축소를 통해 좀 더 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하는 게 중요 . 먼저 PCA(Principal Conponent Analysis) : 가장 대포ㅛ적인 차원 축소 기법이다. 여러 변수 간에 존재하는 상관관계를 이용해 이를 대표하는 주성분을 추출해 차원을 축소하는 기법 | 가장 높은 분산을 가지는 데이터의 축을 찾아 이 축으로 차원을 축소. 이것이 PCA의 주성분이 된다. 즉, 분산이 데이터의 특성을 가장 잘 나타내는 것으로 간주한다. | 제일 먼저 가장 큰 데이터 변동성(Variance)을 기반으로 첫 번째 벡터 축을 생성하고 두 번째 축은 이 벡터 축에 직각이 되는 벡터를 축으로 한다. 세 번째 축은 다시 두 번째 축과 직각이 되는 벡터를 설정하는 방식으로 축을 생성한다. 이렇게 생성된 벡터 축에 원본 데이터를 투영하면 벡터의 축의 개수만큼의 차원으로 원본 데이터가 차원 축소된다. | . PCA를 선형대수 관점에서 해석해놓은 380p 꼭 참고해보자 . 보통 PCA는 다음과 같은 스텝으로 수행된다. . 입력 데이터 세트의 공분산 행렬을 생성한다. | 공분산 행렬의 고유벡터와 고유값을 계산한다. | 고유값이 가장 큰 순으로 K개(PCA 변환 차수만큼)만큼 고유벡터를 추출한다. | 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환한다. | 붓꽃 데이터 세트는 총 네개의 속성으로 되어 있는데 이 4개의 속성을 2개의 PCA 차원으로 압축해 원래 데이터 세트와 압축된 데이터 세트가 어떻게 달라졌는지 확인해보자 | . from sklearn.datasets import load_iris import pandas as pd import matplotlib.pyplot as plt %matplotlib inline # 사이킷런 내장 데이터 셋 API 호출 iris = load_iris() # 넘파이 데이터 셋을 Pandas DataFrame으로 변환 columns = [&#39;sepal_length&#39;,&#39;sepal_width&#39;,&#39;petal_length&#39;,&#39;petal_width&#39;] irisDF = pd.DataFrame(iris.data , columns=columns) irisDF[&#39;target&#39;]=iris.target irisDF.head(3) . sepal_length sepal_width petal_length petal_width target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 각 품종에 따라 원본 붓꽃 데이터 세트가 어떻게 분포돼 있는지 2차원으로 시각화해보자 | 두 개의 속성인 sepal length와 sepal width를 두 축으로 해 품종 데이터 분포를 나타낸다. | . markers=[&#39;^&#39;, &#39;s&#39;, &#39;o&#39;] #setosa의 target 값은 0, versicolor는 1, virginica는 2. 각 target 별로 다른 shape으로 scatter plot for i, marker in enumerate(markers): x_axis_data = irisDF[irisDF[&#39;target&#39;]==i][&#39;sepal_length&#39;] y_axis_data = irisDF[irisDF[&#39;target&#39;]==i][&#39;sepal_width&#39;] plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i]) plt.legend() plt.xlabel(&#39;sepal length&#39;) plt.ylabel(&#39;sepal width&#39;) plt.show() . setosa 품종의 경우 sepal width가 3.0보다 크고, sepal length가 6.0 이하인 곳에 일정하게 분포돼 있다. 나머지 두 품종은 두 축만으로는 분류가 어려운 복잡한 조건임을 알 수 있다. | 이제 PCA로 4개의 속성을 2개로 압축한 뒤 앞의 예제와 비슷하게 2개의 PCA 속성으로 붓꽃 데이터의 품종 분포를 2차원으로 시각화해보자 | . PCA는 여러 속성의 값을 연산해야 하므로 속성의 스케일에 영향을 받는다. 따라서 여러 속성을 PCA로 압축하기 전에 각 속성값을 동일한 스케일로 변환하는 것이 필요 | . from sklearn.preprocessing import StandardScaler # 스케일링 적용 중 iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:, :-1]) . 이제 PCA 데이터로 변환해보자 | . iris_scaled.shape . (150, 4) . from sklearn.decomposition import PCA pca = PCA(n_components=2) #fit( )과 transform( ) 을 호출하여 PCA 변환 데이터 반환 pca.fit(iris_scaled) iris_pca = pca.transform(iris_scaled) . (150, 2) . from sklearn.decomposition import PCA pca = PCA(n_components=2) #fit( )과 transform( ) 을 호출하여 PCA 변환 데이터 반환 pca.fit(iris_scaled) iris_pca = pca.transform(iris_scaled) . print(iris_pca.shape) . (150, 2) . iris_pca는 넘파이 행렬임. DataFrame으로 변환한 뒤 데이터값을 확인해보자 | . pca_columns=[&#39;pca_component_1&#39;,&#39;pca_component_2&#39;] irisDF_pca = pd.DataFrame(iris_pca,columns=pca_columns) irisDF_pca[&#39;target&#39;]=iris.target irisDF_pca.head(3) . pca_component_1 pca_component_2 target . 0 -2.264703 | 0.480027 | 0 | . 1 -2.080961 | -0.674134 | 0 | . 2 -2.364229 | -0.341908 | 0 | . 이제 2개의 속성으로 PCA 변환된 데이터 세트를 2차원상에서 시각화해보자. | . markers=[&#39;^&#39;, &#39;s&#39;, &#39;o&#39;] #pca_component_1 을 x축, pc_component_2를 y축으로 scatter plot 수행. for i, marker in enumerate(markers): x_axis_data = irisDF_pca[irisDF_pca[&#39;target&#39;]==i][&#39;pca_component_1&#39;] y_axis_data = irisDF_pca[irisDF_pca[&#39;target&#39;]==i][&#39;pca_component_2&#39;] plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i]) plt.legend() plt.xlabel(&#39;pca_component_1&#39;) plt.ylabel(&#39;pca_component_2&#39;) plt.show() . setosa는 여전히 구분이 잘 되고 나머지 두 품종도 setosa만큼은 아니지만 그래도 전 보다는 비교적 잘 구분이 되고 있다. 이는 PCA의 첫 번째 새로운 축인 x축이 원본 데이터의 변동성을 잘 반영했기 때문이다. PCA Component별로 원본 데이터의 변동성을 얼마나 반영하고 있는지 확인해보자. PCA 변환을 수행한 PCA 객체의 explained_varianceratio 속성은 전체 변동성에서 개별 PCA 컴포넌트별로 차지하는 변동성 비율을 제공하고 있다. | . print(pca.explained_variance_ratio_) . [0.72962445 0.22850762] . PCA를 2개 요소로만 변환해도 두 변동성을 더한 값인 약 95%의 변동성을 설명할 수 있다는 것을 알았다. | PCA를 적용하기 전과 적용하고 난 후를 각각 분류를 실행한 뒤 결과를 비교해보자 | . from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np rcf = RandomForestClassifier(random_state=156) scores = cross_val_score(rcf, iris.data, iris.target,scoring=&#39;accuracy&#39;,cv=3) print(&#39;원본 데이터 교차 검증 개별 정확도: &#39;,scores) print(&#39;원본 데이터 평균 정확도 :&#39;,np.mean(scores)) . 원본 데이터 교차 검증 개별 정확도: [0.98 0.94 0.96] 원본 데이터 평균 정확도 : 0.96 . 이번에는 PCA 변환한 데이터 세트에 랜덤 포레스트를 적용해보자 | . pca_X = irisDF_pca[[&#39;pca_component_1&#39;, &#39;pca_component_2&#39;]] scores_pca = cross_val_score(rcf, pca_X, iris.target, scoring=&#39;accuracy&#39;, cv=3 ) print(scores_pca) print(np.mean(scores_pca)) . [0.88 0.88 0.88] 0.88 . 원본 데이터 세트 대비 예측 정확도는 PCA 변환 차원 개수에 따라 예측 성능이 떨어질 수밖에 없다. | 다음으로는 좀 더 많은 feature를 가진 데이터 세트를 적은 PCA 컴포넌트 기반으로 변환한 뒤 예측 영향도가 어떻게 되는지 변환된 PCA 데이터 세트에 기반해서 비교해보자 | 새로운 데이터 세트먼저 불러와 보자 | . import pandas as pd df = pd.read_excel(&#39;pca_credit_card.xls&#39;, sheet_name=&#39;Data&#39;, header=1).iloc[0:,1:] print(df.shape) df.head(3) . (30000, 24) . LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 ... BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month . 0 20000 | 2 | 2 | 1 | 24 | 2 | 2 | -1 | -1 | -2 | ... | 0 | 0 | 0 | 0 | 689 | 0 | 0 | 0 | 0 | 1 | . 1 120000 | 2 | 2 | 2 | 26 | -1 | 2 | 0 | 0 | 0 | ... | 3272 | 3455 | 3261 | 0 | 1000 | 1000 | 1000 | 0 | 2000 | 1 | . 2 90000 | 2 | 2 | 2 | 34 | 0 | 0 | 0 | 0 | 0 | ... | 14331 | 14948 | 15549 | 1518 | 1500 | 1000 | 1000 | 1000 | 5000 | 0 | . 3 rows × 24 columns . 맨 마지막 column이 target 값이며 원본 데이터 세트에 PAY_0 다음에 불규칙적으로 PAY_2 column이 있으므로 PAY_0을 PAY_1로 변경해주고 일부 긴 column명은 축약해주자 | . df.rename(columns={&#39;PAY_0&#39;:&#39;PAY_1&#39;,&#39;default payment next month&#39;:&#39;default&#39;}, inplace=True) y_target = df[&#39;default&#39;] # default 컬럼 Drop X_features = df.drop(&#39;default&#39;, axis=1) . 해당 데이터 세트는 23개의 속성 데이터 세트가 있으나 각 속성끼리상관도가 매우 높다. DataFrame의 corr()를 이용해 각 속성 간의 상관도를 구한 뒤 이를 시각화해보자 | . import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline corr = X_features.corr() plt.figure(figsize=(14,14)) sns.heatmap(corr, annot=True, fmt=&#39;.1g&#39;) . &lt;AxesSubplot:&gt; . 높은 상관도를 가진 속성들은 소수의 PCA만으로도 자연스럽게 이 속성들의 변동성을 수용할 수 있다. | 높은 상관도를 가진 BILL_AMT1 ~ BILL_AMT6까지 6개의 속성을 2개의 컴포넌트로 PCA 변환한 뒤 개별 컴포넌트의 변동성을 explained_variance_ratio_속성으로 알아보자 | . from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler #BILL_AMT1 ~ BILL_AMT6까지 6개의 속성명 생성 cols_bill = [&#39;BILL_AMT&#39;+str(i) for i in range(1, 7)] print(&#39;대상 속성명:&#39;, cols_bill) # 2개의 PCA 속성을 가진 PCA 객체 생성하고, explained_variance_ratio_ 계산을 위해 fit( ) 호출 scaler = StandardScaler() df_cols_scaled = scaler.fit_transform(X_features[cols_bill]) pca = PCA(n_components=2) pca.fit(df_cols_scaled) print(&#39;PCA Component별 변동성:&#39;, pca.explained_variance_ratio_) . 대상 속성명: [&#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;] PCA Component별 변동성: [0.90555253 0.0509867 ] . 단 2개의 PCA 컴포넌트만으로도 6개 속성의 변동성을 약 95% 이상 설명할 수 있으며 특히 첫 번째 PCA 축으로 90%의 변동성을 수용할 정도로 이 6개 속성의 상관도가 매우 높다. | 원본 데이터 세트와 6개의 컴포넌트로 PCA변환한 데이터 세트의 분류 예측 결과를 상호 비교해보자 | . import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score rcf = RandomForestClassifier(n_estimators=300, random_state=156) scores = cross_val_score(rcf, X_features, y_target, scoring=&#39;accuracy&#39;, cv=3 ) print(&#39;CV=3 인 경우의 개별 Fold세트별 정확도:&#39;,scores) print(&#39;평균 정확도:{0:.4f}&#39;.format(np.mean(scores))) . CV=3 인 경우의 개별 Fold세트별 정확도: [0.8083 0.8196 0.8232] 평균 정확도:0.8170 . from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler # 원본 데이터셋에 먼저 StandardScaler적용 scaler = StandardScaler() df_scaled = scaler.fit_transform(X_features) # 6개의 Component를 가진 PCA 변환을 수행하고 cross_val_score( )로 분류 예측 수행. pca = PCA(n_components=6) df_pca = pca.fit_transform(df_scaled) scores_pca = cross_val_score(rcf, df_pca, y_target, scoring=&#39;accuracy&#39;, cv=3) print(&#39;CV=3 인 경우의 PCA 변환된 개별 Fold세트별 정확도:&#39;,scores_pca) print(&#39;PCA 변환 데이터 셋 평균 정확도:{0:.4f}&#39;.format(np.mean(scores_pca))) . CV=3 인 경우의 PCA 변환된 개별 Fold세트별 정확도: [0.7911 0.7965 0.8026] PCA 변환 데이터 셋 평균 정확도:0.7967 . 예측 성능 차이가 1~2% 정도 차이난다. | 1~2% 성능 차이를 미비한 성능차이로 보긴 어렵지만 전체 속성의 1/4수준으로도 이정도 수치의 예측 성능을 유지할 수 있다는 것은 PCA의 뛰어난 압축 능력을 잘 보여주는 것이라고 생각해도 좋다 | PCA는 차원 축소를 통해 데이터를 쉽게 인지하는 데 활용할 수 있지만 이보다 더 활발하게 적용되는 영역은 컴퓨터 비전(Computer Vision)분야이다. 특히 얼굴 인식의 경우 Eigen-face라고 불리는 PCA 변환으로 원본 얼굴 이미지를 변환해 사용하는 경우가 많다. | . LDA :LinearDiscriminantAnalysis &#49440;&#54805; &#54032;&#48324; &#48516;&#49437;&#48277;. PCA&#50752; &#50976;&#49324;&#54616;&#44172; &#51077;&#47141; &#45936;&#51060;&#53552; &#49464;&#53944;&#47484; &#51200;&#52264;&#50896; &#44277;&#44036;&#50640; &#53804;&#50689;&#54644; &#52264;&#50896;&#51012; &#52629;&#49548;&#54616;&#45716; &#44592;&#48277;&#51060;&#51648;&#47564; &#51473;&#50836;&#54620; &#52264;&#51060;&#45716; LDA&#45716; &#51648;&#46020;&#54617;&#49845;&#51032; &#48516;&#47448;&#50640;&#49436; &#49324;&#50857;&#54616;&#44592; &#49789;&#46020;&#47197; &#44060;&#48324; &#53364;&#47000;&#49828;&#47484; &#48516;&#48324;&#54624; &#49688; &#51080;&#45716; &#44592;&#51456;&#51012; &#52572;&#45824;&#54620; &#50976;&#51648;&#54616;&#47732;&#49436; &#52264;&#50896;&#51012; &#52629;&#49548;&#54620;&#45796;. PCA&#45716; &#51077;&#47141; &#45936;&#51060;&#53552;&#51032; &#48320;&#46041;&#49457;&#51032; &#44032;&#51109; &#53360; &#52629;&#51012; &#52286;&#50520;&#51648;&#47564; LDA&#45716; &#51077;&#47141; &#45936;&#51060;&#53552;&#51032; &#44208;&#51221; &#44050; &#53364;&#47000;&#49828;&#47484; &#52572;&#45824;&#54620;&#51004;&#47196; &#48516;&#47532;&#54624; &#49688; &#51080;&#45716; &#52629;&#51012; &#52286;&#45716;&#45796;. LDA&#45716; &#53945;&#51221; &#44277;&#44036;&#49345;&#50640;&#49436; &#53364;&#47000;&#49828; &#48516;&#47532;&#47484; &#52572;&#45824;&#54868;&#54616;&#45716; &#52629;&#51012; &#52286;&#44592; &#50948;&#54644; &#53364;&#47000;&#49828; &#44036; &#48516;&#49328;&#44284; &#53364;&#47000;&#49828; &#45236;&#48512; &#48516;&#49328;&#51032; &#48708;&#50984;&#51012; &#52572;&#45824;&#54868;&#54616;&#45716; &#48169;&#49885;&#51004;&#47196; &#52264;&#50896;&#51012; &#52629;&#49548;&#54620;&#45796;. &#51593; &#53364;&#47000;&#49828; &#44036; &#48516;&#49328;&#51008; &#52572;&#45824;&#54620; &#53356;&#44172; &#44032;&#51256;&#44032;&#44256; &#53364;&#47000;&#49828; &#45236;&#48512;&#51032; &#48516;&#49328;&#51008; &#52572;&#45824;&#54620; &#51089;&#44172; &#44032;&#51256;&#44032;&#45716; &#48169;&#49885;&#51060;&#45796;. . 일반적으로 LDA를 구하는 절차는 PCA와 유사하나, 가장 큰 차이점은 공분산 행렬이 아니라 위에 설명한 클래스 간 분산과 클래스 내부 분산 행렬을 생성한 뒤 이 행렬에 기반해 고유벡터를 구하고 입력 데이터를 투영한다는 점이다. . 절차 | . 클래스 내부와 클래스 간 분산 행렬을 구한다. 이 두개의 행렬은 입력 데이터의 결정 값 클래스별로 개별 feature의 평균 벡터를 기반으로 구한다. | 클래스 내부 분산 행렬을 $S_W$, 클래스 간 분산 행렬을 $S_B$라고 하면 다음 식으로 두 행렬을 고유벡터로 분해할 수 있다. (394p 참고) | 고유값이 가장 큰 순으로 K개(LDA 변환 차수만큼) 추출한다. | 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환한다. | from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.preprocessing import StandardScaler from sklearn.datasets import load_iris iris = load_iris() iris_scaled = StandardScaler().fit_transform(iris.data) . 2개의 컴포넌트로 붓꽃 데이터를 LDA 변환하겠다. PCA와 다르게 LDA에서 한 가지 유의해야 할 점은 LDA는 실제로는 PCA와 다르게 비지도 학습이 아닌 지도학습이라는 것이다. 즉 클래스의 결정값이 변환 시에 필요하다. | . lda = LinearDiscriminantAnalysis(n_components=2) # fit()호출 시 target값 입력 lda.fit(iris_scaled, iris.target) iris_lda = lda.transform(iris_scaled) print(iris_lda.shape) . (150, 2) . import pandas as pd import matplotlib.pyplot as plt %matplotlib inline lda_columns=[&#39;lda_component_1&#39;,&#39;lda_component_2&#39;] irisDF_lda = pd.DataFrame(iris_lda,columns=lda_columns) irisDF_lda[&#39;target&#39;]=iris.target #setosa는 세모, versicolor는 네모, virginica는 동그라미로 표현 markers=[&#39;^&#39;, &#39;s&#39;, &#39;o&#39;] #setosa의 target 값은 0, versicolor는 1, virginica는 2. 각 target 별로 다른 shape으로 scatter plot for i, marker in enumerate(markers): x_axis_data = irisDF_lda[irisDF_lda[&#39;target&#39;]==i][&#39;lda_component_1&#39;] y_axis_data = irisDF_lda[irisDF_lda[&#39;target&#39;]==i][&#39;lda_component_2&#39;] plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i]) plt.legend(loc=&#39;upper right&#39;) plt.xlabel(&#39;lda_component_1&#39;) plt.ylabel(&#39;lda_component_2&#39;) plt.show() .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/15/intro.html",
            "relUrl": "/2022/01/15/intro.html",
            "date": " • Jan 15, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "2022/01/13/THU",
            "content": "&#52880;&#44544; &#51452;&#53469; &#44032;&#44201;:&#44256;&#44553; &#54924;&#44480; &#44592;&#48277; &#49457;&#45733; &#54217;&#44032;&#45716; RMSLE&#47484; &#44592;&#48152;&#51004;&#47196; &#54620;&#45796;. &#44032;&#44201;&#51060; &#48708;&#49916; &#51452;&#53469;&#51068;&#49688;&#47197; &#50696;&#52769; &#44208;&#44284; &#50724;&#47448;&#44032; &#51204;&#52404; &#50724;&#47448;&#50640; &#48120;&#52824;&#45716; &#48708;&#51473;&#51060; &#45458;&#51004;&#48064;&#47196; &#51060;&#44163;&#51012; &#49345;&#49604;&#54616;&#44592; &#50948;&#54644; &#50724;&#47448; &#44050;&#51012; &#47196;&#44536; &#48320;&#54872;&#54620; RMSLE&#47484; &#51060;&#50857;&#54620;&#45796;. . 데이터 사전 처리 | . import warnings warnings.filterwarnings(&#39;ignore&#39;) import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline house_df_org = pd.read_csv(&#39;house_price.csv&#39;) house_df = house_df_org.copy() house_df.head(3) . Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 1 | 60 | RL | 65.0 | 8450 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 2 | 2008 | WD | Normal | 208500 | . 1 2 | 20 | RL | 80.0 | 9600 | Pave | NaN | Reg | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 5 | 2007 | WD | Normal | 181500 | . 2 3 | 60 | RL | 68.0 | 11250 | Pave | NaN | IR1 | Lvl | AllPub | ... | 0 | NaN | NaN | NaN | 0 | 9 | 2008 | WD | Normal | 223500 | . 3 rows × 81 columns . Target 값은 맨 마지막 column인 SalePrice이다. | . print(&#39;데이터 세트의 Shape:&#39;, house_df.shape) print(&#39; n전체 feature 들의 type n&#39;,house_df.dtypes.value_counts()) isnull_series = house_df.isnull().sum() print(&#39; nNull 컬럼과 그 건수: n &#39;, isnull_series[isnull_series &gt; 0].sort_values(ascending=False)) . 데이터 세트의 Shape: (1460, 81) 전체 feature 들의 type object 43 int64 35 float64 3 dtype: int64 Null 컬럼과 그 건수: PoolQC 1453 MiscFeature 1406 Alley 1369 Fence 1179 FireplaceQu 690 LotFrontage 259 GarageType 81 GarageYrBlt 81 GarageFinish 81 GarageQual 81 GarageCond 81 BsmtExposure 38 BsmtFinType2 38 BsmtFinType1 37 BsmtCond 37 BsmtQual 37 MasVnrArea 8 MasVnrType 8 Electrical 1 dtype: int64 . null값이 너무 많은 feature는 drop | 회귀 모델을 적용하기 전에 Target 값의 분포도가 정규 분포인지 확인해보자 | . plt.title(&#39;Original Sale Price Histogram&#39;) sns.distplot(house_df[&#39;SalePrice&#39;]) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Original Sale Price Histogram&#39;}, xlabel=&#39;SalePrice&#39;, ylabel=&#39;Density&#39;&gt; . 데이터 값의 분포가 중심에서 벗어나 왼쪽으로 치우친 형태로 정규 분포에서 벗어나 있음을 알 수 있다. | 정규 분포가 아닌 결괏값을 정규 분포 형태로 변환하기 위해 로그 변환을 적용하자. 로그로 변환된 값은 추루에 expm1()함수를 통해 환원하면 된다. | . plt.title(&#39;Log Transformed Sale Price Histogram&#39;) log_SalePrice = np.log1p(house_df[&#39;SalePrice&#39;]) sns.distplot(log_SalePrice) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Log Transformed Sale Price Histogram&#39;}, xlabel=&#39;SalePrice&#39;, ylabel=&#39;Density&#39;&gt; . 정규 분포 형태로 결괏값이 분포함을 확인했으니 Target값을 로그 변환한 뒤 DataFrame에 반영하자 | . original_SalePrice = house_df[&#39;SalePrice&#39;] house_df[&#39;SalePrice&#39;] = np.log1p(house_df[&#39;SalePrice&#39;]) # Null 이 너무 많은 컬럼들과 불필요한 컬럼 삭제 house_df.drop([&#39;Id&#39;,&#39;PoolQC&#39; , &#39;MiscFeature&#39;, &#39;Alley&#39;, &#39;Fence&#39;,&#39;FireplaceQu&#39;], axis=1 , inplace=True) # Drop 하지 않는 숫자형 Null컬럼들은 평균값으로 대체 house_df.fillna(house_df.mean(),inplace=True) # Null 값이 있는 피처명과 타입을 추출 null_column_count = house_df.isnull().sum()[house_df.isnull().sum() &gt; 0] print(&#39;## Null 피처의 Type : n&#39;, house_df.dtypes[null_column_count.index]) . ## Null 피처의 Type : MasVnrType object BsmtQual object BsmtCond object BsmtExposure object BsmtFinType1 object BsmtFinType2 object Electrical object GarageType object GarageFinish object GarageQual object GarageCond object dtype: object . house_df.mean() 함수는 숫자형 칼럼만 자동으로 mean처리 하기 때문에 애초에 문자형 column은 그대로 null이 유지되고 있다. | 문자형 feature는 모두 원-핫 인코딩으로 변환하자. | 판다스의 get_dummies()는 자동으로 문자열 feature를 원-핫 인코딩 변환하면서 Null 값은 None으로 대체해주기 때문에 별도의 Null값을 대체하는 로직이 필요없다. | 웟-핫 인코딩을 적용하면 당연히 column은 증가한다. | . print(&#39;get_dummies() 수행 전 데이터 Shape:&#39;, house_df.shape) house_df_ohe = pd.get_dummies(house_df) print(&#39;get_dummies() 수행 후 데이터 Shape:&#39;, house_df_ohe.shape) null_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() &gt; 0] print(&#39;## Null 피처의 Type : n&#39;, house_df_ohe.dtypes[null_column_count.index]) . get_dummies() 수행 전 데이터 Shape: (1460, 75) get_dummies() 수행 후 데이터 Shape: (1460, 271) ## Null 피처의 Type : Series([], dtype: object) . column이 늘어났고 이제 Null값을 가진 feature는 존재하지 않는다. 이 정도에서 데이터 세트의 기본적인 가공은 마치고 회귀 모델을 생성해 학습한 후 예측 결과를 평가해보자 | 앞에서 예측 평가는 RMSLE를 이용한다 했으나 이미 Target값인 SalePrice를 로그 변환했다. 예측값 역시 로스 변환된 SalePrice 값을 기반으로 예측하므로 원본 SalePrice 예측값의 로그 변환 값이다. 실제 값도 로스 변환됐고, 예측값도 이를 반영한 로그 변환 값이므로 예측 결과 오류에 RMSE만 적용하면 RMSLE가 자동으로 측정된다. | . 여러 모델의 로그 변환된 RMSE를 측정할 것이므로 이를 계산하는 함수를 먼저 생성하자. | . def get_rmse(model): pred = model.predict(X_test) mse = mean_squared_error(y_test , pred) rmse = np.sqrt(mse) print(&#39;{0} 로그 변환된 RMSE: {1}&#39;.format(model.__class__.__name__,np.round(rmse, 3))) return rmse def get_rmses(models): rmses = [ ] for model in models: rmse = get_rmse(model) rmses.append(rmse) return rmses . 첫번째 함수는 단일 모델의 RMSE 값을 두번째 함수는 첫번째 함수를 이용해 여러 모델의 RMSE 값을 반환한다. | 이제 선형 회귀 모델을 학습하고 예측, 평가해보자 | . from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error y_target = house_df_ohe[&#39;SalePrice&#39;] X_features = house_df_ohe.drop(&#39;SalePrice&#39;,axis=1, inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156) # LinearRegression, Ridge, Lasso 학습, 예측, 평가 lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge() ridge_reg.fit(X_train, y_train) lasso_reg = Lasso() lasso_reg.fit(X_train, y_train) models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) . LinearRegression 로그 변환된 RMSE: 0.132 Ridge 로그 변환된 RMSE: 0.128 Lasso 로그 변환된 RMSE: 0.176 . [0.131895765791545, 0.12750846334053068, 0.17628250556471392] . 라쏘 회귀의 경우 회귀 성능이 타 회귀 방식보다 많이 떨어지는 결과 나왔다 $ to$ 최적 하이퍼 파라미타 튜닝이 필요해 보임 $ to$ alpha 하이퍼 파라미터 최적화를 릿지와 라쏘 모델에 대해서 수행하자. | feature별 회귀 계수를 시각화해서 모델별로 어떠한 feature의 회귀계수로 구성되는지 확인해보자. | . def get_top_bottom_coef(model): # coef_ 속성을 기반으로 Series 객체를 생성. index는 컬럼명. coef = pd.Series(model.coef_, index=X_features.columns) # + 상위 10개 , - 하위 10개 coefficient 추출하여 반환. coef_high = coef.sort_values(ascending=False).head(10) coef_low = coef.sort_values(ascending=False).tail(10) return coef_high, coef_low . 해당 함수를 이용해 모델별 회귀 계수를 시각화하자. 시각화를 위한 함수로 visualize_coefficient(models)를 생성한다. 이 함수는 list 객체로 모델을 입력 받아 모델별로 회귀 계수 상하위 10개를 추출해 가로 막대 그래프 형태로 출력한다. | . def visualize_coefficient(models): # 3개 회귀 모델의 시각화를 위해 3개의 컬럼을 가지는 subplot 생성 fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=3) fig.tight_layout() # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 회귀 계수 시각화. for i_num, model in enumerate(models): # 상위 10개, 하위 10개 회귀 계수를 구하고, 이를 판다스 concat으로 결합. coef_high, coef_low = get_top_bottom_coef(model) coef_concat = pd.concat( [coef_high , coef_low] ) # 순차적으로 ax subplot에 barchar로 표현. 한 화면에 표현하기 위해 tick label 위치와 font 크기 조정. axs[i_num].set_title(model.__class__.__name__+&#39; Coeffiecents&#39;, size=25) axs[i_num].tick_params(axis=&quot;y&quot;,direction=&quot;in&quot;, pad=-120) for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()): label.set_fontsize(22) sns.barplot(x=coef_concat.values, y=coef_concat.index , ax=axs[i_num]) # 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 회귀 계수 시각화. models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) . 모델별 회귀 계수를 보면 OLS기반의 LinearRegression과 Ridge의 경우는 회귀 계수가 유사한 형태로 분포돼 있다. 하지만 Lasso는 전체적으로 회귀 계수 값이 매우 작고 YearBuilt feature가 유난히 크고 나머지 feature의 회귀계수는 작다. | Lasso $ to$ 학습 데이터의 데이터 분할에 문제일 수도 있기에 train_test_split()으로 분할하지 않고 전체 데이터 세트인 X_features와 y_target을 5개의 교차검증폴드세트로 분할해 평균RMSE를 측정해보자 | . from sklearn.model_selection import cross_val_score def get_avg_rmse_cv(models): for model in models: # 분할하지 않고 전체 데이터로 cross_val_score( ) 수행. 모델별 CV RMSE값과 평균 RMSE 출력 rmse_list = np.sqrt(-cross_val_score(model, X_features, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5)) rmse_avg = np.mean(rmse_list) print(&#39; n{0} CV RMSE 값 리스트: {1}&#39;.format( model.__class__.__name__, np.round(rmse_list, 3))) print(&#39;{0} CV 평균 RMSE 값: {1}&#39;.format( model.__class__.__name__, np.round(rmse_avg, 3))) # 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 CV RMSE값 출력 models = [lr_reg, ridge_reg, lasso_reg] get_avg_rmse_cv(models) . LinearRegression CV RMSE 값 리스트: [0.135 0.165 0.168 0.111 0.198] LinearRegression CV 평균 RMSE 값: 0.155 Ridge CV RMSE 값 리스트: [0.117 0.154 0.142 0.117 0.189] Ridge CV 평균 RMSE 값: 0.144 Lasso CV RMSE 값 리스트: [0.161 0.204 0.177 0.181 0.265] Lasso CV 평균 RMSE 값: 0.198 . 그래도 여전히 라쏘의 경우가 OLS 모델이나 릿지 모델보다 성능이 떨어진다. 릿지와 라쏘 모델에 대허서 alpha 하이퍼 파라미터 변화를 통해 최적값을 도출해보자 | 먼저 모델별로 최적화 하이퍼 파라미터 작업을 반복적으로 진행하기 위해 이를 위한 별도의 함수를 생성하자. | . from sklearn.model_selection import GridSearchCV def get_best_params(model, params): grid_model = GridSearchCV(model, param_grid=params, scoring=&#39;neg_mean_squared_error&#39;, cv=5) grid_model.fit(X_features, y_target) rmse = np.sqrt(-1* grid_model.best_score_) print(&#39;{0} 5 CV 시 최적 평균 RMSE 값: {1}, 최적 alpha:{2}&#39;.format(model.__class__.__name__, np.round(rmse, 4), grid_model.best_params_)) return grid_model.best_estimator_ ridge_params = { &#39;alpha&#39;:[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] } lasso_params = { &#39;alpha&#39;:[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] } best_rige = get_best_params(ridge_reg, ridge_params) best_lasso = get_best_params(lasso_reg, lasso_params) . Ridge 5 CV 시 최적 평균 RMSE 값: 0.1418, 최적 alpha:{&#39;alpha&#39;: 12} Lasso 5 CV 시 최적 평균 RMSE 값: 0.142, 최적 alpha:{&#39;alpha&#39;: 0.001} . 라쏘 모델의 경우 alpha 값 최적화 이후 예측 성능이 많이 좋아졌다. 선형 모델이 최적 alpha 값을 설정한 뒤 train_test_split()으로 분할된 학습 데이터와 테스트 데이터를 이용해 학습,예측,평가를 수행하고 모델별 회귀 계수를 시각화해보자 | . lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge(alpha=12) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 모든 모델의 RMSE 출력 models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) # 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) . LinearRegression 로그 변환된 RMSE: 0.132 Ridge 로그 변환된 RMSE: 0.124 Lasso 로그 변환된 RMSE: 0.12 . alpha 값 최적화 후 테스트 데이터 세트의 예측 성능이 더 좋아졌으며 모델별 회귀 계수도 많이 달라졌다. | 다만 라쏘의 경우 릿지에 비해 동일 feature라도 회귀 계수의 값이 상당히 작다. | 데이터 세트를 추가적으로 가공하여 모델 튜닝을 좀 더 진행해보자. | 첫 번째는 feature 데이터 세트의 데이터 분포도이고 두 번째는 이상치 데이터 처리이다. | . 앞 부분에서는 Target 데이터 세트의 데이터 분포도의 왜곡을 확인했다. feature 데이터 세트의 경우도 지나치게 왜곡된 feature가 존재할 경우 회귀 예측 성능을 저하시킬 수 있다. 모든 숫자형 feature의 데이터 분포도를 확인해 분포도가 어느 정도로 왜곡됐는지 알아보자 . 사이파이의 stats 모듈의 skew() 함수를 이용해 칼럼의 데이터 세트의 왜곡된 정도를 쉽게 추출할 수 있다. 일반적으로 skew() 함수의 반환 값이 1 이상인 경우를 왜곡 정도가 높다고 판단하지만 상황에 따라 편차는 있다. 여기서는 1 이상의 값을 반환하는 feature만 추출해 왜곡 정도를 완화하기 위해 로그 변환을 적용하겠다. *주의 : skew()를 적용하는 숫자형 feature에서 원-핫 인코딩된 카테고리 숫자형 feature는 제외해야 한다. 카테고리 feature는 코드성 feature이므로 인코딩시 당연히 왜곡될 가능성이 높다. 예를 들어 화장실 여부가 1과 0으로 표현됐는데 1로 1000건 0으로 10건일 때 이는 왜곡과 무관하므로. 따라서 skew() 함수를 적용하는 DataFrame은 원-핫 인코딩이 적용된 house_df_ohe이 아니라 그냥 house_df여야 한다. . from scipy.stats import skew # object가 아닌 숫자형 피쳐의 컬럼 index 객체 추출. features_index = house_df.dtypes[house_df.dtypes != &#39;object&#39;].index # house_df에 컬럼 index를 [ ]로 입력하면 해당하는 컬럼 데이터 셋 반환. apply lambda로 skew( )호출 skew_features = house_df[features_index].apply(lambda x : skew(x)) # skew 정도가 1 이상인 컬럼들만 추출. skew_features_top = skew_features[skew_features &gt; 1] print(skew_features_top.sort_values(ascending=False)) . MiscVal 24.451640 PoolArea 14.813135 LotArea 12.195142 3SsnPorch 10.293752 LowQualFinSF 9.002080 KitchenAbvGr 4.483784 BsmtFinSF2 4.250888 ScreenPorch 4.117977 BsmtHalfBath 4.099186 EnclosedPorch 3.086696 MasVnrArea 2.673661 LotFrontage 2.382499 OpenPorchSF 2.361912 BsmtFinSF1 1.683771 WoodDeckSF 1.539792 TotalBsmtSF 1.522688 MSSubClass 1.406210 1stFlrSF 1.375342 GrLivArea 1.365156 dtype: float64 . 이제 추출된 왜곡 정도가 높은 feature를 로그 변환하자 | . house_df[skew_features_top.index] = np.log1p(house_df[skew_features_top.index]) . 원-핫 인코딩 적용 안 된 걸로 수행했기 때문에 다시 원-핫 인코딩 수행해주자 | . house_df_ohe = pd.get_dummies(house_df) y_target = house_df_ohe[&#39;SalePrice&#39;] X_features = house_df_ohe.drop(&#39;SalePrice&#39;,axis=1, inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156) # 피처들을 로그 변환 후 다시 최적 하이퍼 파라미터와 RMSE 출력 ridge_params = { &#39;alpha&#39;:[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] } lasso_params = { &#39;alpha&#39;:[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] } best_ridge = get_best_params(ridge_reg, ridge_params) best_lasso = get_best_params(lasso_reg, lasso_params) . Ridge 5 CV 시 최적 평균 RMSE 값: 0.1275, 최적 alpha:{&#39;alpha&#39;: 10} Lasso 5 CV 시 최적 평균 RMSE 값: 0.1252, 최적 alpha:{&#39;alpha&#39;: 0.001} . 릿지 모델의 경우 최적 alpha 값이 12에서 10으로 변경됐고 두 모델 모두 feature의 로그 변환 이전과 비교해 RMSE값이 향상됐다. | 다시 위의 train_test_split으로 분할된 학습 데이터와 테스트 데이터를 이용해 모델의 학습/예측/평가 및 모델별 회귀 계수를 시각화해보자. | . lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge(alpha=10) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 모든 모델의 RMSE 출력 models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) # 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) . LinearRegression 로그 변환된 RMSE: 0.128 Ridge 로그 변환된 RMSE: 0.122 Lasso 로그 변환된 RMSE: 0.119 . 세 모델 모두 GrLivArea, 즉 주거 공간 크기가 회귀 계수가 가장 높은 feature가 됐다. | . 다음으로 분석할 요소는 이상치 데이터이다. 특히 회귀 계수가 높은 feature, 즉 예측에 많은 영향을 미치는 중요 feature의 이상치 데이터 처리가 중요하다. 먼저 GrLivArea feature의 데이터 분포도를 살펴보자 . plt.scatter(x = house_df_org[&#39;GrLivArea&#39;], y = house_df_org[&#39;SalePrice&#39;]) plt.ylabel(&#39;SalePrice&#39;, fontsize=15) plt.xlabel(&#39;GrLivArea&#39;, fontsize=15) plt.show() . 일반적으로 주거 공간이 큰 집일수록 가격이 비싸기 때문에 양의 상관도가 매우 높음을 직관적으로 알 수 있다. 하지만 위 그림에서 x축 5000쪽에 데이터 값 두개 너무 어긋나 있음 | 이상치로 간주하고 삭제하자 | . cond1 = house_df_ohe[&#39;GrLivArea&#39;] &gt; np.log1p(4000) cond2 = house_df_ohe[&#39;SalePrice&#39;] &lt; np.log1p(500000) outlier_index = house_df_ohe[cond1 &amp; cond2].index print(&#39;아웃라이어 레코드 index :&#39;, outlier_index.values) print(&#39;아웃라이어 삭제 전 house_df_ohe shape:&#39;, house_df_ohe.shape) # DataFrame의 index를 이용하여 아웃라이어 레코드 삭제. house_df_ohe.drop(outlier_index, axis=0, inplace=True) print(&#39;아웃라이어 삭제 후 house_df_ohe shape:&#39;, house_df_ohe.shape) . 아웃라이어 레코드 index : [ 523 1298] 아웃라이어 삭제 전 house_df_ohe shape: (1460, 271) 아웃라이어 삭제 후 house_df_ohe shape: (1458, 271) . 업데이트된 house_df_ohe를 기반으로 feature데이터 세트와 target데이터 세트를 다시 생성하고 해당함수를 이용해 릿지와 라쏘 모델의 최적화를 수행하고 결과를 출력해보자 | . y_target = house_df_ohe[&#39;SalePrice&#39;] X_features = house_df_ohe.drop(&#39;SalePrice&#39;,axis=1, inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156) ridge_params = { &#39;alpha&#39;:[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] } lasso_params = { &#39;alpha&#39;:[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] } best_ridge = get_best_params(ridge_reg, ridge_params) best_lasso = get_best_params(lasso_reg, lasso_params) . Ridge 5 CV 시 최적 평균 RMSE 값: 0.1125, 최적 alpha:{&#39;alpha&#39;: 8} Lasso 5 CV 시 최적 평균 RMSE 값: 0.1122, 최적 alpha:{&#39;alpha&#39;: 0.001} . 단 두개의 이상치 데이터만 제거했는데 예측 수치가 매우 크게 향상 됐음을 알 수 있다. 웬만큼의 하이퍼 파라미터 튜닝을 해도 이 정도의 수치 개선은 어렵다. GrLivArea 속성이 회귀 모델에서 차지하는 영향도가 크기에 이 이상치를 개선하는 것이 성능 개선에 큰 의미를 가졌다. | 회귀에 중요한 영향을 미치는 feature를 위주로 이상치 데이터를 찾으려는 노력은 중요하다. | . lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge(alpha=8) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 모든 모델의 RMSE 출력 models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) # 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) . LinearRegression 로그 변환된 RMSE: 0.129 Ridge 로그 변환된 RMSE: 0.103 Lasso 로그 변환된 RMSE: 0.1 . 이번에는 회귀 트리를 이용해 회귀 모델을 만들어보자 . XGBoost | . from xgboost import XGBRegressor xgb_params = {&#39;n_estimators&#39;:[1000]} xgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05, colsample_bytree=0.5, subsample=0.8) best_xgb = get_best_params(xgb_reg, xgb_params) . XGBRegressor 5 CV 시 최적 평균 RMSE 값: 0.1178, 최적 alpha:{&#39;n_estimators&#39;: 1000} . LightGBM | . from lightgbm import LGBMRegressor lgbm_params = {&#39;n_estimators&#39;:[1000]} lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1) best_lgbm = get_best_params(lgbm_reg, lgbm_params) . LGBMRegressor 5 CV 시 최적 평균 RMSE 값: 0.1163, 최적 alpha:{&#39;n_estimators&#39;: 1000} . feature중요도 시각화 | . def get_top_features(model): ftr_importances_values = model.feature_importances_ ftr_importances = pd.Series(ftr_importances_values, index=X_features.columns ) ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] return ftr_top20 def visualize_ftr_importances(models): # 2개 회귀 모델의 시각화를 위해 2개의 컬럼을 가지는 subplot 생성 fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=2) fig.tight_layout() # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 피처 중요도 시각화. for i_num, model in enumerate(models): # 중요도 상위 20개의 피처명과 그때의 중요도값 추출 ftr_top20 = get_top_features(model) axs[i_num].set_title(model.__class__.__name__+&#39; Feature Importances&#39;, size=25) #font 크기 조정. for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()): label.set_fontsize(22) sns.barplot(x=ftr_top20.values, y=ftr_top20.index , ax=axs[i_num]) # 앞 예제에서 get_best_params( )가 반환한 GridSearchCV로 최적화된 모델의 피처 중요도 시각화 models = [best_xgb, best_lgbm] visualize_ftr_importances(models) . 회귀 모델의 예측 결과 혼합을 통한 최종 예측 . 이번에는 개별 회귀 모델의 예측 결괏값을 혼합해 이를 기반으로 최종 회귀 값을 예측하겠다. 가령 A모델과 B모델, 두 모델의 예측값이 있다면 각각 예측값의 40%,60%를 더해서 최종 회귀 값으로 예측하는 것이다. 기준은 딱히 없으나 두 개 중 성능이 조금 좋은 쪽에 가중치를 약간 더 뒀다. A회귀 모델의 예측값이 [100,80,60]이고 B회귀 모델의 예측값이 [150,80,50]이라면 각각의 값에 0.4,0.6을 곱하는 것이다. 최종 혼합 모델, 개별 모델의 RMSE값을 출력하는 함수를 생성하고 각 모델의 예측값을 계산한 뒤 개별 모델과 최종 혼합 모델의 RMSE를 구한다. . def get_rmse_pred(preds): for key in preds.keys(): pred_value = preds[key] mse = mean_squared_error(y_test , pred_value) rmse = np.sqrt(mse) print(&#39;{0} 모델의 RMSE: {1}&#39;.format(key, rmse)) # 개별 모델의 학습 ridge_reg = Ridge(alpha=8) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 개별 모델 예측 ridge_pred = ridge_reg.predict(X_test) lasso_pred = lasso_reg.predict(X_test) # 개별 모델 예측값 혼합으로 최종 예측값 도출 pred = 0.4 * ridge_pred + 0.6 * lasso_pred preds = {&#39;최종 혼합&#39;: pred, &#39;Ridge&#39;: ridge_pred, &#39;Lasso&#39;: lasso_pred} #최종 혼합 모델, 개별모델의 RMSE 값 출력 get_rmse_pred(preds) . 최종 혼합 모델의 RMSE: 0.10007930884470487 Ridge 모델의 RMSE: 0.1034517754660322 Lasso 모델의 RMSE: 0.10024170460890011 . 이번에는 XGBoost와 LightGBM을 혼합해 결과를 살펴보자 | . xgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05, colsample_bytree=0.5, subsample=0.8) lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1) xgb_reg.fit(X_train, y_train) lgbm_reg.fit(X_train, y_train) xgb_pred = xgb_reg.predict(X_test) lgbm_pred = lgbm_reg.predict(X_test) pred = 0.5 * xgb_pred + 0.5 * lgbm_pred preds = {&#39;최종 혼합&#39;: pred, &#39;XGBM&#39;: xgb_pred, &#39;LGBM&#39;: lgbm_pred} get_rmse_pred(preds) . 최종 혼합 모델의 RMSE: 0.10170077353447762 XGBM 모델의 RMSE: 0.10738295638346222 LGBM 모델의 RMSE: 0.10382510019327311 . 개별 모델보다 향상 됐음을 알 수 있다. | . 스태킹 앙상블 모델을 통한 회귀 예측 . 핵심은 여러 개별 모델의 예측 데이터를 각각 스태킹 형태로 결합해 최종 메타 모델의 학습용 feature 데이터 세트와 테스트용 feature 데이터 세트를 만드는 것이다. . from sklearn.model_selection import KFold from sklearn.metrics import mean_absolute_error # 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ): # 지정된 n_folds값으로 KFold 생성. kf = KFold(n_splits=n_folds, shuffle=False, random_state=0) #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 train_fold_pred = np.zeros((X_train_n.shape[0] ,1 )) test_pred = np.zeros((X_test_n.shape[0],n_folds)) print(model.__class__.__name__ , &#39; model 시작 &#39;) for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)): #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 print(&#39; t 폴드 세트: &#39;,folder_counter,&#39; 시작 &#39;) X_tr = X_train_n[train_index] y_tr = y_train_n[train_index] X_te = X_train_n[valid_index] #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행. model.fit(X_tr , y_tr) #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장. train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. test_pred[:, folder_counter] = model.predict(X_test_n) # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1) #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터 return train_fold_pred , test_pred_mean . X_train_n = X_train.values X_test_n = X_test.values y_train_n = y_train.values # 각 개별 기반(Base)모델이 생성한 학습용/테스트용 데이터 반환. ridge_train, ridge_test = get_stacking_base_datasets(ridge_reg, X_train_n, y_train_n, X_test_n, 5) lasso_train, lasso_test = get_stacking_base_datasets(lasso_reg, X_train_n, y_train_n, X_test_n, 5) xgb_train, xgb_test = get_stacking_base_datasets(xgb_reg, X_train_n, y_train_n, X_test_n, 5) lgbm_train, lgbm_test = get_stacking_base_datasets(lgbm_reg, X_train_n, y_train_n, X_test_n, 5) . Ridge model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 Lasso model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 XGBRegressor model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 LGBMRegressor model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 . Stack_final_X_train = np.concatenate((ridge_train, lasso_train, xgb_train, lgbm_train), axis=1) Stack_final_X_test = np.concatenate((ridge_test, lasso_test, xgb_test, lgbm_test), axis=1) # 최종 메타 모델은 라쏘 모델을 적용. meta_model_lasso = Lasso(alpha=0.0005) #기반 모델의 예측값을 기반으로 새롭게 만들어진 학습 및 테스트용 데이터로 예측하고 RMSE 측정. meta_model_lasso.fit(Stack_final_X_train, y_train) final = meta_model_lasso.predict(Stack_final_X_test) mse = mean_squared_error(y_test , final) rmse = np.sqrt(mse) print(&#39;스태킹 회귀 모델의 최종 RMSE 값은:&#39;, rmse) . 스태킹 회귀 모델의 최종 RMSE 값은: 0.09799152965189672 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/13/intro.html",
            "relUrl": "/2022/01/13/intro.html",
            "date": " • Jan 13, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "2022/01/12/WED",
            "content": "&#47196;&#51648;&#49828;&#54001; &#54924;&#44480; . 로지스틱 회귀는 선형 회귀 방식을 분류에 적용한 알고리즘이다. 회귀가 선형인지 비선형인가는 독립 변수가 아닌 가중치 변수가 선형인지 아닌지를 따른다. 로지스틱 회귀가 선형 회귀와 다른 점은 학습을 통해 선형 함수의 회귀 최적선을 찾는 것이 아니라 시그모이드(sigmoid) 함수 최적선을 찾고 이 시그모이드 함수의 반환 값을 확률로 간주해 확률에 따라 분류를 결정한다는 것이다. 많은 자연, 사회 현상에서 특정 변수의 확률 값은 선형이 아니라 시그모이드 함수와 같이 S자 커브 형태를 가진다. 시그모이드 함수 :$y= frac{1}{1+e^(-x)}$ 시그모이드 함수는 $x$값이 아무리 음,양으로 커지거나 작아져도 $y$값은 항상 0과 1사이 값을 반환한다. $x$ 값이 커지면 1에 근사하며 $x$ 값이 작아지면 0에 근사한다. 지금까지는 부동산 가격과 같은 연속형 값을 구하는 데 회귀를 사용했다. 이번에는 약간 다르게 가령 종양의 크기에 따라 악성 종양인지 그렇지 않은지 1,0을 이용해 예측해보자. 로지스틱 회귀는 선형 회귀 방식을 기반으로 하되 시그모이드 함수를 이용해 분류를 수행하는 회귀이다. . x=np.arange(-6,6,0.1) y=1/(1+np.exp(-x)) import matplotlib.pyplot as plt plt.plot(x,y) . [&lt;matplotlib.lines.Line2D at 0x1ca3205da30&gt;] . 이제 위스콘신 유방암 데이터 세트를 이용해 로지스틱 회귀로 암 여부를 판단해 보자 | . import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression cancer = load_breast_cancer() . 선형 회귀 계열의 로지스틱 회귀는 데이터의 정규 분포도에 따라 예측 성능 영향을 받을 수 있으므로 데이터에 먼저 정규 분포 형태의 표준 스케일링을 적용하자. | . from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split # StandardScaler( )로 평균이 0, 분산 1로 데이터 분포도 변환 scaler = StandardScaler() data_scaled = scaler.fit_transform(cancer.data) X_train , X_test, y_train , y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0) . from sklearn.metrics import accuracy_score, roc_auc_score # 로지스틱 회귀를 이용하여 학습 및 예측 수행. lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) lr_preds = lr_clf.predict(X_test) # accuracy와 roc_auc 측정 print(&#39;accuracy: {:0.3f}&#39;.format(accuracy_score(y_test, lr_preds))) print(&#39;roc_auc: {:0.3f}&#39;.format(roc_auc_score(y_test , lr_preds))) . accuracy: 0.977 roc_auc: 0.972 . 사이킷런 LogisticRegression 클래스의 주요 하이퍼 파라미터로 penalty와 C가 있다. penalty는 규제의 유형을 설정하며 default는 l2이다. C는 규제 강도를 조절하는 alpha값의 역수이다. 즉 C 값이 작을 수록 규제 강도가 크다. | GridSearchCV를 이용해 위스콘신 데이터 세트에서 이 하이퍼 파라미터를 최적화 해보자 | . import warnings warnings.filterwarnings(&#39;ignore&#39;) from sklearn.model_selection import GridSearchCV params={&#39;penalty&#39;:[&#39;l2&#39;, &#39;l1&#39;], &#39;C&#39;:[0.01, 0.1, 1, 1, 5, 10]} grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring=&#39;accuracy&#39;, cv=3 ) grid_clf.fit(data_scaled, cancer.target) print(&#39;최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}&#39;.format(grid_clf.best_params_, grid_clf.best_score_)) . 최적 하이퍼 파라미터:{&#39;C&#39;: 1, &#39;penalty&#39;: &#39;l2&#39;}, 최적 평균 정확도:0.975 . 로지스틱 회귀는 가볍고 빠르지만 이진 분류 예측 성능도 뛰어나다. 이 때문에 로지스틱 회귀를 이진 분류의 기본 모델로 사용하는 경우가 많다. 또한 로지스틱 회귀는 희소한 데이터 세트 분류에도 뛰어난 성능을 보여서 텍스트 분류에서도 자주 사용된다. | . . 지금까지 선형 회귀에 대해 알아봤다. 선형 회귀는 회귀 계수의 관계를 모두 선형으로 가정하는 방식이다. 일반적으로 선형 회귀는 회귀 계수를 선형으로 결합하는 회귀 함수를 구해 여기에 독립 변수를 입력해 결괏값을 예측하는 것이다. 비선형 회귀 역시 비선형 회귀 함수를 통해 결괏값을 예측한다. 다만 비선형 회귀는 회귀 계수의 결합이 비선형일 뿐이다. 재차 언급하지만 머신러닝 기반의 회귀는 회귀계수를 기반으로 하는 최적 회귀 함수를 도출하는 것이 주요 목표이다. 이번엔 회귀 함수를 기반으로 하지 않고 결정 트리와 같이 트리를 기반으로 하는 회귀방식으로 소개하겠다. | . &#54924;&#44480; &#53944;&#47532; . 즉 회귀를 위한 트리를 생성하고 이를 기반으로 회귀 예측을 하는 것이다. 분류 트리와 크게 다르지 않다. 다만 리프 노드에서 예측 결정 값을 만드는 과정에 차이가 있는데 분류 트리가 특정 클래스 레이블을 결정하는 것과 달리 회귀 트리는 리프 노드에 속한 데이터 값을 평균값을 구해 회귀 예측값을 계산한다. 리프 노드 생성 기준에 부합하는 트리 분할이 완료됐다면 리프 노드에 소속된 데이터 값의 평균값을 구해서 최종적으로 리프 노드에 결정 값으로 할당한다. . 사이킷런의 랜덤 포레스트 회귀 트리인 RandomForestRegressor를 이용해 앞의 선형 회귀에서 다룬 보스턴 주택 가격 예측을 수행해보자. | . from sklearn.datasets import load_boston from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor import pandas as pd import numpy as np # 보스턴 데이터 세트 로드 boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF[&#39;PRICE&#39;] = boston.target y_target = bostonDF[&#39;PRICE&#39;] X_data = bostonDF.drop([&#39;PRICE&#39;], axis=1,inplace=False) rf = RandomForestRegressor(random_state=0, n_estimators=1000) neg_mse_scores = cross_val_score(rf, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39; 5 교차 검증의 개별 Negative MSE scores: &#39;, np.round(neg_mse_scores, 2)) print(&#39; 5 교차 검증의 개별 RMSE scores : &#39;, np.round(rmse_scores, 2)) print(&#39; 5 교차 검증의 평균 RMSE : {0:.3f} &#39;.format(avg_rmse)) . 5 교차 검증의 개별 Negative MSE scores: [ -7.88 -13.14 -20.57 -46.23 -18.88] 5 교차 검증의 개별 RMSE scores : [2.81 3.63 4.54 6.8 4.34] 5 교차 검증의 평균 RMSE : 4.423 . 이번에는 랜덤 포레스트뿐만 아니라 결정 트리, GBM, XGBoost, LightGBM의 Regressor를 모두 이용해 예측을 수행해보자. | . 아래 함수는 입력 모델과 데이터 세트를 입력 받아 교차 검증으로 평균 RMSE를 계산해주는 함수이다. | . def get_model_cv_prediction(model, X_data, y_target): neg_mse_scores = cross_val_score(model, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39;##### &#39;,model.__class__.__name__ , &#39; #####&#39;) print(&#39; 5 교차 검증의 평균 RMSE : {0:.3f} &#39;.format(avg_rmse)) . 이제 다양한 유형의 회귀 트리를 생성하고 이를 이용해 가격을 예측해보자 | . from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4) rf_reg = RandomForestRegressor(random_state=0, n_estimators=1000) gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000) xgb_reg = XGBRegressor(n_estimators=1000) lgb_reg = LGBMRegressor(n_estimators=1000) # 트리 기반의 회귀 모델을 반복하면서 평가 수행 models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg] for model in models: get_model_cv_prediction(model, X_data, y_target) . ##### DecisionTreeRegressor ##### 5 교차 검증의 평균 RMSE : 5.978 ##### RandomForestRegressor ##### 5 교차 검증의 평균 RMSE : 4.423 ##### GradientBoostingRegressor ##### 5 교차 검증의 평균 RMSE : 4.269 ##### XGBRegressor ##### 5 교차 검증의 평균 RMSE : 4.251 ##### LGBMRegressor ##### 5 교차 검증의 평균 RMSE : 4.646 . 회귀 트리 Regressor 클래스는 선형 회귀와 다른 처리 방식이므로 회귀 계수를 제공하는 coef_ 속성이 없다. 대신 feature별 중요도를 알아보자 | . import seaborn as sns %matplotlib inline rf_reg = RandomForestRegressor(n_estimators=1000) # 앞 예제에서 만들어진 X_data, y_target 데이터 셋을 적용하여 학습합니다. rf_reg.fit(X_data, y_target) feature_series = pd.Series(data=rf_reg.feature_importances_, index=X_data.columns ) feature_series = feature_series.sort_values(ascending=False) sns.barplot(x= feature_series, y=feature_series.index) . &lt;AxesSubplot:&gt; . 이번에는 회귀 트리 Regressor가 어떻게 예측값을 판단하는 선형 회귀와 비교해 시각화해보자. 결정 트리의 하이퍼 파라미터인 max_depth의 크기를 변화시키면서 어떻게 회귀 트리 예측선이 변화하는지 살펴보자. 회귀 예측선을 쉽게 표현하귀 위해서 단 1개의 변수만 추출하자. 일단 이 데이터 세트를 시각화 해보자 | . import matplotlib.pyplot as plt %matplotlib inline bostonDF_sample = bostonDF[[&#39;RM&#39;,&#39;PRICE&#39;]] bostonDF_sample = bostonDF_sample.sample(n=100,random_state=0) print(bostonDF_sample.shape) plt.figure() plt.scatter(bostonDF_sample.RM , bostonDF_sample.PRICE,c=&quot;darkorange&quot;) . (100, 2) . &lt;matplotlib.collections.PathCollection at 0x1ca1cd27880&gt; . 다음으로 보스턴 데이터 세트에 대해 LinearRegression과 DecisionTreeRegressor를 max_depth를 각각 2,7로 학습해보자. 이렇게 학습된 Regressor에 RM feature값을 4.5~8.5까지의 100개의 테스트 데이터 세트로 제공했을 때 예측값을 구하자. | . import numpy as np from sklearn.linear_model import LinearRegression # 선형 회귀와 결정 트리 기반의 Regressor 생성. DecisionTreeRegressor의 max_depth는 각각 2, 7 lr_reg = LinearRegression() rf_reg2 = DecisionTreeRegressor(max_depth=2) rf_reg7 = DecisionTreeRegressor(max_depth=7) # 실제 예측을 적용할 테스트용 데이터 셋을 4.5 ~ 8.5 까지 100개 데이터 셋 생성. X_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1) # 보스턴 주택가격 데이터에서 시각화를 위해 피처는 RM만, 그리고 결정 데이터인 PRICE 추출 X_feature = bostonDF_sample[&#39;RM&#39;].values.reshape(-1,1) y_target = bostonDF_sample[&#39;PRICE&#39;].values.reshape(-1,1) # 학습과 예측 수행. lr_reg.fit(X_feature, y_target) rf_reg2.fit(X_feature, y_target) rf_reg7.fit(X_feature, y_target) pred_lr = lr_reg.predict(X_test) pred_rf2 = rf_reg2.predict(X_test) pred_rf7 = rf_reg7.predict(X_test) . 학습된 Regressor에서 예측한 Price 회귀선을 그려보자 | . fig , (ax1, ax2, ax3) = plt.subplots(figsize=(14,4), ncols=3) # X축값을 4.5 ~ 8.5로 변환하며 입력했을 때, 선형 회귀와 결정 트리 회귀 예측 선 시각화 # 선형 회귀로 학습된 모델 회귀 예측선 ax1.set_title(&#39;Linear Regression&#39;) ax1.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=&quot;darkorange&quot;) ax1.plot(X_test, pred_lr,label=&quot;linear&quot;, linewidth=2 ) # DecisionTreeRegressor의 max_depth를 2로 했을 때 회귀 예측선 ax2.set_title(&#39;Decision Tree Regression: n max_depth=2&#39;) ax2.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=&quot;darkorange&quot;) ax2.plot(X_test, pred_rf2, label=&quot;max_depth:3&quot;, linewidth=2 ) # DecisionTreeRegressor의 max_depth를 7로 했을 때 회귀 예측선 ax3.set_title(&#39;Decision Tree Regression: n max_depth=7&#39;) ax3.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=&quot;darkorange&quot;) ax3.plot(X_test, pred_rf7, label=&quot;max_depth:7&quot;, linewidth=2) . [&lt;matplotlib.lines.Line2D at 0x1ca1d4d4280&gt;] . 선형 회귀는 직선으로 예측 회귀선을 표현하는 데 반해, 회귀 트리의 경우 분할되는 데이터 지점에 따라 브랜치를 만들면서 계단 형태로 회귀선을 만든다. max_depth가 7인 경우에는 학습 데이터 세트의 이상치 데이터도 학습하면서 복잡한 계단 형태의 회귀선을 만들어 과적합이 되기 쉬운 모델이 되었음을 알 수 있다. | . &#51088;&#51204;&#44144; &#45824;&#50668; &#49688;&#50836; &#50696;&#52769; . 데이터 클렌징 및 가공, 데이터 세트를 이용해 모델을 학습한 후 대여 횟수를 예측해보자 | . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(&quot;ignore&quot;, category=RuntimeWarning) bike_df = pd.read_csv(&#39;./bike_train.csv&#39;) print(bike_df.shape) bike_df.head(3) . (10886, 12) . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | . bike_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB . null 데이터는 없으며, datetime만 type이 object이다. 일단 이 column에 대해 알아보자 | . bike_df[[&#39;datetime&#39;]] . datetime . 0 2011-01-01 00:00:00 | . 1 2011-01-01 01:00:00 | . 2 2011-01-01 02:00:00 | . 3 2011-01-01 03:00:00 | . 4 2011-01-01 04:00:00 | . ... ... | . 10881 2012-12-19 19:00:00 | . 10882 2012-12-19 20:00:00 | . 10883 2012-12-19 21:00:00 | . 10884 2012-12-19 22:00:00 | . 10885 2012-12-19 23:00:00 | . 10886 rows × 1 columns . 이렇게 년,월,일,시간 형식으로 돼 있으므로 이에 대한 적절한 가공이 필요하다. 이를 각각 분리하자. 판다스에서는 datetime과 같은 형태의 문자열을 년,월,일,시간,분,초로 편리하게 변환하려면 먼저 문자열을 datetime으로 변경해야한다. (우연히 판다스의 datetime 타입과 예제 데이터 세트의 datetime column명이 동일한 것이지 둘을 혼동해선 안 된다) | . bike_df[&#39;datetime&#39;] = bike_df.datetime.apply(pd.to_datetime) bike_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null datetime64[ns] 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: datetime64[ns](1), float64(3), int64(8) memory usage: 1020.7 KB . datetime column의 type이 datetime64로 적절히 변경된 것을 알 수 있다. | 간단히 살펴보자 | . bike_df[[&#39;datetime&#39;]].head(3) . datetime . 0 2011-01-01 00:00:00 | . 1 2011-01-01 01:00:00 | . 2 2011-01-01 02:00:00 | . 겉으로 보기엔 바뀐 게 없어보이지만 아마 type은 잘 바뀌었을 것이다. | . bike_df[&#39;year&#39;] = bike_df.datetime.apply(lambda x : x.year) bike_df[&#39;month&#39;] = bike_df.datetime.apply(lambda x : x.month) bike_df[&#39;day&#39;] = bike_df.datetime.apply(lambda x : x.day) bike_df[&#39;hour&#39;] = bike_df.datetime.apply(lambda x: x.hour) print(bike_df.info()) bike_df.head(3) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 16 columns): # Column Non-Null Count Dtype -- -- 0 datetime 10886 non-null datetime64[ns] 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 12 year 10886 non-null int64 13 month 10886 non-null int64 14 day 10886 non-null int64 15 hour 10886 non-null int64 dtypes: datetime64[ns](1), float64(3), int64(12) memory usage: 1.3 MB None . datetime season holiday workingday weather temp atemp humidity windspeed casual registered count year month day hour . 0 2011-01-01 00:00:00 | 1 | 0 | 0 | 1 | 9.84 | 14.395 | 81 | 0.0 | 3 | 13 | 16 | 2011 | 1 | 1 | 0 | . 1 2011-01-01 01:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 8 | 32 | 40 | 2011 | 1 | 1 | 1 | . 2 2011-01-01 02:00:00 | 1 | 0 | 0 | 1 | 9.02 | 13.635 | 80 | 0.0 | 5 | 27 | 32 | 2011 | 1 | 1 | 2 | . drop_columns = [&#39;datetime&#39;,&#39;casual&#39;,&#39;registered&#39;] bike_df.drop(drop_columns, axis=1,inplace=True) . 기존의 datetime column은 삭제했고, casual,resistered의 합이 count이므로 불필요하기에 같이 삭제하겠음. 오히려 상관도가 높아 예측을 저해할 우려가 있으므로 삭제함. | 다양한 회귀 모델을 데이터 세트에 적용해 예측 성능을 측정해보자 | 사이킷런에서는 RMSLE를 제공하지 않기 때문에 이를 수행하는 함수를 직접 만들어보자 | . from sklearn.metrics import mean_squared_error, mean_absolute_error # log 값 변환 시 언더플로우 영향으로 log() 가 아닌 log1p() 를 이용하여 RMSLE 계산 def rmsle(y, pred): log_y = np.log1p(y) log_pred = np.log1p(pred) squared_error = (log_y - log_pred) ** 2 rmsle = np.sqrt(np.mean(squared_error)) return rmsle # 사이킷런의 mean_square_error() 를 이용하여 RMSE 계산 def rmse(y,pred): return np.sqrt(mean_squared_error(y,pred)) # 책에서는 mean_absolute_error()를 MSE로 잘못 기재함. # MAE, RMSE, RMSLE 를 모두 계산 def evaluate_regr(y,pred): rmsle_val = rmsle(y,pred) rmse_val = rmse(y,pred) # MAE 는 scikit learn의 mean_absolute_error() 로 계산 mae_val = mean_absolute_error(y,pred) print(&#39;RMSLE: {0:.3f}, RMSE: {1:.3F}, MAE: {2:.3F}&#39;.format(rmsle_val, rmse_val, mae_val)) . &#47196;&#44536; &#48320;&#54872;, feature &#51064;&#53076;&#46377;&#44284; &#47784;&#45944; &#54617;&#49845;,&#50696;&#52769;,&#54217;&#44032; . 이제 회귀 모델을 이용해 자전거 대혀 횟수를 예측해보자. 회귀 모델을 적용하기 전에 데이터 세트에 대해서 먼저 처리해야할 사항이 있다. 결괏값이 정규 분포로 돼 있는지 확인하는 것과 카테고리형 회귀 모델의 경우 원-핫 인코딩으로 feature를 인코딩하는 것이다. . 먼저 사이킷런의 LinearRegression 객체를 이용해 회귀 예측을 해보자 | . from sklearn.model_selection import train_test_split , GridSearchCV from sklearn.linear_model import LinearRegression , Ridge , Lasso y_target = bike_df[&#39;count&#39;] X_features = bike_df.drop([&#39;count&#39;],axis=1,inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0) lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) pred = lr_reg.predict(X_test) evaluate_regr(y_test ,pred) . RMSLE: 1.165, RMSE: 140.900, MAE: 105.924 . 실제 Target 데이터 값인 횟수 (count)를 감안하면 예측 오류로서는 비교적 큰 값이다. 실제 값과 예측값이 어느 정도 차이가 나는지 DataFrame의 column으로 만들어서 오류 값이 가장 큰 순으로 5개만 확인해보자. | . def get_top_error_data(y_test, pred, n_tops = 5): # DataFrame에 컬럼들로 실제 대여횟수(count)와 예측 값을 서로 비교 할 수 있도록 생성. result_df = pd.DataFrame(y_test.values, columns=[&#39;real_count&#39;]) result_df[&#39;predicted_count&#39;]= np.round(pred) result_df[&#39;diff&#39;] = np.abs(result_df[&#39;real_count&#39;] - result_df[&#39;predicted_count&#39;]) # 예측값과 실제값이 가장 큰 데이터 순으로 출력. print(result_df.sort_values(&#39;diff&#39;, ascending=False)[:n_tops]) get_top_error_data(y_test,pred,n_tops=20) . real_count predicted_count diff 1618 890 322.0 568.0 3151 798 241.0 557.0 966 884 327.0 557.0 412 745 194.0 551.0 2817 856 310.0 546.0 2277 813 267.0 546.0 2314 766 222.0 544.0 454 721 177.0 544.0 1003 713 171.0 542.0 2394 684 142.0 542.0 1181 891 357.0 534.0 1379 745 212.0 533.0 2003 770 241.0 529.0 1029 901 378.0 523.0 3227 724 202.0 522.0 1038 873 353.0 520.0 3197 694 176.0 518.0 507 688 174.0 514.0 637 900 393.0 507.0 87 594 95.0 499.0 . 회귀에서 이렇게 큰 예측 오류가 발생할 경우 가장 먼저 살펴볼 것은 Target 값의 분포가 왜곡된 형태를 이루고 있는지 확인하는 것이다. Target 값의 분포는 정규 분포 형태가 가장 좋다. 그렇지 않고 왜곡된 경우에는 회귀 예측 성능이 저하되는 경우가 발생하기 쉽다. pandas의 DataFrame의 hist()를 이용해 자전거 대여 모델의 Target 값이 count column이 정규분포를 이루는지 확인해보자 | . y_target.hist() . &lt;AxesSubplot:&gt; . count column이 0~200사이에 왜곡돼 있는 것을 확인할 수 있다. 이렇게 왜곡도니 값을 정규 분포 형태로 바꾸는 가장 일반적인 방법을 로그를 적용해 변환하는 것이다. | 이렇게 로그값으로 변경된 Target값은 다시 expm1() 함수를 적용해 원래 scale 값으로 원상복구하면 된다. | . y_log_transform = np.log1p(y_target) y_log_transform.hist() . &lt;AxesSubplot:&gt; . 정규 분포와 흡사하진 않지만 변환하기 전보다는 왜곡 정도가 많이 향상됐음을 알 수 있다. | 다시 학습한 후 성능을 평가해보자 | . y_target_log = np.log1p(y_target) # 로그 변환된 y_target_log를 반영하여 학습/테스트 데이터 셋 분할 X_train, X_test, y_train, y_test = train_test_split(X_features, y_target_log, test_size=0.3, random_state=0) lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) pred = lr_reg.predict(X_test) # 테스트 데이터 셋의 Target 값은 Log 변환되었으므로 다시 expm1를 이용하여 원래 scale로 변환 y_test_exp = np.expm1(y_test) # 예측 값 역시 Log 변환된 타겟 기반으로 학습되어 예측되었으므로 다시 exmpl으로 scale변환 pred_exp = np.expm1(pred) evaluate_regr(y_test_exp ,pred_exp) . RMSLE: 1.017, RMSE: 162.594, MAE: 109.286 . RMSLE 오류는 줄어들었지만 RMSE는 오히려 더 늘어났다. 왜??????? | 일단 각 feature의 회귀 계수값을 시각화 해보자 | . coef = pd.Series(lr_reg.coef_, index=X_features.columns) coef_sort = coef.sort_values(ascending=False) sns.barplot(x=coef_sort.values, y=coef_sort.index) . &lt;AxesSubplot:&gt; . Year feature의 회귀 계수값이 독보적으로 큰 값을 가지고 있다. Year는 2011,2012년 두개의 값으로만 이루어졌는데 상식적으로 Year가 자전거 대여 횟수에 크게 영향을 준다(회귀 계수가 큼)는 것은 납득하기 어렵다. 그럼 왜 Year feature의 회귀계수가 클까? | Year feature는 datetime이라는 카테고리형 feature지만 숫자형 값으로 돼 있다. 더군다나 아주 큰 2011,2012로 이루어져있다. 사이킷런은 카테고리만을 위한 데이터 타입이 없으며 모두 숫자로 변환해야 한다. 하지만 이처럼 카테고리 값을 선형 회귀에 사용할 경우 회귀 계수를 연산할 때 이 숫자형 값에 크게 영향을 받는 경우가 발생할 수 있다. 따라서 선형 회귀에서는 이러한 feature 인코딩에 원-핫 인코딩을 적용해 변환해야한다. | 오랜만에 판다스의 get_dummies()를 이용해 year칼럼과 더불어 datetime칼럼 그리고 holiday 등등 모두 원-핫 인코딩 후 다시 예측 성능을 확인해보자 | . X_features_ohe = pd.get_dummies(X_features, columns=[&#39;year&#39;,&#39;month&#39;,&#39;hour&#39;, &#39;holiday&#39;, &#39;workingday&#39;,&#39;season&#39;,&#39;weather&#39;]) . X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=0) # 모델과 학습/테스트 데이터 셋을 입력하면 성능 평가 수치를 반환 def get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=False): model.fit(X_train, y_train) pred = model.predict(X_test) if is_expm1 : y_test = np.expm1(y_test) pred = np.expm1(pred) print(&#39;###&#39;,model.__class__.__name__,&#39;###&#39;) evaluate_regr(y_test, pred) # end of function get_model_predict # model 별로 평가 수행 lr_reg = LinearRegression() ridge_reg = Ridge(alpha=10) lasso_reg = Lasso(alpha=0.01) for model in [lr_reg, ridge_reg, lasso_reg]: get_model_predict(model,X_train, X_test, y_train, y_test,is_expm1=True) . ### LinearRegression ### RMSLE: 0.589, RMSE: 97.482, MAE: 63.105 ### Ridge ### RMSLE: 0.589, RMSE: 98.407, MAE: 63.648 ### Lasso ### RMSLE: 0.634, RMSE: 113.031, MAE: 72.658 . 선형 회귀에 예측 성능이 많이 향상 됐다. 원-핫 인코딩된 데이터에서 회귀 계수가 높은 feature를 다시 시각화해보자. 원-핫 인코딩으로 feature가 늘어났으므로 상위 10개만 추출해보자 | . coef = pd.Series(lr_reg.coef_ , index=X_features_ohe.columns) coef_sort = coef.sort_values(ascending=False)[:10] sns.barplot(x=coef_sort.values , y=coef_sort.index) . &lt;AxesSubplot:&gt; . 이번에는 회귀 트리를 이용해 회귀 예측을 수행해보자. 앞에서 적용한 Target 값의 로그 변환된 값과 원-핫 인코딩된 feature 데이터 세트를 그래도 이용해 여러 모델에 대한 성능을 평가해보자. | . from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor # 랜덤 포레스트, GBM, XGBoost, LightGBM model 별로 평가 수행 rf_reg = RandomForestRegressor(n_estimators=500) gbm_reg = GradientBoostingRegressor(n_estimators=500) xgb_reg = XGBRegressor(n_estimators=500) lgbm_reg = LGBMRegressor(n_estimators=500) for model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]: get_model_predict(model,X_train, X_test, y_train, y_test,is_expm1=True) . ### RandomForestRegressor ### RMSLE: 0.354, RMSE: 50.738, MAE: 31.493 ### GradientBoostingRegressor ### RMSLE: 0.340, RMSE: 55.822, MAE: 34.355 ### XGBRegressor ### RMSLE: 0.339, RMSE: 50.950, MAE: 30.891 ### LGBMRegressor ### RMSLE: 0.316, RMSE: 46.473, MAE: 28.777 . 앞의 선형 회귀 모델보다 회귀 예측 성능이 개선됐다. 하지만 이게 선형회귀 보다 회귀 트리가 더 낫다는 건 아니다. 데이터 세트의 유형에 따라 결과는 얼마든지 달라질 수 있다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/12/intro.html",
            "relUrl": "/2022/01/12/intro.html",
            "date": " • Jan 12, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "2022/01/11/TUE",
            "content": "&#45796;&#54637; &#54924;&#44480;&#50752; &#44284;(&#45824;)&#51201;&#54633;/&#44284;&#49548;&#51201;&#54633;&#51032; &#51060;&#54644; . 지금까지 설명한 회귀는 독립변수(feature)와 종속변수(target)의 관계가 일차방정식 형태로 표현된 회귀였다. 회귀가 독립변수의 단항식이 아닌, 2차 또는 3차 방정식과 같은 다항식으로 표현되는 다항 회귀에 대해 알아보자. 유의할 점은 다항회귀는 비선형 회귀가 아닌 선형회귀라는 점이다. (회귀에서 선형 회귀/ 비선형 회귀를 나누는 기준은 회귀 계수가 선형/비선형인지에 따른 것이지 독립 변수의 선형/비선형 여부와는 무관) 사이킷런은 다항 회귀를 위한 클래스를 명시적으로 제공하지 않는다. 대신 다항 회귀 역시 선형 회귀이기 때문에 비선형 함수를 선형 모델에 적용시키는 방법을 사용해 구현해보자. . 다음 예제는 PolynomialFeatures를 이용해 단항값 [$x_1,x_2$] 를 다항값 [$1 , x_1, x_2, x_1^2, x_1x_2, x_2^2$]으로 변환하는 예제이다. | . from sklearn.preprocessing import PolynomialFeatures import numpy as np # 다항식으로 변환할 단항식 생성, [[0,1],[2,3]]의 2X2 행렬 생성 X = np.arange(4).reshape(2,2) print(&#39;일차 단항식 계수 feature: n&#39;,X ) # degree = 2 인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용하여 변환 poly = PolynomialFeatures(degree=2) poly.fit(X) poly_ftr = poly.transform(X) print(&#39;변환된 2차 다항식 계수 feature: n&#39;, poly_ftr) . 일차 단항식 계수 feature: [[0 1] [2 3]] 변환된 2차 다항식 계수 feature: [[1. 0. 1. 0. 0. 1.] [1. 2. 3. 4. 6. 9.]] . 단항 계수 feature [$x_1,x_2$]를 2차 단항 계수 [$1,x_1,x_2,x_1^2,x_1x_2,x_2^2$]로 변경하므로 첫 번째 입력 단항 계수 feature[$x_1 = 0, x_2 = 1$]은 [$1, x_1=0, x_2= 1, x_1^2 = 0, x_1x_2 = 0, x_2^2 = 0$] 형태인 [$1,0,1,0,0,1$]로 변환된다. . | 두번 째 입력 단항 계수 feature [$x_1 = 2, x_2 = 3$]은 [$1, x_1=2, x_2= 3, x_1^2 = 4, x_1x_2 = 6, x_2^2 = 9$]로 변환된다. 이렇게 변환된 Polynomial feature에 선형 회귀를 적용해 다항회귀를 구현한다. Polynomial Feature 클래스가 어떻게 단항식 값을 다항식 값으로 변경하는지 설명했으니 이번에는 3차 다항 계수를 이용해 3차 다항 회귀 함수식을 PolynomialFeatures와 LinearRegression 클래스를 이용해 유도해보자. . | . . 3차 다항 회귀 함수를 임의로 설정하고 이의 회귀 계수를 예측하자. 먼저 3차 다항 회귀의 결정 함수식은 다음과 같이 $y=1+2x_1+3x_1^2+4x_2^3$로 설정하고 이를 위한 함수 polynomial_func()를 만들자. 해당 함수는 다항 계수 feature값이 입력되면 결정 값을 반환한다. | . def polynomial_func(X): y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 return y X = np.arange(0,4).reshape(2,2) print(&#39;일차 단항식 계수 feature: n&#39; ,X) y = polynomial_func(X) print(&#39;삼차 다항식 결정값: n&#39;, y) . 일차 단항식 계수 feature: [[0 1] [2 3]] 삼차 다항식 결정값: [ 5 125] . 이제 일차 단항식 계수를 삼차 다항식 계수로 변환하고, 이를 선형 회귀에 적용하면 다항 회귀로 구현된다. | PolynomialFeatures(degree=3)은 단항 계수 feature[$x_1,x_2$]를 3차 다항 계수[$1,x_1,x_2,x_1^2,x_1x_2,x_2^2,x_1^3,x_1^2x_2,x_1x_2^2,x_1^3$]과 같이 10개의 다항 계수로 변환한다. | . poly_ftr = PolynomialFeatures(degree=3).fit_transform(X) print(&#39;3차 다항식 계수 feature: n&#39;,poly_ftr) # Linear Regression에 3차 다항식 계수 feature와 3차 다항식 결정값으로 학습 후 회귀 계수 확인 from sklearn.linear_model import LinearRegression model = LinearRegression() model.fit(poly_ftr,y) print(&#39;Polynomial 회귀 계수 n&#39; , np.round(model.coef_, 2)) print(&#39;Polynomial 회귀 Shape :&#39;, model.coef_.shape) . 3차 다항식 계수 feature: [[ 1. 0. 1. 0. 0. 1. 0. 0. 0. 1.] [ 1. 2. 3. 4. 6. 9. 8. 12. 18. 27.]] Polynomial 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] Polynomial 회귀 Shape : (10,) . 일차 단항식 계수 feature는 2개였지만, 3차 다항식 Polynomial 변환 이후에는 다항식 계수 feature가 10개로 늘어난다. 이 feature데이터 세트에 LinearRegression을 통해 3차 다항 회귀 형태의 다항 회귀를 적용하면 회귀 계수가 10개로 늘어난다. 10개의 회귀계수는 [$0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34$]가 도출됐으며 원래 다항식 $y=1+2x_1+3x_1^2+4x_2^3$ 계수값인 [$1,2,0,3,0,0,0,0,0,4$]롸는 차이가 있지만 다항 회귀로 근사하고 있음을 알 수 있다. 이처럼 사이킷런은 PolynomialFeatures로 feature를 변환한 후에 LinearRegression클래스로 다항 회귀를 구현한다. | . 전처럼 feature 변환과 선형 회귀 적용을 각각 별도로 하는 것보다는 사이킷런의 Pipeline객체를 이용해 한 번에 다항 회귀를 구현해보자 | . from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline import numpy as np def polynomial_func(X): y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 return y # Pipeline 객체로 Streamline 하게 Polynomial Feature변환과 Linear Regression을 연결 model = Pipeline([(&#39;poly&#39;, PolynomialFeatures(degree=3)), (&#39;linear&#39;, LinearRegression())]) X = np.arange(4).reshape(2,2) y = polynomial_func(X) model = model.fit(X, y) print(&#39;Polynomial 회귀 계수 n&#39;, np.round(model.named_steps[&#39;linear&#39;].coef_, 2)) . Polynomial 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] . &#45796;&#54637; &#54924;&#44480;&#47484; &#51060;&#50857;&#54620; &#44284;&#49548;&#51201;&#54633; &#48143; &#44284;&#51201;&#54633; &#51060;&#54644; . 다항 회귀는 feature의 직선적 관계가 아닌 복잡한 다항 관계를 모델링할 수 있다. 다항식의 차수가 높아질수록 매우 복잡한 feature간의 관계까지 모델링이 가능하다. 하지만 다항 회귀의 차수(degree)를 높일수록 학습 데이터에만 너무 맞춘 학습이 이뤄져서 정작 테스트 데이터 환경에서는 오히려 예측 정확도가 떨어진다. 즉 차수가 높아질수록 과적합의 문제가 크게 발생. . 그 예를 살펴보자 | . 다항 회귀의 차수를 변화시키면서 그에 따른 회귀 예측 곡선과 예측 정확도를 비교해보자 | . import numpy as np import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score %matplotlib inline # random 값으로 구성된 X값에 대해 Cosine 변환값을 반환. def true_fun(X): return np.cos(1.5 * np.pi * X) # X는 0 부터 1까지 30개의 random 값을 순서대로 sampling 한 데이타 입니다. np.random.seed(0) n_samples = 30 X = np.sort(np.random.rand(n_samples)) # y 값은 cosine 기반의 true_fun() 에서 약간의 Noise 변동값을 더한 값입니다. y = true_fun(X) + np.random.randn(n_samples) * 0.1 . plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x26dcffe9910&gt; . 이제 예측 결과를 비교할 다항식 차수를 1,4,15로 변경하며서 예측 결과를 비교해보자 | 다항식 차수별로 학습을 수행한 뒤 cross_val_score()로 MSE 값을 수해 차수별 예측 성능을 평가한다. | 그리고 0부터 1까지 균일하게 구성된 100개의 테스트용 데이터 세트를 이용해 차수별 회귀 예측 곡선을 그려보자 | . plt.figure(figsize=(14, 5)) degrees = [1, 4, 15] # 다항 회귀의 차수(degree)를 1, 4, 15로 각각 변화시키면서 비교합니다. for i in range(len(degrees)): ax = plt.subplot(1, len(degrees), i + 1) plt.setp(ax, xticks=(), yticks=()) # 개별 degree별로 Polynomial 변환합니다. polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) linear_regression = LinearRegression() pipeline = Pipeline([(&quot;polynomial_features&quot;, polynomial_features), (&quot;linear_regression&quot;, linear_regression)]) pipeline.fit(X.reshape(-1, 1), y) # 교차 검증으로 다항 회귀를 평가합니다. scores = cross_val_score(pipeline, X.reshape(-1,1), y,scoring=&quot;neg_mean_squared_error&quot;, cv=10) coefficients = pipeline.named_steps[&#39;linear_regression&#39;].coef_ print(&#39; nDegree {0} 회귀 계수는 {1} 입니다.&#39;.format(degrees[i], np.round(coefficients),2)) print(&#39;Degree {0} MSE 는 {1:.2f} 입니다.&#39;.format(degrees[i] , -1*np.mean(scores))) # 0 부터 1까지 테스트 데이터 세트를 100개로 나눠 예측을 수행합니다. # 테스트 데이터 세트에 회귀 예측을 수행하고 예측 곡선과 실제 곡선을 그려서 비교합니다. X_test = np.linspace(0, 1, 100) # 예측값 곡선 plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=&quot;Model&quot;) # 실제 값 곡선 plt.plot(X_test, true_fun(X_test), &#39;--&#39;, label=&quot;True function&quot;) plt.scatter(X, y, edgecolor=&#39;b&#39;, s=20, label=&quot;Samples&quot;) plt.xlabel(&quot;x&quot;); plt.ylabel(&quot;y&quot;); plt.xlim((0, 1)); plt.ylim((-2, 2)); plt.legend(loc=&quot;best&quot;) plt.title(&quot;Degree {} nMSE = {:.2e}(+/- {:.2e})&quot;.format(degrees[i], -scores.mean(), scores.std())) plt.show() . Degree 1 회귀 계수는 [-2.] 입니다. Degree 1 MSE 는 0.41 입니다. Degree 4 회귀 계수는 [ 0. -18. 24. -7.] 입니다. Degree 4 MSE 는 0.04 입니다. Degree 15 회귀 계수는 [-2.98300000e+03 1.03899000e+05 -1.87415800e+06 2.03715960e+07 -1.44873157e+08 7.09315008e+08 -2.47065753e+09 6.24561150e+09 -1.15676562e+10 1.56895047e+10 -1.54006170e+10 1.06457389e+10 -4.91378211e+09 1.35919860e+09 -1.70381087e+08] 입니다. Degree 15 MSE 는 182493841.87 입니다. . 실선으로 표현된 예측 곡선은 다항 회귀 예측 곡선이며 점선으로 표현된 곡선은 실제 데이터 세트 X,Y,코사인 곡선이다. 학습 데이터는 0부터 1까지의 30개의 임의의 X값과 그에 따른 코사인 Y값에 잡음을 변동 값으로 추가해 구성했으며 MSE 평가는 학습 데이터를 10개의 교차 검증 세트로 나누어 측정해서 평균한 것이다. | . 세 그래프에서 볼 수 있듯이 가운데 Degree=4 예측 곡선이 실제 데이터 세트와 가장 유사함을 알 수 있다. 변동하는 잡음까진 예측하지 못하지만 MSE=0.043으로 가장 뛰어난 예측 성능을 나타냄 | Degree=15 예측곡선은 MSE값이 182493841이 될 정도로 어처구니 없는 오류가 발생. 예측 곡선을 보면 데이터 세트의 변동 잡음값까지 지나치게 반영한 결과 (과적합)예측 곡선이 학습 데이터세트만 정확히 예측하고 테스트값의 실제 곡선과는 완전히 다른 형태의 곡선이 만들어졌음. 결과적으로 학습 데이터에 너무 치중한 나머지 과적합이 심한 모델이 되었고 어이없는 수준의 MSE가 발생됐음. | . $ to$ 즉 좋은 예측 모델은 학습 데이터의 패턴을 지나치게 단순화한 과소적합 모델(Degree=1)도 아니고 학습 데이터의 패턴을 하나하나 감안한 지나치게 복잡한 과적합 모델(Degree=15)도 아닌 학습 데이터의 패턴을 잘 반영하면서도 복잡하지 않은 균형 잡힌 모델이다. . &#54200;&#54693;-&#48516;&#49328; &#53944;&#47112;&#51060;&#46300;&#50724;&#54532; . 머신러닝이 극복해야 할 가장 중요한 이슈 중 하나이다. 앞의 Degree=1 모델은 매우 단순화된 모델로서 지나치게 한 방향으로 치우친 경향이 있는 고편향(High Bias)성을 지님. 반대로 Degree=15과 같은 모델은 학습 데이터 하나하나의 특성을 반영하면서 매우 복잡한 모델이 되었고 지나치게 높은 변동성을 갖게 되었음. 즉 고분산(High Variance)성을 가졌다. . 일반적으로 편향과 분산은 한 쪽이 높으면 한 쪽이 낮아지는 경향이 있다. 즉, 편향이 높으면 분산은 낮아지고 반대로 분산이 높으면 편향이 낮아진다. 편향이 너무 높으면 전체 오류가 높으며 편향을 점점 낮추면 동시에 분산이 높아지고 전체 오류도 낮아진다. 편향을 낮추고 분산을 높이면서 전체오류가 가장 낮은 골디락스지점을 통과하면서 분산을 지속적으로 높이면 전체 오류값이 다시 증가하면서 예측 성능이 다시 저하된다. . | 높은 평향 낮은 분산에서 과소적합되기 쉬우며 낮은 평향 높은 분산에서 과적합되기 쉽다. 편향과 분산이 서로 트레이드오프를 이루면서 오류 Cost값이 최대로 낮아지는 모델을 구축하는 것이 가장 효율적인 머신러닝 예측 모델일 것이다. . | . &#44508;&#51228; &#49440;&#54805; &#47784;&#45944; - &#47551;&#51648;, &#46972;&#50136;, &#50648;&#47532;&#49828;&#54001;&#45367; . 회귀 모델은 적절히 데이터에 적합하면서도 회귀 계수가 기하급수적으로 커지는 것을 제어할 수 있어야한다. 이전까지 선형 모델의 비용함수는 RSS를 최소화하는, 즉 실제값과 예측값의 차이를 최소화하는 것만 고려했고 이는 학습 데이터에 지나치게 맞추게 되고 회귀계수가 쉽게 커짐. 이럴 경우 변동성은 높아지고 테스트 데이터 세트에서는 예측 성능이 저하되기 쉽다. 이를 반영해 비용함수는 학습 데이터의 잔차 오류값을 최소로 하는 RSS최소화 방법과 과접합을 방지하기 위해 계수 값이 커지지 않도록 하는 방법이 서로 균형을 이뤄야한다. . 320p~321p 참고! | 이를 요약하면 $alpha$값으로 페널티를 부여해 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식을 규제(Regularization)라고 부른다. 규제는 크게 L1,L2방식이 있다. L2 규제는 회귀 계수의 제곱에 대해 패널티를 부여(릿지 회귀), L1 규제는 릿쏘 회귀로서 회귀계수의 절댓값에 패널티를 부여한다. L1 규제를 적용하면 영향력이 크지 않은 회귀 계수 값을 0으로 변환한다. | . 릿지 회귀 . from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # boston 데이타셋 로드 boston = load_boston() # boston 데이타셋 DataFrame 변환 bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names) # boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. bostonDF[&#39;PRICE&#39;] = boston.target print(&#39;Boston 데이타셋 크기 :&#39;,bostonDF.shape) y_target = bostonDF[&#39;PRICE&#39;] X_data = bostonDF.drop([&#39;PRICE&#39;],axis=1,inplace=False) ridge = Ridge(alpha = 10) neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(&#39; 5 folds 의 개별 Negative MSE scores: &#39;, np.round(neg_mse_scores, 3)) print(&#39; 5 folds 의 개별 RMSE scores : &#39;, np.round(rmse_scores,3)) print(&#39; 5 folds 의 평균 RMSE : {0:.3f} &#39;.format(avg_rmse)) . Boston 데이타셋 크기 : (506, 14) 5 folds 의 개별 Negative MSE scores: [-11.422 -24.294 -28.144 -74.599 -28.517] 5 folds 의 개별 RMSE scores : [3.38 4.929 5.305 8.637 5.34 ] 5 folds 의 평균 RMSE : 5.518 . 규제가 없는 LinearRegression의 RMSE 평균보다 뛰어난 예측 성능을 보임 | alpha 값을 변화시키며 관찰해보자 - 회귀 계수값이 작아질 것임. | . alphas = [0 , 0.1 , 1 , 10 , 100] # alphas list 값을 iteration하면서 alpha에 따른 평균 rmse 구함. for alpha in alphas : ridge = Ridge(alpha = alpha) #cross_val_score를 이용하여 5 fold의 평균 RMSE 계산 neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores)) print(&#39;alpha {0} 일 때 5 folds 의 평균 RMSE : {1:.3f} &#39;.format(alpha,avg_rmse)) . alpha 0 일 때 5 folds 의 평균 RMSE : 5.829 alpha 0.1 일 때 5 folds 의 평균 RMSE : 5.788 alpha 1 일 때 5 folds 의 평균 RMSE : 5.653 alpha 10 일 때 5 folds 의 평균 RMSE : 5.518 alpha 100 일 때 5 folds 의 평균 RMSE : 5.330 . alpha가 100일 때 평균 RMSE가 가장 좋다. | 이번에는 alpha값의 변화에 따른 feature의 회귀 계수 값을 시각화해보자 | . import seaborn as sns # 각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성 fig , axs = plt.subplots(figsize=(18,6) , nrows=1 , ncols=5) # 각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 DataFrame 생성 coeff_df = pd.DataFrame() # alphas 리스트 값을 차례로 입력해 회귀 계수 값 시각화 및 데이터 저장. pos는 axis의 위치 지정 for pos , alpha in enumerate(alphas) : ridge = Ridge(alpha = alpha) ridge.fit(X_data , y_target) # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. coeff = pd.Series(data=ridge.coef_ , index=X_data.columns ) colname=&#39;alpha:&#39;+str(alpha) coeff_df[colname] = coeff # 막대 그래프로 각 alpha 값에서의 회귀 계수를 시각화. 회귀 계수값이 높은 순으로 표현 coeff = coeff.sort_values(ascending=False) axs[pos].set_title(colname) axs[pos].set_xlim(-3,6) sns.barplot(x=coeff.values , y=coeff.index, ax=axs[pos]) # for 문 바깥에서 맷플롯립의 show 호출 및 alpha에 따른 피처별 회귀 계수를 DataFrame으로 표시 plt.show() . alpha 값이 커질수록 회귀 계수 값은 작아짐을 알 수 있다. 특히 NOX feature의 경우 alpha 값을 계속 증가시킴에 따라 회귀 계수가 크게 작아지고 있다. | Dataframe에 저장된 alpha 값의 변화에 따른 릿지 회귀 계수 값을 구해보자 | . ridge_alphas = [0 , 0.1 , 1 , 10 , 100] sort_column = &#39;alpha:&#39;+str(ridge_alphas[0]) coeff_df.sort_values(by=sort_column, ascending=False) . alpha:0 alpha:0.1 alpha:1 alpha:10 alpha:100 . RM 3.809865 | 3.818233 | 3.854000 | 3.702272 | 2.334536 | . CHAS 2.686734 | 2.670019 | 2.552393 | 1.952021 | 0.638335 | . RAD 0.306049 | 0.303515 | 0.290142 | 0.279596 | 0.315358 | . ZN 0.046420 | 0.046572 | 0.047443 | 0.049579 | 0.054496 | . INDUS 0.020559 | 0.015999 | -0.008805 | -0.042962 | -0.052826 | . B 0.009312 | 0.009368 | 0.009673 | 0.010037 | 0.009393 | . AGE 0.000692 | -0.000269 | -0.005415 | -0.010707 | 0.001212 | . TAX -0.012335 | -0.012421 | -0.012912 | -0.013993 | -0.015856 | . CRIM -0.108011 | -0.107474 | -0.104595 | -0.101435 | -0.102202 | . LSTAT -0.524758 | -0.525966 | -0.533343 | -0.559366 | -0.660764 | . PTRATIO -0.952747 | -0.940759 | -0.876074 | -0.797945 | -0.829218 | . DIS -1.475567 | -1.459626 | -1.372654 | -1.248808 | -1.153390 | . NOX -17.766611 | -16.684645 | -10.777015 | -2.371619 | -0.262847 | . alpha 값이 증가하면서 회귀 계수가 지속적으로 작아지고 있음을 알 수 있다. 하지만 릿지 회귀의 경우에는 회귀 계수를 0으로 만들진 않는다. | . 라쏘 회귀 . 유의 : L1 규제는 불필요한 회귀 계수를 급격하게 감소시켜 0으로 만들고 제거한다. 이러한 측면에서 L1 규제는 적절한 feature만 회귀에 포함시키는 feature 선택의 특성을 갖고 있다. | . from sklearn.linear_model import Lasso, ElasticNet # alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환 def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True): coeff_df = pd.DataFrame() if verbose : print(&#39;####### &#39;, model_name , &#39;#######&#39;) for param in params: if model_name ==&#39;Ridge&#39;: model = Ridge(alpha=param) elif model_name ==&#39;Lasso&#39;: model = Lasso(alpha=param) elif model_name ==&#39;ElasticNet&#39;: model = ElasticNet(alpha=param, l1_ratio=0.7) neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores)) print(&#39;alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.3f} &#39;.format(param, avg_rmse)) # cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출 model.fit(X_data , y_target) # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. coeff = pd.Series(data=model.coef_ , index=X_data.columns ) colname=&#39;alpha:&#39;+str(param) coeff_df[colname] = coeff return coeff_df # end of get_linear_regre_eval . lasso_alphas = [ 0.07, 0.1, 0.5, 1, 3] coeff_lasso_df =get_linear_reg_eval(&#39;Lasso&#39;, params=lasso_alphas, X_data_n=X_data, y_target_n=y_target) . ####### Lasso ####### alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 5.612 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.615 alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 5.669 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.776 alpha 3일 때 5 폴드 세트의 평균 RMSE: 6.189 . alpha가 0.07일 때 가장 좋은 평균RMSE를 보인다. 앞의 릿지일때보단 저하됐다. 그렇지만 규제가 없을 때보단 낫다. | . sort_column = &#39;alpha:&#39;+str(lasso_alphas[0]) coeff_lasso_df.sort_values(by=sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.789725 | 3.703202 | 2.498212 | 0.949811 | 0.000000 | . CHAS 1.434343 | 0.955190 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.270936 | 0.274707 | 0.277451 | 0.264206 | 0.061864 | . ZN 0.049059 | 0.049211 | 0.049544 | 0.049165 | 0.037231 | . B 0.010248 | 0.010249 | 0.009469 | 0.008247 | 0.006510 | . NOX -0.000000 | -0.000000 | -0.000000 | -0.000000 | 0.000000 | . AGE -0.011706 | -0.010037 | 0.003604 | 0.020910 | 0.042495 | . TAX -0.014290 | -0.014570 | -0.015442 | -0.015212 | -0.008602 | . INDUS -0.042120 | -0.036619 | -0.005253 | -0.000000 | -0.000000 | . CRIM -0.098193 | -0.097894 | -0.083289 | -0.063437 | -0.000000 | . LSTAT -0.560431 | -0.568769 | -0.656290 | -0.761115 | -0.807679 | . PTRATIO -0.765107 | -0.770654 | -0.758752 | -0.722966 | -0.265072 | . DIS -1.176583 | -1.160538 | -0.936605 | -0.668790 | -0.000000 | . alpha의 크기가 증가함에 따라 일부 feature의 회귀 계수는 아예 0으로 바뀌고 있음을 알 수 있다. 이때 회귀 계수가 0인 feature는 회귀 식에서 제외되면서 feature선택의 효과를 얻을 수 있다. | . 엘라스틱넷 회귀 . L2와 L1규제를 결합한 회귀. 엘라스틱넷은 라쏘 회귀가 서로 상관관계가 높은 feature들의 경우에 이들 중요서 중요 feature만을 selection하고 다른 feature들은 모두 회귀 계수를 0으로 만드는 경향이 강하다. 이러한 성향으로 인해 alpha값에 따라 회귀 계수의 값이 급격히 변동할 수도 있는데 엘라스틱넷 회귀는 이를 완화하기 위해 L2 규제를 라쏘회귀에 추가한 것. 반대로 엘라스틱넷 회귀의 단점은 L1과 L2 규제가 결합된 규제로 인해 수행시간이 다소 오래 걸린다는 점이 있다. | 유의해야할 점 : ElasticNet 클래스에서의 alpha 파라미터 값은 L1과 L2규제의 alpha값인 a와 b를 더한 a+b값을 의미한다. ElasticNet 클래스의 11_ratio 파라미터 값은 a/(a+b)이다. 11_ratio가 0이면 a가 0이므로 L2규제와 동일, 1이면 L1규제와 동일하다. | . # l1_ratio는 0.7로 고정 elastic_alphas = [ 0.07, 0.1, 0.5, 1, 3] coeff_elastic_df =get_linear_reg_eval(&#39;ElasticNet&#39;, params=elastic_alphas, X_data_n=X_data, y_target_n=y_target) . ####### ElasticNet ####### alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 5.542 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.526 alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 5.467 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.597 alpha 3일 때 5 폴드 세트의 평균 RMSE: 6.068 . sort_column = &#39;alpha:&#39;+str(elastic_alphas[0]) coeff_elastic_df.sort_values(by=sort_column, ascending=False) . alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 . RM 3.574162 | 3.414154 | 1.918419 | 0.938789 | 0.000000 | . CHAS 1.330724 | 0.979706 | 0.000000 | 0.000000 | 0.000000 | . RAD 0.278880 | 0.283443 | 0.300761 | 0.289299 | 0.146846 | . ZN 0.050107 | 0.050617 | 0.052878 | 0.052136 | 0.038268 | . B 0.010122 | 0.010067 | 0.009114 | 0.008320 | 0.007020 | . AGE -0.010116 | -0.008276 | 0.007760 | 0.020348 | 0.043446 | . TAX -0.014522 | -0.014814 | -0.016046 | -0.016218 | -0.011417 | . INDUS -0.044855 | -0.042719 | -0.023252 | -0.000000 | -0.000000 | . CRIM -0.099468 | -0.099213 | -0.089070 | -0.073577 | -0.019058 | . NOX -0.175072 | -0.000000 | -0.000000 | -0.000000 | -0.000000 | . LSTAT -0.574822 | -0.587702 | -0.693861 | -0.760457 | -0.800368 | . PTRATIO -0.779498 | -0.784725 | -0.790969 | -0.738672 | -0.423065 | . DIS -1.189438 | -1.173647 | -0.975902 | -0.725174 | -0.031208 | . alpha가 0.5일 때 RMSE가 5.468로 가장 좋은 예측 성능을 보이고 있다. alpha값에 따른 feature들의 회귀 계수들의 값이 라쏘보다는 상대적으로 0이 되는 값이 적음을 알 수 있다. | 지금까지 규제 선형 회귀의 가장 대표적인 기법인 릿지, 라쏘, 엘라스틱넷 회귀를 살펴봤다. 이들 중 어떤 것이 가장 좋은지는 상황에 따라 다르므로 각각의 알고리즘에서 하이퍼 파라미터를 변경해 가면서 최적의 예측 성능을 찾아내야 한다. 하지만 선형 회귀의 경우 최적의 하이퍼 파라미터를 찾아내는 것 못지않게 먼저 데이터 분포도의 정규화와 인코딩 방법이 매우 중요하다. | . 선형 회귀 모델을 위한 데이터 변환 . 선형 회귀 모델과 같은 선형 모델은 일반적으로 feature와 target값 간에 선형의 관계가 있다고 가정하고 이러한 최적의 선형함수를 찾아내 결과값을 예측한다. 또한 선형 회귀 모델을 적용하기 전에 먼저 데이터에 대한 스케일링/정규화 작업을 수행하는 것이 일반적이다. 이러한 스케일링과 정규화 작업이 무조건 성능향상으로 이어지는 것은 아니지만 중요 feature들이나 target값의 분포도가 심하게 왜곡됐을 경우에 이러한 변환 작업을 수행한다. . | 일반적으로 feature 데이터 세트와 target 데이터 세트에 적용하는 스케일링/정규화 방법이 상이하다. . feature 데이터 세트에는 StandardScaler 클래스를 이용해 표준 정규 분포를 가진 데이터 세트로 변환하거나 MinMaxScaler 클래스를 이용해 최솟값이 0이고 최댓값이 1인 값으로 정규화를 수행한다. 이때 성능 향상이 없다면 다항 특성을 적용하여 변환할 수 있으며 또는 로그 변환을 통해 변환을 수행할 수 있다. 로그 변환이 가장 유용한 방법이다. 타깃값의 경우엔 일반적으로 로그 변환을 수행한다. | . | . 보스턴 주택가격 feature 데이터 세트에 위에서 언급한 표준 정규 분포 변환, 최댓값/최솟값 정규화, 로그 변환을 차례로 적용한 후에 예측 성능을 평가해보자 | . from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures # method는 표준 정규 분포 변환(Standard), 최대값/최소값 정규화(MinMax), 로그변환(Log) 결정 # p_degree는 다향식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음. def get_scaled_data(method=&#39;None&#39;, p_degree=None, input_data=None): if method == &#39;Standard&#39;: scaled_data = StandardScaler().fit_transform(input_data) elif method == &#39;MinMax&#39;: scaled_data = MinMaxScaler().fit_transform(input_data) elif method == &#39;Log&#39;: scaled_data = np.log1p(input_data) else: scaled_data = input_data if p_degree != None: scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data) return scaled_data . 이제 Ridge 클래스의 alpha값을 변화시키면서 feature 데이터 세트를 여러가지 방법으로 변환한 데이터 세트를 입력 받을 경우에 RMSE값이 어떻게 변하는지 살펴보자 . alphas = [0.1, 1, 10, 100] #변환 방법은 모두 6개, 원본 그대로, 표준정규분포, 표준정규분포+다항식 특성 # 최대/최소 정규화, 최대/최소 정규화+다항식 특성, 로그변환 scale_methods=[(None, None), (&#39;Standard&#39;, None), (&#39;Standard&#39;, 2), (&#39;MinMax&#39;, None), (&#39;MinMax&#39;, 2), (&#39;Log&#39;, None)] for scale_method in scale_methods: X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=X_data) print(&#39; n## 변환 유형:{0}, Polynomial Degree:{1}&#39;.format(scale_method[0], scale_method[1])) get_linear_reg_eval(&#39;Ridge&#39;, params=alphas, X_data_n=X_data_scaled, y_target_n=y_target, verbose=False) . ## 변환 유형:None, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.788 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.653 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.518 alpha 100일 때 5 폴드 세트의 평균 RMSE: 5.330 ## 변환 유형:Standard, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.826 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.803 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.637 alpha 100일 때 5 폴드 세트의 평균 RMSE: 5.421 ## 변환 유형:Standard, Polynomial Degree:2 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 8.827 alpha 1일 때 5 폴드 세트의 평균 RMSE: 6.871 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.485 alpha 100일 때 5 폴드 세트의 평균 RMSE: 4.634 ## 변환 유형:MinMax, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.764 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.465 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.754 alpha 100일 때 5 폴드 세트의 평균 RMSE: 7.635 ## 변환 유형:MinMax, Polynomial Degree:2 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.298 alpha 1일 때 5 폴드 세트의 평균 RMSE: 4.323 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.185 alpha 100일 때 5 폴드 세트의 평균 RMSE: 6.538 ## 변환 유형:Log, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 4.770 alpha 1일 때 5 폴드 세트의 평균 RMSE: 4.676 alpha 10일 때 5 폴드 세트의 평균 RMSE: 4.836 alpha 100일 때 5 폴드 세트의 평균 RMSE: 6.241 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/11/intro.html",
            "relUrl": "/2022/01/11/intro.html",
            "date": " • Jan 11, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "2022/01/10/MON",
            "content": "&#54924;&#44480; . 회귀 분석은 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향으르 이용한 통계학 기법이다. 회귀는 여러 개의 독립 변수와 한 개의 종속 변수 간의 상관관계를 모델링하는 기법을 통칭한다. 예를 들면, 아파트의 방 개수, 방 크기, 주변 학군, 등 여러 개의 독립변수에 따라 아파트 가격이라는 종속변수가 어떤 관계를 나타내는지를 모델링하고 예측하는 것이다. $Y = W_1*X_1 + W_2*X_2 + W_3*X_3 + dots + W_n*X_n$ 이라는 선형 회귀식을 예로 들면 $Y$는 종속변수, 즉 아파트 가격을 의미하며, 나머지 $X_1,X_2,X_3, dots,X_n$은 방 개수, 방 크기, 주변 학군 등의 독립 변수를 의미한다. 그리고 $W_1,W_2,W_3, dots,W_n$은 이 독립변수의 값에 영향을 미치는 회귀 계수이다. 머신 러닝의 관점에서 보면 독립변수는 feature에 해당하며 종속변수는 결정 값, 즉 레이블을 의미한다. 머신러닝 회귀 예측의 핵심은 주어진 feature와 결정 값 데이터 기반에서 학습을 통해 최적의 회귀계수를 찾아내는 것이다. . &#54924;&#44480; &#51333;&#47448; . 독립 변수의 개수가 1개이면 단일 회귀, 여러 개이면 다중 회귀이다. 또한 회귀계수의 결합이 선형이면 선형 회귀, 비선형이면 비선형 회귀이다. . 지도학습은 두 가지 유형으로 나뉘는데 바로 분류와 회귀이다. 이 두가지 기법의 가장 큰 차이는 분류는 예측값이 카테고리와 같은 이산형 클래스 값이고 회귀는 연속형 숫자값이라는 것이다. | 여러 가지 회귀 중에서 선형 회귀가 가장 많이 사용된다. 선형 회귀는 실제 값과 예측값의 차이(오류의 제곱 값)를 최소화하는 직선형 회귀선을 최적화하는 방식이다. 선형 회귀 모델은 규제 방법에 따라 또 나눌 수 있다. 여기서 규제란 일반적인 선형 회귀의 과적합 문제를 해결하기 위해서 회귀 계수에 페널티 값을 적용하는 것을 의미한다. | . . 단순 선형 회귀에 대해 알아보자 : 독립 변수도 하나 종속 변수도 하나인 선형 회귀를 의미한. 예를 들면 주택 가격이 주택의 크기로만 결정되는 것. | 예측값 $ hat{Y}$는 $w_0 + w_1*X$로 계산할 수 있다. 독립변수가 1개인 단순 선형 회귀에서는 이 기울기 $w_1$과 절편 $w_0$을 회귀 계수로 지칭한다. 그리고 회귀 모델을 $ hat{Y} = w_0 + w_1*X$와 같은 1차 함수로 모델링했다면 실제 주택 가격은 이러한 1차 함수 값에서 실제 값만큼의 오류 값을 뺀 또는 더한 값이 된다. ($w_0 + w_1*X +$ 오류값) | 이렇게 실제 값과 회귀 모델의 차이에 따른 오류 값을 남은 오류, 즉 잔차라고 부른다. 최적의 회귀 모델을 만든다는 것이 바로 전체 데이터의 잔차 합이 최소가 되는 모델을 만든다는 것이며 상쇄될 것을 고려해 대개 절댓값을 취하거나 제곱을 한 뒤 오류 합을 구한다. $Error^2 = RSS $ | RSS는 비용이며 $w$변수(회귀 계수)로 구성되는 $RSS$를 비용 함수라고 한다. 머신 러닝 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값(즉, 오류값)을 지속해서 감소시키고 최종적으로는 더 이상 감소하지 않는 최소의 오류값을 구하는 것이다. 비용함수를 손실함수라고도 한다. | . . &#44221;&#49324;&#54616;&#44053;&#48277; . 점진적으로 반복적인 계산을 통해 $W$ 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 $W$ 파라미터를 구하는 방식이다. 경사 하강법은 반복적으로 비용 함수의 반환 값 즉, 예측값과 실제 값의 차이가 작아지는 방향성을 가지고 $W$ 파라미터를 지속해서 보정해 나간다. 오류를 감소시키는 방향으로 $W$값을 계속 업데이트해 나가면서 더 이상 그 오류 값이 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그때의 $W$ 값을 최적 파라미터로 반환한다. . 예를 들어 비용 함수가 포물선 형태의 2차 함수라면 경사 하강법은 최초 $w$에서부터 미분을 적용한 뒤 이 미분 값이 계속 감소하는 방향으로 순차적으로 $w$를 업데이트한다. 마침내 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그때의 $w$를 반환한다. | . 경사 하강법을 파이썬 코드로 구현해보자 | . import numpy as np import matplotlib.pyplot as plt np.random.seed(0) # y = 4X + 6 식을 근사(w1=4, w0=6). random 값은 Noise를 위해 만듬 X = 2 * np.random.rand(100,1) # X의 범위 설정 y = 6 + 4 * X + np.random.randn(100,1) # X, y 데이터 셋 scatter plot으로 시각화 plt.scatter(X, y) . &lt;matplotlib.collections.PathCollection at 0x1b994b27550&gt; . def get_weight_updates(w1, w0, X, y, learning_rate=0.01): N = len(y) # 먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화 w1_update = np.zeros_like(w1) w0_update = np.zeros_like(w0) # 예측 배열 계산하고 예측과 실제 값의 차이 계산 y_pred = np.dot(X, w1.T) + w0 diff = y-y_pred # w0_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성 w0_factors = np.ones((N,1)) # w1과 w0을 업데이트할 w1_update와 w0_update 계산 w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff)) w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff)) return w1_update, w0_update . def gradient_descent_steps(X, y, iters=10000): # w0와 w1을 모두 0으로 초기화. w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) # 인자로 주어진 iters 만큼 반복적으로 get_weight_updates() 호출하여 w1, w0 업데이트 수행. for ind in range(iters): w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . def get_cost(y, y_pred): N = len(y) cost = np.sum(np.square(y - y_pred))/N return cost w1, w0 = gradient_descent_steps(X, y, iters=1000) print(&quot;w1:{0:.3f} w0:{1:.3f}&quot;.format(w1[0,0], w0[0,0])) y_pred = w1[0,0] * X + w0 print(&#39;Gradient Descent Total Cost:{0:.4f}&#39;.format(get_cost(y, y_pred))) . w1:4.022 w0:6.162 Gradient Descent Total Cost:0.9935 . y_pred에 기반해 회귀선을 그려보자 . plt.scatter(X, y) plt.plot(X,y_pred) . [&lt;matplotlib.lines.Line2D at 0x1b9980f5940&gt;] . . 경사 하강법을 이용해 회귀선이 잘 만들어졌음을 알 수 있다. 일반적으로 경사 하강법은 모든 학습 데이터에 대해 반복적으로 비용함수 최소화를 위한 값을 업데이트하기 때문에 수행 시간이 매우 오래 걸린다. 이 때문에 실전에서는 확률적 경사 하강법(Stochastic Gradient Descent)을 이용한다. 확률적 경사 하강법을 일부 데이터만 이용해 w가 업데이트되는 값을 계산하므로 경사하강법에 비해 빠른 속도를 보장한다. 해보자. | . def stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) prev_cost = 100000 iter_index =0 for ind in range(iters): np.random.seed(ind) # 전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터 추출하여 sample_X, sample_y로 저장 stochastic_random_index = np.random.permutation(X.shape[0]) sample_X = X[stochastic_random_index[0:batch_size]] sample_y = y[stochastic_random_index[0:batch_size]] # 랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update, w0_update 계산 후 업데이트 w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 . 만들어진 함수를 이용해 w1,w0 및 예측 오류 비용을 계산해 보자 . w1, w0 = stochastic_gradient_descent_steps(X, y, iters=1000) print(&quot;w1:&quot;,round(w1[0,0],3),&quot;w0:&quot;,round(w0[0,0],3)) y_pred = w1[0,0] * X + w0 print(&#39;Stochastic Gradient Descent Total Cost:{0:.4f}&#39;.format(get_cost(y, y_pred))) . w1: 4.028 w0: 6.156 Stochastic Gradient Descent Total Cost:0.9937 . 확률적 경사 하강법으로 구한 w0,w1 결과는 경사 하강법으로 구한 w1,w0과 큰 차이가 없으며 예측 오류 비용 또한 경사하강법으로 구한 것보다 아주 조금 높다. 즉 성능 차이가 미미하다. 데이터가 크면 확률적 경사 하강법을 이용하라. | . . &#49324;&#51060;&#53431;&#47088; LinearRegression&#51012; &#51060;&#50857;&#54620; &#48372;&#49828;&#53556; &#51452;&#53469; &#44032;&#44201; &#50696;&#52769; . LinearRegression 클래스는 예측값과 실제 값의 RSS를 최소화 해 OLS 추정 방식으로 구현한 클래스이다. . OLS 기반의 회귀 계수 계산은 입력 feature의 독립성에 많은 영향을 받는다. feature간의 상관관계가 매우 높은 경우 분산이 매우 커져서 오류에 매우 민감해진다. 이러한 현상을 다중공선성(multi-collinearity)문제라고 한다. 일반적으로 상관관계가 높은 feature가 많은 경우 독립적인 중요한 feature만 남기고 제거하거나 규제를 적용한다. 또한 매우 많은 feature가 다중 공선성 문제를 가지고 있다면 PCA를 통해 차원 축소를 수행하는 것도 고려해 볼 수 있다. | . 회귀를 위한 평가 지표 : 실제 값과 회귀 예측값의 차이 값을 기반으로 한 지표가 중심이다. | . 305p 참고! | . import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from scipy import stats from sklearn.datasets import load_boston %matplotlib inline # boston 데이타셋 로드 boston = load_boston() # boston 데이타셋 DataFrame 변환 bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names) # boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. bostonDF[&#39;PRICE&#39;] = boston.target print(&#39;Boston 데이타셋 크기 :&#39;,bostonDF.shape) bostonDF.head() . Boston 데이타셋 크기 : (506, 14) . CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE . 0 0.00632 | 18.0 | 2.31 | 0.0 | 0.538 | 6.575 | 65.2 | 4.0900 | 1.0 | 296.0 | 15.3 | 396.90 | 4.98 | 24.0 | . 1 0.02731 | 0.0 | 7.07 | 0.0 | 0.469 | 6.421 | 78.9 | 4.9671 | 2.0 | 242.0 | 17.8 | 396.90 | 9.14 | 21.6 | . 2 0.02729 | 0.0 | 7.07 | 0.0 | 0.469 | 7.185 | 61.1 | 4.9671 | 2.0 | 242.0 | 17.8 | 392.83 | 4.03 | 34.7 | . 3 0.03237 | 0.0 | 2.18 | 0.0 | 0.458 | 6.998 | 45.8 | 6.0622 | 3.0 | 222.0 | 18.7 | 394.63 | 2.94 | 33.4 | . 4 0.06905 | 0.0 | 2.18 | 0.0 | 0.458 | 7.147 | 54.2 | 6.0622 | 3.0 | 222.0 | 18.7 | 396.90 | 5.33 | 36.2 | . 다음으로 각 column이 회귀 결과에 미치는 영향이 어느 정도인지 시각화해서 알아보자. 총 8개의 칼럼에 대해 값이 증가할수록 PRICE값이 어떻게 변화하는지 확인하자. . fig, axs = plt.subplots(figsize=(16,8) , ncols=4 , nrows=2) lm_features = [&#39;RM&#39;,&#39;ZN&#39;,&#39;INDUS&#39;,&#39;NOX&#39;,&#39;AGE&#39;,&#39;PTRATIO&#39;,&#39;LSTAT&#39;,&#39;RAD&#39;] for i , feature in enumerate(lm_features): row = int(i/4) col = i%4 # 시본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현 sns.regplot(x=feature , y=&#39;PRICE&#39;,data=bostonDF , ax=axs[row][col]) . 다른 칼럼보다 RM과 LSTAT의 PRICE 영향도가 가장 두드러지게 나타난다. RM은 양 방향의 선형성이 가장 크다. 즉 방의 크기가 클수록 방의 가격이 증가하는 모습을 확연히 보여준다. | . from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error , r2_score y_target = bostonDF[&#39;PRICE&#39;] X_data = bostonDF.drop([&#39;PRICE&#39;],axis=1,inplace=False) X_train , X_test , y_train , y_test = train_test_split(X_data , y_target ,test_size=0.3, random_state=156) # Linear Regression OLS로 학습/예측/평가 수행. lr = LinearRegression() lr.fit(X_train ,y_train ) y_preds = lr.predict(X_test) mse = mean_squared_error(y_test, y_preds) rmse = np.sqrt(mse) print(&#39;MSE : {0:.3f} , RMSE : {1:.3F}&#39;.format(mse , rmse)) print(&#39;Variance score : {0:.3f}&#39;.format(r2_score(y_test, y_preds))) . MSE : 17.297 , RMSE : 4.159 Variance score : 0.757 . print(&#39;절편 값:&#39;,lr.intercept_) print(&#39;회귀 계수값:&#39;, np.round(lr.coef_, 1)) . 절편 값: 40.99559517216445 회귀 계수값: [ -0.1 0.1 0. 3. -19.8 3.4 0. -1.7 0.4 -0. -0.9 0. -0.6] . coef_ 속성은 회귀 계수 값만 가지고 있으므로 이를 feature별 회귀 계수 값으로 다시 mapping하고 높은 값 순으로 출력해보자. 이를 위해 pandas Series의 sort_values() 함수를 이용한다. . coeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns ) coeff.sort_values(ascending=False) . RM 3.4 CHAS 3.0 RAD 0.4 ZN 0.1 INDUS 0.0 AGE 0.0 TAX -0.0 B 0.0 CRIM -0.1 LSTAT -0.6 PTRATIO -0.9 DIS -1.7 NOX -19.8 dtype: float64 . from sklearn.model_selection import cross_val_score y_target = bostonDF[&#39;PRICE&#39;] X_data = bostonDF.drop([&#39;PRICE&#39;],axis=1,inplace=False) lr = LinearRegression() # cross_val_score( )로 5 Fold 셋으로 MSE 를 구한 뒤 이를 기반으로 다시 RMSE 구함. neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) # cross_val_score(scoring=&quot;neg_mean_squared_error&quot;)로 반환된 값은 모두 음수 print(&#39; 5 folds 의 개별 Negative MSE scores: &#39;, np.round(neg_mse_scores, 2)) print(&#39; 5 folds 의 개별 RMSE scores : &#39;, np.round(rmse_scores, 2)) print(&#39; 5 folds 의 평균 RMSE : {0:.3f} &#39;.format(avg_rmse)) . 5 folds 의 개별 Negative MSE scores: [-12.46 -26.05 -33.07 -80.76 -33.31] 5 folds 의 개별 RMSE scores : [3.53 5.1 5.75 8.99 5.77] 5 folds 의 평균 RMSE : 5.829 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/10/intro.html",
            "relUrl": "/2022/01/10/intro.html",
            "date": " • Jan 10, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "2022/01/09/SUN",
            "content": "print(&#39;학습 데이터 레이블 값 비율&#39;) print(y_train.value_counts()/y_train.shape[0] * 100) print(&#39;테스트 데이터 레이블 값 비율&#39;) print(y_test.value_counts()/y_test.shape[0] * 100) . 학습 데이터 레이블 값 비율 0 99.827451 1 0.172549 Name: Class, dtype: float64 테스트 데이터 레이블 값 비율 0 99.826785 1 0.173215 Name: Class, dtype: float64 . 학습데이터 레이블과 테스트 레이블을 살펴본 결 과 잘 분할 됐음. 이제 로지스틱 회귀와 LightGBM 기반의 모델이 데이터 가공을 수행하면서 예측 성능이 어떻게 변하는지 살펴보자 . 먼저 로지스틱 회귀를 이용해 신용 카드 사기 여부를 예측해보자 . from sklearn.linear_model import LogisticRegression lr_clf = LogisticRegression() lr_clf.fit( X_train, y_train) lr_pred = lr_clf.predict(X_test) lr_pred_proba = lr_clf.predict_proba(X_test)[:, 1] # 3장에서 사용한 get_clf_eval() 함수를 이용하여 평가 수행. get_clf_eval(y_test, lr_pred, lr_pred_proba) . 이번에는 LightGBM을 이용한 모델을 만들어보자 . (앞으로 수행할 예제 코드에서 반복적으로 모델을 변경해 학습/예측/평가할 것이므로 이를 위한 별도의 함수를 생성해보자) . def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None): model.fit(ftr_train, tgt_train) pred = model.predict(ftr_test) pred_proba = model.predict_proba(ftr_test)[:, 1] get_clf_eval(tgt_test, pred, pred_proba) . 먼저, 본 데이터 세트는 극도로 불균형한 레이블 값 분포를 지녔음 $ to$ LightGBMClassifier객체 생성 시 boost_from_average=False로 파라미터를 설정해야한다. | . LightGBM으로 모델을 학습한 뒤 별도의 테스트 데이터 세트에서 예측 평가를 수행해보자 | . from lightgbm import LGBMClassifier lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . 오차 행렬 [[85290 5] [ 36 112]] 정확도: 0.9995, 정밀도: 0.9573, 재현율: 0.7568, F1: 0.8453, AUC:0.9790 . 재현율과 ROC-AUC가 회귀보다 높은 수치 기록. | . 왜곡된 분포도를 가지는 데이터를 재가공한 뒤 모델을 다시 테스트해보자. | 대부분의 선형 모델은 중요 feature들의 값이 정규 분포 형태를 유지하는 것을 선호. | Amount feature는 중요 feature일 가능성이 높음. | . import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings import seaborn as sns warnings.filterwarnings(&quot;ignore&quot;) %matplotlib inline card_df = pd.read_csv(&#39;./creditcard.csv&#39;) card_df.head(3) plt.figure(figsize=(8, 4)) plt.xticks(range(0, 30000, 1000), rotation=60) sns.distplot(card_df[&#39;Amount&#39;]) . &lt;AxesSubplot:xlabel=&#39;Amount&#39;, ylabel=&#39;Density&#39;&gt; . # Amount를 정규분포 형태로 변환 후 로지스틱 회귀 및 LightGBM 수행. X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) print(&#39;### 로지스틱 회귀 예측 성능 ###&#39;) lr_clf = LogisticRegression() get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) print(&#39;### LightGBM 예측 성능 ###&#39;) lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . 성능이 크게 개선되지 X $ to$ 이번에는 StandardScaler가 아니라 로그 변환을 수행해보자. | 로그 변환은 데이터 분포도가 심하게 왜곡되어 있을 경우 적용하는 중요 기법 중 하나.(원래 값을 log 값으로 변환해 원래 큰 값을 상대적으로 작은 값으로 변환하기 때문에 데이터 분포도의 왜곡을 상당수준 개선해준다.) | . def get_preprocessed_df(df=None): df_copy = df.copy() # 넘파이의 log1p( )를 이용하여 Amount를 로그 변환 amount_n = np.log1p(df_copy[&#39;Amount&#39;]) df_copy.insert(0, &#39;Amount_Scaled&#39;, amount_n) df_copy.drop([&#39;Time&#39;,&#39;Amount&#39;], axis=1, inplace=True) return df_copy . 이제 Amount feature를 log 변환한 후 다시 로지스틱 회귀와 LightGBM 모델을 적용한 후 예측 성능을 확인해보자 | . . # log1p 와 expm1 설명 import numpy as np print(1e-1000 == 0.0) print(np.log(1e-1000)) print(np.log(1e-1000 + 1)) print(np.log1p(1e-1000)) . True -inf 0.0 0.0 . var_1 = np.log1p(100) var_2 = np.expm1(var_1) print(var_1, var_2) . 4.61512051684126 100.00000000000003 . . X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) print(&#39;### 로지스틱 회귀 예측 성능 ###&#39;) get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) print(&#39;### LightGBM 예측 성능 ###&#39;) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . ### 로지스틱 회귀 예측 성능 ### 오차 행렬 [[85283 12] [ 59 89]] 정확도: 0.9992, 정밀도: 0.8812, 재현율: 0.6014, F1: 0.7149, AUC:0.9727 ### LightGBM 예측 성능 ### 오차 행렬 [[85290 5] [ 35 113]] 정확도: 0.9995, 정밀도: 0.9576, 재현율: 0.7635, F1: 0.8496, AUC:0.9796 . 두 모델 모두 정밀도, 재현율, ROC-AUC에서 약간씩 성능이 개선되었음을 알 수 있음 | . &#51060;&#49345;&#52824; &#45936;&#51060;&#53552; :Outlier ?? &#51204;&#52404; &#45936;&#51060;&#53552;&#51032; &#54056;&#53556;&#50640;&#49436; &#48279;&#50612;&#45212; &#51060;&#49345; &#44050;&#51012; &#44032;&#51652; &#45936;&#51060;&#53552;. IQR&#48169;&#49885;&#51060; &#51080;&#51020;. IQR&#51008; &#49324;&#48516;&#50948; &#44050;&#51032; &#54200;&#52264;&#47484; &#51060;&#50857;&#54616;&#45716; &#44592;&#48277;&#51004;&#47196;&#49436; &#55124;&#55176; &#48149;&#49828;&#54540;&#46991;&#51004;&#47196; &#49884;&#44033;&#54868; &#54624; &#49688; &#51080;&#45796;. . 먼저 사분위란 전체 데이터를 값이 높은 순으로 정렬하고 이를 25%씩 구간 분할하는 것을 지칭. 가령 100명의 시험 성적이 0점부터 100점까지 있다면 이를 100등부터 1등까지 성적순으로 정렬한 뒤 1/4구간으로 나누는 것. 여기서 25%~75% 범위를 IQR이라고 한다. IQR에 보통은 1.5를 공해서 75%지점에 더해주고 25%지점에서 빼준 뒤 그 범위를 넘어가면 outlier로 간주!. 이를 시각화한 도표가 boxplot이다. . 먼저 어떤 feature의 이상치 데이터를 검출할 것인지 선택. | 매우 많은 feature가 있을 경우 상관성이 높은 feature들을 위주로 이상치를 검출하자. | . import seaborn as sns plt.figure(figsize=(9, 9)) corr = card_df.corr() sns.heatmap(corr, cmap=&#39;RdBu&#39;) . &lt;AxesSubplot:&gt; . 양의 상관관계가 높을수록 색이 진한 파란색. 음의 상관관계가 높을수록 색이 진한 빨간색. . | 이중 결정 레이블에 속하는 Class feature와 음의 상관관계가 가장 높은 feature는 V14와 V17, 이중 V14에서 outlier를 제거해보자 . | . import numpy as np def get_outlier(df=None, column=None, weight=1.5): # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함. fraud = df[df[&#39;Class&#39;]==1][column] quantile_25 = np.percentile(fraud.values, 25) quantile_75 = np.percentile(fraud.values, 75) # IQR을 구하고, IQR에 1.5를 곱하여 최대값과 최소값 지점 구함. iqr = quantile_75 - quantile_25 iqr_weight = iqr * weight lowest_val = quantile_25 - iqr_weight highest_val = quantile_75 + iqr_weight # 최대값 보다 크거나, 최소값 보다 작은 값을 아웃라이어로 설정하고 DataFrame index 반환. outlier_index = fraud[(fraud &lt; lowest_val) | (fraud &gt; highest_val)].index return outlier_index . outlier_index = get_outlier(df=card_df, column=&#39;V14&#39;, weight=1.5) print(&#39;이상치 데이터 인덱스:&#39;, outlier_index) . 이상치 데이터 인덱스: Int64Index([8296, 8615, 9035, 9252], dtype=&#39;int64&#39;) . 이를 삭제하는 로직을 추가 | . def get_preprocessed_df(df=None): df_copy = df.copy() amount_n = np.log1p(df_copy[&#39;Amount&#39;]) df_copy.insert(0, &#39;Amount_Scaled&#39;, amount_n) df_copy.drop([&#39;Time&#39;,&#39;Amount&#39;], axis=1, inplace=True) # 이상치 데이터 삭제하는 로직 추가 outlier_index = get_outlier(df=df_copy, column=&#39;V14&#39;, weight=1.5) df_copy.drop(outlier_index, axis=0, inplace=True) return df_copy X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) print(&#39;### 로지스틱 회귀 예측 성능 ###&#39;) get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) print(&#39;### LightGBM 예측 성능 ###&#39;) get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test) . ### 로지스틱 회귀 예측 성능 ### 오차 행렬 [[85281 14] [ 48 98]] 정확도: 0.9993, 정밀도: 0.8750, 재현율: 0.6712, F1: 0.7597, AUC:0.9743 ### LightGBM 예측 성능 ### 오차 행렬 [[85290 5] [ 25 121]] 정확도: 0.9996, 정밀도: 0.9603, 재현율: 0.8288, F1: 0.8897, AUC:0.9780 . outlier 제거한 뒤 로지스틱 회귀와 LightGBM 모두 예측 성능이 크게 향상됐음을 알 수 있다. | . . SMOTE 기법으로 오버 샘플링을 적용한 뒤 로지스틱 회구와 LightGBM 모델의 예측 성능을 평가해보자 | SMOTE를 적용할 땐, 반드시 학습 데이터 세트만 오버 샘플링 해야함 | . from imblearn.over_sampling import SMOTE smote = SMOTE(random_state=0) X_train_over, y_train_over = smote.fit_sample(X_train, y_train) print(&#39;SMOTE 적용 전 학습용 피처/레이블 데이터 세트: &#39;, X_train.shape, y_train.shape) print(&#39;SMOTE 적용 후 학습용 피처/레이블 데이터 세트: &#39;, X_train_over.shape, y_train_over.shape) print(&#39;SMOTE 적용 후 레이블 값 분포: n&#39;, pd.Series(y_train_over).value_counts()) . SMOTE 적용 전 학습용 피처/레이블 데이터 세트: (199362, 29) (199362,) SMOTE 적용 후 학습용 피처/레이블 데이터 세트: (398040, 29) (398040,) SMOTE 적용 후 레이블 값 분포: 0 199020 1 199020 Name: Class, dtype: int64 . lr_clf = LogisticRegression() # ftr_train과 tgt_train 인자값이 SMOTE 증식된 X_train_over와 y_train_over로 변경됨에 유의 get_model_train_eval(lr_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test) . 오차 행렬 [[82937 2358] [ 11 135]] 정확도: 0.9723, 정밀도: 0.0542, 재현율: 0.9247, F1: 0.1023, AUC:0.9737 . 재현울은 크게 증가하나 정밀도가 급격히 저하 | 이 정도로 저조한 정밀도는 현실 업무에 적용할 수 없음 | . lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False) get_model_train_eval(lgbm_clf, ftr_train=X_train_over, ftr_test=X_test, tgt_train=y_train_over, tgt_test=y_test) . 오차 행렬 [[85283 12] [ 22 124]] 정확도: 0.9996, 정밀도: 0.9118, 재현율: 0.8493, F1: 0.8794, AUC:0.9814 . 재현율은 높아졌으나 정밀도는 좀 낮아짐. | SMOTE를 적용하면 재현율은 높아지나 정밀도는 낮아지는 것이 일반적임. | . &#49828;&#53468;&#53433; &#50521;&#49345;&#48660; . 스태킹은 개별적인 여러 알고리즘을 서로 결합해 예측 결과를 도출한다는 점에서 배깅,부스팅과 일맥상통. 허나 가장 큰 차이점은 개별 알고리즘으로 예측한 데이터를 기반으로 다시 마지막 특정 알고리즘으로 재예측시도함. 즉 개별 알고리즘의 예측 결과 데이터 세트를 최종적인 메타 데이터 세트로 만들어 별도의 ML 알고리즘으로 최종학습을 수행하고 테스트 데이터를 기반으로 다시 최종 예측을 수행하는 방식. 이렇게 개별 모델의 예측된 데이터 세트를 다시 기반으로 하여 학습하고 예측하는 방식을 메타 모델이라고 한다. 따라서 스태킹 모델은 두 종류의 모델 필요. 첫번째는 기반 모델, 두번째는 이 개별 기반 모델의 예측 데이터를 학습 데이터로 만들어서 학습하는 최종 메타 모델. 스태킹 모델의 핵심은 여러 개별 모델의 예측 데이터를 각각 스태킹 형태로 결합해, 즉 잘 쌓아서, 최종 메타 모델의 학습용 feature데이터 세트와 테스트용 feature 데이터 세트를 만드는 것이다. 이를 도식화 한 248p참고 . 기본 스태킹 모델을 위스콘신 암 데이터 세트에 적용해보자 | . import numpy as np from sklearn.neighbors import KNeighborsClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.linear_model import LogisticRegression from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score cancer_data = load_breast_cancer() X_data = cancer_data.data y_label = cancer_data.target X_train , X_test , y_train , y_test = train_test_split(X_data , y_label , test_size=0.2 , random_state=0) . 스태킹에 사용될 머신러닝 알고리즘 클래스를 생성하자. | 개별 모델은 KNN, 랜덤 포레스트, 결정 트리, 에이다 부스트이며 이들 모델의 예측 결과를 합한 데이터 세트로 학습,예측 수행하는 최종 모델은 로지스틱 회귀이다. | . knn_clf = KNeighborsClassifier(n_neighbors=4) rf_clf = RandomForestClassifier(n_estimators=100, random_state=0) dt_clf = DecisionTreeClassifier() ada_clf = AdaBoostClassifier(n_estimators=100) # 최종 Stacking 모델을 위한 Classifier생성. lr_final = LogisticRegression(C=10) . 개별 모델들을 학습시키자 | . knn_clf.fit(X_train, y_train) rf_clf.fit(X_train , y_train) dt_clf.fit(X_train , y_train) ada_clf.fit(X_train, y_train) . AdaBoostClassifier(n_estimators=100) . 개별 모델의 예측 데이터 세트를 반환하고 각 모델의 예측 정확도를 살펴보자 | . knn_pred = knn_clf.predict(X_test) rf_pred = rf_clf.predict(X_test) dt_pred = dt_clf.predict(X_test) ada_pred = ada_clf.predict(X_test) print(&#39;KNN 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, knn_pred))) print(&#39;랜덤 포레스트 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, rf_pred))) print(&#39;결정 트리 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, dt_pred))) print(&#39;에이다부스트 정확도: {0:.4f} :&#39;.format(accuracy_score(y_test, ada_pred))) . KNN 정확도: 0.9211 랜덤 포레스트 정확도: 0.9649 결정 트리 정확도: 0.9123 에이다부스트 정확도: 0.9561 : . 개별 알고리즘으로부터 예측된 예측값을 칼럼 레벨로 옆으로 붙여서 feature값으로 만들어, 최종 메타 모델인 로지스틱 회귀에서 학습 데이터로 다시 사용하자. | . pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred]) print(pred.shape) # transpose를 이용해 행과 열의 위치 교환. 컬럼 레벨로 각 알고리즘의 예측 결과를 피처로 만듦. pred = np.transpose(pred) print(pred.shape) . (4, 114) (114, 4) . lr_final.fit(pred, y_test) final = lr_final.predict(pred) print(&#39;최종 메타 모델의 예측 정확도: {0:.4f}&#39;.format(accuracy_score(y_test , final))) . 최종 메타 모델의 예측 정확도: 0.9737 . 개별 모델 정확도보다 스태킹으로 재구성해 최종 메타 모델에서 학습하고 예측한 결과가 더 높음. | . CV &#49464;&#53944; &#44592;&#48152;&#51032; &#49828;&#53468;&#53433; . 과적합을 개선하기 위해 최종 메타 모델을 위한 데이터 세트를 만들 때 교차 검증을 기반으로 예측된 결과 데이터 세트를 이용한다. 앞에서 마지막에 메타 모델인 로지스틱 회귀 모델을 기반으로 최종 학습할 때 레이블 데이터 세트로 학습데이터가 아닌 테스트용 레이블 데이터 세트를 기반으로 학습했기에 과적합 문제가 발생할 수 있다. CV세트 기반의 스태킹은 이에 대한 개선을 위해 개별 모델들이 각각 교차 검증으로 메타모델을 위한 학습용 스태킹 데이터 생성과 예측을 위한 테스트용 스태킹 데이터를 생성한 뒤 이를 기반으로 메타 모델이 학습과 예측을 수행한다. 자세한 건 284p참고하자 . from sklearn.model_selection import KFold from sklearn.metrics import mean_absolute_error # 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ): # 지정된 n_folds값으로 KFold 생성. kf = KFold(n_splits=n_folds, shuffle=False, random_state=0) #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 train_fold_pred = np.zeros((X_train_n.shape[0] ,1 )) test_pred = np.zeros((X_test_n.shape[0],n_folds)) print(model.__class__.__name__ , &#39; model 시작 &#39;) for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)): #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 print(&#39; t 폴드 세트: &#39;,folder_counter,&#39; 시작 &#39;) X_tr = X_train_n[train_index] y_tr = y_train_n[train_index] X_te = X_train_n[valid_index] #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행. model.fit(X_tr , y_tr) #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장. train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. test_pred[:, folder_counter] = model.predict(X_test_n) # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1) #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터 return train_fold_pred , test_pred_mean . knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7) rf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7) dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7) ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7) . KNeighborsClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 RandomForestClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 DecisionTreeClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 AdaBoostClassifier model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 폴드 세트: 5 시작 폴드 세트: 6 시작 . Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1) Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1) print(&#39;원본 학습 피처 데이터 Shape:&#39;,X_train.shape, &#39;원본 테스트 피처 Shape:&#39;,X_test.shape) print(&#39;스태킹 학습 피처 데이터 Shape:&#39;, Stack_final_X_train.shape, &#39;스태킹 테스트 피처 데이터 Shape:&#39;,Stack_final_X_test.shape) . 원본 학습 피처 데이터 Shape: (455, 30) 원본 테스트 피처 Shape: (114, 30) 스태킹 학습 피처 데이터 Shape: (455, 4) 스태킹 테스트 피처 데이터 Shape: (114, 4) . lr_final.fit(Stack_final_X_train, y_train) stack_final = lr_final.predict(Stack_final_X_test) print(&#39;최종 메타 모델의 예측 정확도: {0:.4f}&#39;.format(accuracy_score(y_test, stack_final))) . 최종 메타 모델의 예측 정확도: 0.9737 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/09/intro.html",
            "relUrl": "/2022/01/09/intro.html",
            "date": " • Jan 9, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "2022/01/08/SAT",
            "content": "LightGBM . :XGBoost보다 학습에 걸리는 시간이 적다. 10,000건 이하의 적은 데이터 세트에 적용할 경우 과적합이 발생하기 쉽다. 리프 중심 트리(Leaf Wise) 분할 방식을 사용한다. 기존의 대부분 트리 기반 알고리즘은 트리의 깊이를 효과적으로 줄이기 위한 균형 트리 분할(Level wise) 방식을 사용. 즉 최대한 규형 잡힌 트리를 유지하면서 분할하기 때문에 트리의 깊이가 최소화될 수 있다. 하지만 LightGBM의 리프 중심 트리 분할 방식은 트리의 균형을 맞추지 않고 최대 손실 값(max delta loss)을 가지는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적인 규칙 트리가 생성된다. 하지만 이렇게 최대 손실값을 가지는 리프 노드를 지속적으로 분할해 생성된 규칙 트리는 학습을 반복할수록 결국은 균형 트리 방식보다 예측 오류 손실을 최소화할 수 있다는 것이 LightGBM의 구현 사상이다. Xgboost와 다르게 리프 노드가 계속 분할되면서 트리의 깊이가 깊어지므로 이러한 트리 특성에 맞는 하이퍼 파라미터 설정이 필요하다. (245p 균형 트리 분할과 리프 중심 트리 분할 도식화한 거 살펴보기) . learning_rate를 작게 하면서 n_estimators를 크게 하는 것이 부스팅 계열 튜닝에서 가장 기본적이 튜닝 방안이다. . LightGBM을 이용해 위스콘신 유방암 데이터 세트를 예측해보자. . from lightgbm import LGBMClassifier import pandas as pd import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split dataset = load_breast_cancer() ftr = dataset.data target = dataset.target # 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출 X_train, X_test, y_train, y_test=train_test_split(ftr, target, test_size=0.2, random_state=156 ) # 앞서 XGBoost와 동일하게 n_estimators는 400 설정. lgbm_wrapper = LGBMClassifier(n_estimators=400) # LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능. evals = [(X_test, y_test)] lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;logloss&quot;, eval_set=evals, verbose=True) preds = lgbm_wrapper.predict(X_test) pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1] . [1] valid_0&#39;s binary_logloss: 0.565079 [2] valid_0&#39;s binary_logloss: 0.507451 [3] valid_0&#39;s binary_logloss: 0.458489 [4] valid_0&#39;s binary_logloss: 0.417481 [5] valid_0&#39;s binary_logloss: 0.385507 [6] valid_0&#39;s binary_logloss: 0.355773 [7] valid_0&#39;s binary_logloss: 0.329587 [8] valid_0&#39;s binary_logloss: 0.308478 [9] valid_0&#39;s binary_logloss: 0.285395 [10] valid_0&#39;s binary_logloss: 0.267055 [11] valid_0&#39;s binary_logloss: 0.252013 [12] valid_0&#39;s binary_logloss: 0.237018 [13] valid_0&#39;s binary_logloss: 0.224756 [14] valid_0&#39;s binary_logloss: 0.213383 [15] valid_0&#39;s binary_logloss: 0.203058 [16] valid_0&#39;s binary_logloss: 0.194015 [17] valid_0&#39;s binary_logloss: 0.186412 [18] valid_0&#39;s binary_logloss: 0.179108 [19] valid_0&#39;s binary_logloss: 0.174004 [20] valid_0&#39;s binary_logloss: 0.167155 [21] valid_0&#39;s binary_logloss: 0.162494 [22] valid_0&#39;s binary_logloss: 0.156886 [23] valid_0&#39;s binary_logloss: 0.152855 [24] valid_0&#39;s binary_logloss: 0.151113 [25] valid_0&#39;s binary_logloss: 0.148395 [26] valid_0&#39;s binary_logloss: 0.145869 [27] valid_0&#39;s binary_logloss: 0.143036 [28] valid_0&#39;s binary_logloss: 0.14033 [29] valid_0&#39;s binary_logloss: 0.139609 [30] valid_0&#39;s binary_logloss: 0.136109 [31] valid_0&#39;s binary_logloss: 0.134867 [32] valid_0&#39;s binary_logloss: 0.134729 [33] valid_0&#39;s binary_logloss: 0.1311 [34] valid_0&#39;s binary_logloss: 0.131143 [35] valid_0&#39;s binary_logloss: 0.129435 [36] valid_0&#39;s binary_logloss: 0.128474 [37] valid_0&#39;s binary_logloss: 0.126683 [38] valid_0&#39;s binary_logloss: 0.126112 [39] valid_0&#39;s binary_logloss: 0.122831 [40] valid_0&#39;s binary_logloss: 0.123162 [41] valid_0&#39;s binary_logloss: 0.125592 [42] valid_0&#39;s binary_logloss: 0.128293 [43] valid_0&#39;s binary_logloss: 0.128123 [44] valid_0&#39;s binary_logloss: 0.12789 [45] valid_0&#39;s binary_logloss: 0.122818 [46] valid_0&#39;s binary_logloss: 0.12496 [47] valid_0&#39;s binary_logloss: 0.125578 [48] valid_0&#39;s binary_logloss: 0.127381 [49] valid_0&#39;s binary_logloss: 0.128349 [50] valid_0&#39;s binary_logloss: 0.127004 [51] valid_0&#39;s binary_logloss: 0.130288 [52] valid_0&#39;s binary_logloss: 0.131362 [53] valid_0&#39;s binary_logloss: 0.133363 [54] valid_0&#39;s binary_logloss: 0.1332 [55] valid_0&#39;s binary_logloss: 0.134543 [56] valid_0&#39;s binary_logloss: 0.130803 [57] valid_0&#39;s binary_logloss: 0.130306 [58] valid_0&#39;s binary_logloss: 0.132514 [59] valid_0&#39;s binary_logloss: 0.133278 [60] valid_0&#39;s binary_logloss: 0.134804 [61] valid_0&#39;s binary_logloss: 0.136888 [62] valid_0&#39;s binary_logloss: 0.138745 [63] valid_0&#39;s binary_logloss: 0.140497 [64] valid_0&#39;s binary_logloss: 0.141368 [65] valid_0&#39;s binary_logloss: 0.140764 [66] valid_0&#39;s binary_logloss: 0.14348 [67] valid_0&#39;s binary_logloss: 0.143418 [68] valid_0&#39;s binary_logloss: 0.143682 [69] valid_0&#39;s binary_logloss: 0.145076 [70] valid_0&#39;s binary_logloss: 0.14686 [71] valid_0&#39;s binary_logloss: 0.148051 [72] valid_0&#39;s binary_logloss: 0.147664 [73] valid_0&#39;s binary_logloss: 0.149478 [74] valid_0&#39;s binary_logloss: 0.14708 [75] valid_0&#39;s binary_logloss: 0.14545 [76] valid_0&#39;s binary_logloss: 0.148767 [77] valid_0&#39;s binary_logloss: 0.149959 [78] valid_0&#39;s binary_logloss: 0.146083 [79] valid_0&#39;s binary_logloss: 0.14638 [80] valid_0&#39;s binary_logloss: 0.148461 [81] valid_0&#39;s binary_logloss: 0.15091 [82] valid_0&#39;s binary_logloss: 0.153011 [83] valid_0&#39;s binary_logloss: 0.154807 [84] valid_0&#39;s binary_logloss: 0.156501 [85] valid_0&#39;s binary_logloss: 0.158586 [86] valid_0&#39;s binary_logloss: 0.159819 [87] valid_0&#39;s binary_logloss: 0.161745 [88] valid_0&#39;s binary_logloss: 0.162829 [89] valid_0&#39;s binary_logloss: 0.159142 [90] valid_0&#39;s binary_logloss: 0.156765 [91] valid_0&#39;s binary_logloss: 0.158625 [92] valid_0&#39;s binary_logloss: 0.156832 [93] valid_0&#39;s binary_logloss: 0.154616 [94] valid_0&#39;s binary_logloss: 0.154263 [95] valid_0&#39;s binary_logloss: 0.157156 [96] valid_0&#39;s binary_logloss: 0.158617 [97] valid_0&#39;s binary_logloss: 0.157495 [98] valid_0&#39;s binary_logloss: 0.159413 [99] valid_0&#39;s binary_logloss: 0.15847 [100] valid_0&#39;s binary_logloss: 0.160746 [101] valid_0&#39;s binary_logloss: 0.16217 [102] valid_0&#39;s binary_logloss: 0.165293 [103] valid_0&#39;s binary_logloss: 0.164749 [104] valid_0&#39;s binary_logloss: 0.167097 [105] valid_0&#39;s binary_logloss: 0.167697 [106] valid_0&#39;s binary_logloss: 0.169462 [107] valid_0&#39;s binary_logloss: 0.169947 [108] valid_0&#39;s binary_logloss: 0.171 [109] valid_0&#39;s binary_logloss: 0.16907 [110] valid_0&#39;s binary_logloss: 0.169521 [111] valid_0&#39;s binary_logloss: 0.167719 [112] valid_0&#39;s binary_logloss: 0.166648 [113] valid_0&#39;s binary_logloss: 0.169053 [114] valid_0&#39;s binary_logloss: 0.169613 [115] valid_0&#39;s binary_logloss: 0.170059 [116] valid_0&#39;s binary_logloss: 0.1723 [117] valid_0&#39;s binary_logloss: 0.174733 [118] valid_0&#39;s binary_logloss: 0.173526 [119] valid_0&#39;s binary_logloss: 0.1751 [120] valid_0&#39;s binary_logloss: 0.178254 [121] valid_0&#39;s binary_logloss: 0.182968 [122] valid_0&#39;s binary_logloss: 0.179017 [123] valid_0&#39;s binary_logloss: 0.178326 [124] valid_0&#39;s binary_logloss: 0.177149 [125] valid_0&#39;s binary_logloss: 0.179171 [126] valid_0&#39;s binary_logloss: 0.180948 [127] valid_0&#39;s binary_logloss: 0.183861 [128] valid_0&#39;s binary_logloss: 0.187579 [129] valid_0&#39;s binary_logloss: 0.188122 [130] valid_0&#39;s binary_logloss: 0.1857 [131] valid_0&#39;s binary_logloss: 0.187442 [132] valid_0&#39;s binary_logloss: 0.188578 [133] valid_0&#39;s binary_logloss: 0.189729 [134] valid_0&#39;s binary_logloss: 0.187313 [135] valid_0&#39;s binary_logloss: 0.189279 [136] valid_0&#39;s binary_logloss: 0.191068 [137] valid_0&#39;s binary_logloss: 0.192414 [138] valid_0&#39;s binary_logloss: 0.191255 [139] valid_0&#39;s binary_logloss: 0.193453 [140] valid_0&#39;s binary_logloss: 0.196969 [141] valid_0&#39;s binary_logloss: 0.196378 [142] valid_0&#39;s binary_logloss: 0.196367 [143] valid_0&#39;s binary_logloss: 0.19869 [144] valid_0&#39;s binary_logloss: 0.200352 [145] valid_0&#39;s binary_logloss: 0.19712 . C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:726: UserWarning: &#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;early_stopping()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:736: UserWarning: &#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;log_evaluation()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; . 조기 중단으로 145번 반복까지만 수행하고 학습을 종료. 이제 학습된 LightGBM 모델을 기반으로 예측 성능을 평가해보자 . from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.metrics import precision_score, recall_score from sklearn.metrics import f1_score, roc_auc_score # 수정된 get_clf_eval() 함수 def get_clf_eval(y_test, pred=None, pred_proba=None): confusion = confusion_matrix( y_test, pred) accuracy = accuracy_score(y_test , pred) precision = precision_score(y_test , pred) recall = recall_score(y_test , pred) f1 = f1_score(y_test,pred) # ROC-AUC 추가 roc_auc = roc_auc_score(y_test, pred_proba) print(&#39;오차 행렬&#39;) print(confusion) # ROC-AUC print 추가 print(&#39;정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}&#39;.format(accuracy, precision, recall, f1, roc_auc)) . get_clf_eval(y_test, preds, pred_proba) . 오차 행렬 [[33 4] [ 1 76]] 정확도: 0.9561, 정밀도: 0.9500, 재현율: 0.9870, F1: 0.9682, AUC:0.9905 . from lightgbm import plot_importance import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=(10, 12)) # 사이킷런 래퍼 클래스를 입력해도 무방. plot_importance(lgbm_wrapper, ax=ax) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}, xlabel=&#39;Feature importance&#39;, ylabel=&#39;Features&#39;&gt; . 이번에는 캐글의 산탄데르 고객 만족 데이터 세트에 대해 고객 만족 여부를 XGBoost와 LightGBM을 활용해 예측해보자 . 데이터 전처리 먼저! . import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib cust_df = pd.read_csv(&quot;./train_santander.csv&quot;,encoding=&#39;latin-1&#39;) print(&#39;dataset shape:&#39;, cust_df.shape) cust_df.head(3) . dataset shape: (76020, 371) . ID var3 var15 imp_ent_var16_ult1 imp_op_var39_comer_ult1 imp_op_var39_comer_ult3 imp_op_var40_comer_ult1 imp_op_var40_comer_ult3 imp_op_var40_efect_ult1 imp_op_var40_efect_ult3 ... saldo_medio_var33_hace2 saldo_medio_var33_hace3 saldo_medio_var33_ult1 saldo_medio_var33_ult3 saldo_medio_var44_hace2 saldo_medio_var44_hace3 saldo_medio_var44_ult1 saldo_medio_var44_ult3 var38 TARGET . 0 1 | 2 | 23 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 39205.17 | 0 | . 1 3 | 2 | 34 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 49278.03 | 0 | . 2 4 | 2 | 23 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 67333.77 | 0 | . 3 rows × 371 columns . 클래스 값 포함한 feature가 371개 | . cust_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 76020 entries, 0 to 76019 Columns: 371 entries, ID to TARGET dtypes: float64(111), int64(260) memory usage: 215.2 MB . null값 없음 $ to$ 그러나 null값이 없는 대신 극단적인 숫자로 대체해놨을수도 있음 . | 전체 데이터에서 만족과 불만족의 비율을 살펴보자 $ to$ 레이블인 Target 속성 값의 분포를 알아보자 . | . print(cust_df[&#39;TARGET&#39;].value_counts()) unsatisfied_cnt = cust_df[cust_df[&#39;TARGET&#39;] == 1][&#39;TARGET&#39;].count() total_cnt = cust_df[&#39;TARGET&#39;].count() print(&#39;unsatisfied 비율은 {0:.2f}&#39;.format((unsatisfied_cnt / total_cnt))) . 0 73012 1 3008 Name: TARGET, dtype: int64 unsatisfied 비율은 0.04 . 불만족 비율은 4%에 불과 | . cust_df.describe() . ID var3 var15 imp_ent_var16_ult1 imp_op_var39_comer_ult1 imp_op_var39_comer_ult3 imp_op_var40_comer_ult1 imp_op_var40_comer_ult3 imp_op_var40_efect_ult1 imp_op_var40_efect_ult3 ... saldo_medio_var33_hace2 saldo_medio_var33_hace3 saldo_medio_var33_ult1 saldo_medio_var33_ult3 saldo_medio_var44_hace2 saldo_medio_var44_hace3 saldo_medio_var44_ult1 saldo_medio_var44_ult3 var38 TARGET . count 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | ... | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 76020.000000 | 7.602000e+04 | 76020.000000 | . mean 75964.050723 | -1523.199277 | 33.212865 | 86.208265 | 72.363067 | 119.529632 | 3.559130 | 6.472698 | 0.412946 | 0.567352 | ... | 7.935824 | 1.365146 | 12.215580 | 8.784074 | 31.505324 | 1.858575 | 76.026165 | 56.614351 | 1.172358e+05 | 0.039569 | . std 43781.947379 | 39033.462364 | 12.956486 | 1614.757313 | 339.315831 | 546.266294 | 93.155749 | 153.737066 | 30.604864 | 36.513513 | ... | 455.887218 | 113.959637 | 783.207399 | 538.439211 | 2013.125393 | 147.786584 | 4040.337842 | 2852.579397 | 1.826646e+05 | 0.194945 | . min 1.000000 | -999999.000000 | 5.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 5.163750e+03 | 0.000000 | . 25% 38104.750000 | 2.000000 | 23.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 6.787061e+04 | 0.000000 | . 50% 76043.000000 | 2.000000 | 28.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.064092e+05 | 0.000000 | . 75% 113748.750000 | 2.000000 | 40.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | ... | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 1.187563e+05 | 0.000000 | . max 151838.000000 | 238.000000 | 105.000000 | 210000.000000 | 12888.030000 | 21024.810000 | 8237.820000 | 11073.570000 | 6600.000000 | 6600.000000 | ... | 50003.880000 | 20385.720000 | 138831.630000 | 91778.730000 | 438329.220000 | 24650.010000 | 681462.900000 | 397884.300000 | 2.203474e+07 | 1.000000 | . 8 rows × 371 columns . var3 칼럼의 경우 min값이 -999999이다. NaN값이나 특정 예외 값을 -999999로 변환한 것으로 보임. | . print(cust_df.var3.value_counts()[:10]) . 2 74165 8 138 -999999 116 9 110 3 108 1 105 13 98 7 97 4 86 12 85 Name: var3, dtype: int64 . -999999값이 116개나 있음을 알 수 있다. . | var3은 숫자 형이고 다른 값에 비해 -999999은 편차가 너무 심하므로 -999999을 값이 가장 많은 2로 변환하자. . | ID feature는 단순 식별자에 불과하므로 feature를 drop하자. . | 그리고 클래스 데이터 세트와 faeture 데이터 세트를 분리해 별도의 데이터 세트로 별도로 저장하자 . | . cust_df[&#39;var3&#39;].replace(-999999, 2, inplace=True) cust_df.drop(&#39;ID&#39;,axis=1 , inplace=True) # 피처 세트와 레이블 세트분리. 레이블 컬럼은 DataFrame의 맨 마지막에 위치해 컬럼 위치 -1로 분리 X_features = cust_df.iloc[:, :-1] y_labels = cust_df.iloc[:, -1] print(&#39;피처 데이터 shape:{0}&#39;.format(X_features.shape)) . 피처 데이터 shape:(76020, 369) . 학습과 성능 평가를 위해서 원본 데이터 세트에서 학습 데이터 세트와 테스트 데이터 세트를 분리하자 . | 비대칭한 데이터 세트이므로 클래스인 Target 값 분포도가 학습 데이터와 테스트 데이터 세트에 모두 비슷하게 추출됐는지 확인하자. . | . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=0) train_cnt = y_train.count() test_cnt = y_test.count() print(&#39; n학습 세트 Shape:{0}, 테스트 세트 Shape:{1}&#39;.format(X_train.shape , X_test.shape)) print(&#39; n학습 세트 레이블 값 분포 비율&#39;) print(y_train.value_counts()/train_cnt) print(&#39; n 테스트 세트 레이블 값 분포 비율&#39;) print(y_test.value_counts()/test_cnt,&#39; n&#39;) . 학습 세트 Shape:(60816, 369), 테스트 세트 Shape:(15204, 369) 학습 세트 레이블 값 분포 비율 0 0.960964 1 0.039036 Name: TARGET, dtype: float64 테스트 세트 레이블 값 분포 비율 0 0.9583 1 0.0417 Name: TARGET, dtype: float64 . 학습과 테스트 데이터 세트 모두 Target의 값의 분포가 원본 데이터와 유사. | . XGBoost의 학습 모델을 생성하고 예측 결과를 ROC AUC로 평가해보자 | . from xgboost import XGBClassifier from sklearn.metrics import roc_auc_score # n_estimators는 500으로, random state는 예제 수행 시마다 동일 예측 결과를 위해 설정. xgb_clf = XGBClassifier(n_estimators=500, random_state=156) # 성능 평가 지표를 auc로, 조기 중단 파라미터는 100으로 설정하고 학습 수행. xgb_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;auc&quot;, eval_set=[(X_train, y_train), (X_test, y_test)]) xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(xgb_roc_score)) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages xgboost sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1]. warnings.warn(label_encoder_deprecation_msg, UserWarning) . [0] validation_0-auc:0.82005 validation_1-auc:0.81157 [1] validation_0-auc:0.83400 validation_1-auc:0.82452 [2] validation_0-auc:0.83870 validation_1-auc:0.82746 [3] validation_0-auc:0.84419 validation_1-auc:0.82922 [4] validation_0-auc:0.84783 validation_1-auc:0.83298 [5] validation_0-auc:0.85125 validation_1-auc:0.83500 [6] validation_0-auc:0.85501 validation_1-auc:0.83653 [7] validation_0-auc:0.85831 validation_1-auc:0.83782 [8] validation_0-auc:0.86143 validation_1-auc:0.83802 [9] validation_0-auc:0.86452 validation_1-auc:0.83914 [10] validation_0-auc:0.86717 validation_1-auc:0.83954 [11] validation_0-auc:0.87013 validation_1-auc:0.83983 [12] validation_0-auc:0.87369 validation_1-auc:0.84033 [13] validation_0-auc:0.87620 validation_1-auc:0.84054 [14] validation_0-auc:0.87799 validation_1-auc:0.84135 [15] validation_0-auc:0.88072 validation_1-auc:0.84117 [16] validation_0-auc:0.88238 validation_1-auc:0.84101 [17] validation_0-auc:0.88354 validation_1-auc:0.84071 [18] validation_0-auc:0.88458 validation_1-auc:0.84052 [19] validation_0-auc:0.88592 validation_1-auc:0.84023 [20] validation_0-auc:0.88790 validation_1-auc:0.84012 [21] validation_0-auc:0.88846 validation_1-auc:0.84022 [22] validation_0-auc:0.88980 validation_1-auc:0.84007 [23] validation_0-auc:0.89019 validation_1-auc:0.84009 [24] validation_0-auc:0.89195 validation_1-auc:0.83974 [25] validation_0-auc:0.89255 validation_1-auc:0.84015 [26] validation_0-auc:0.89332 validation_1-auc:0.84101 [27] validation_0-auc:0.89389 validation_1-auc:0.84088 [28] validation_0-auc:0.89420 validation_1-auc:0.84074 [29] validation_0-auc:0.89665 validation_1-auc:0.83999 [30] validation_0-auc:0.89741 validation_1-auc:0.83959 [31] validation_0-auc:0.89916 validation_1-auc:0.83952 [32] validation_0-auc:0.90106 validation_1-auc:0.83901 [33] validation_0-auc:0.90253 validation_1-auc:0.83885 [34] validation_0-auc:0.90278 validation_1-auc:0.83887 [35] validation_0-auc:0.90293 validation_1-auc:0.83864 [36] validation_0-auc:0.90463 validation_1-auc:0.83834 [37] validation_0-auc:0.90500 validation_1-auc:0.83810 [38] validation_0-auc:0.90519 validation_1-auc:0.83810 [39] validation_0-auc:0.90533 validation_1-auc:0.83813 [40] validation_0-auc:0.90575 validation_1-auc:0.83776 [41] validation_0-auc:0.90691 validation_1-auc:0.83720 [42] validation_0-auc:0.90716 validation_1-auc:0.83684 [43] validation_0-auc:0.90737 validation_1-auc:0.83672 [44] validation_0-auc:0.90759 validation_1-auc:0.83674 [45] validation_0-auc:0.90769 validation_1-auc:0.83693 [46] validation_0-auc:0.90779 validation_1-auc:0.83686 [47] validation_0-auc:0.90793 validation_1-auc:0.83678 [48] validation_0-auc:0.90831 validation_1-auc:0.83694 [49] validation_0-auc:0.90871 validation_1-auc:0.83676 [50] validation_0-auc:0.90892 validation_1-auc:0.83655 [51] validation_0-auc:0.91070 validation_1-auc:0.83669 [52] validation_0-auc:0.91240 validation_1-auc:0.83641 [53] validation_0-auc:0.91354 validation_1-auc:0.83690 [54] validation_0-auc:0.91389 validation_1-auc:0.83693 [55] validation_0-auc:0.91408 validation_1-auc:0.83681 [56] validation_0-auc:0.91548 validation_1-auc:0.83680 [57] validation_0-auc:0.91560 validation_1-auc:0.83667 [58] validation_0-auc:0.91631 validation_1-auc:0.83664 [59] validation_0-auc:0.91729 validation_1-auc:0.83591 [60] validation_0-auc:0.91765 validation_1-auc:0.83576 [61] validation_0-auc:0.91788 validation_1-auc:0.83534 [62] validation_0-auc:0.91876 validation_1-auc:0.83513 [63] validation_0-auc:0.91896 validation_1-auc:0.83510 [64] validation_0-auc:0.91900 validation_1-auc:0.83508 [65] validation_0-auc:0.91911 validation_1-auc:0.83518 [66] validation_0-auc:0.91975 validation_1-auc:0.83510 [67] validation_0-auc:0.91986 validation_1-auc:0.83523 [68] validation_0-auc:0.92012 validation_1-auc:0.83457 [69] validation_0-auc:0.92019 validation_1-auc:0.83460 [70] validation_0-auc:0.92029 validation_1-auc:0.83446 [71] validation_0-auc:0.92041 validation_1-auc:0.83462 [72] validation_0-auc:0.92093 validation_1-auc:0.83394 [73] validation_0-auc:0.92099 validation_1-auc:0.83410 [74] validation_0-auc:0.92140 validation_1-auc:0.83394 [75] validation_0-auc:0.92148 validation_1-auc:0.83368 [76] validation_0-auc:0.92330 validation_1-auc:0.83413 [77] validation_0-auc:0.92424 validation_1-auc:0.83359 [78] validation_0-auc:0.92512 validation_1-auc:0.83353 [79] validation_0-auc:0.92549 validation_1-auc:0.83293 [80] validation_0-auc:0.92586 validation_1-auc:0.83253 [81] validation_0-auc:0.92686 validation_1-auc:0.83187 [82] validation_0-auc:0.92714 validation_1-auc:0.83230 [83] validation_0-auc:0.92810 validation_1-auc:0.83216 [84] validation_0-auc:0.92832 validation_1-auc:0.83206 [85] validation_0-auc:0.92878 validation_1-auc:0.83196 [86] validation_0-auc:0.92883 validation_1-auc:0.83200 [87] validation_0-auc:0.92890 validation_1-auc:0.83208 [88] validation_0-auc:0.92928 validation_1-auc:0.83174 [89] validation_0-auc:0.92950 validation_1-auc:0.83160 [90] validation_0-auc:0.92958 validation_1-auc:0.83155 [91] validation_0-auc:0.92969 validation_1-auc:0.83165 [92] validation_0-auc:0.92974 validation_1-auc:0.83172 [93] validation_0-auc:0.93042 validation_1-auc:0.83160 [94] validation_0-auc:0.93043 validation_1-auc:0.83150 [95] validation_0-auc:0.93048 validation_1-auc:0.83132 [96] validation_0-auc:0.93094 validation_1-auc:0.83090 [97] validation_0-auc:0.93102 validation_1-auc:0.83091 [98] validation_0-auc:0.93179 validation_1-auc:0.83066 [99] validation_0-auc:0.93255 validation_1-auc:0.83058 [100] validation_0-auc:0.93296 validation_1-auc:0.83029 [101] validation_0-auc:0.93370 validation_1-auc:0.82955 [102] validation_0-auc:0.93369 validation_1-auc:0.82962 [103] validation_0-auc:0.93448 validation_1-auc:0.82893 [104] validation_0-auc:0.93460 validation_1-auc:0.82837 [105] validation_0-auc:0.93494 validation_1-auc:0.82815 [106] validation_0-auc:0.93594 validation_1-auc:0.82744 [107] validation_0-auc:0.93598 validation_1-auc:0.82728 [108] validation_0-auc:0.93625 validation_1-auc:0.82651 [109] validation_0-auc:0.93632 validation_1-auc:0.82650 [110] validation_0-auc:0.93673 validation_1-auc:0.82621 [111] validation_0-auc:0.93678 validation_1-auc:0.82620 [112] validation_0-auc:0.93726 validation_1-auc:0.82591 [113] validation_0-auc:0.93797 validation_1-auc:0.82498 ROC AUC: 0.8413 . from sklearn.model_selection import GridSearchCV # 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소 xgb_clf = XGBClassifier(n_estimators=100) params = {&#39;max_depth&#39;:[5, 7] , &#39;min_child_weight&#39;:[1,3] ,&#39;colsample_bytree&#39;:[0.5, 0.75] } # 하이퍼 파라미터 테스트의 수행속도를 향상 시키기 위해 cv 를 지정하지 않음. gridcv = GridSearchCV(xgb_clf, param_grid=params) gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=&quot;auc&quot;, eval_set=[(X_train, y_train), (X_test, y_test)]) print(&#39;GridSearchCV 최적 파라미터:&#39;,gridcv.best_params_) xgb_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:,1], average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(xgb_roc_score)) . 이전 예제의 ROC AUC보다 파라미터 조정후 성능이 좀 개선 됐음, 앞에서 구한 최적화 파라미터를 기반으로 다른 하이퍼 파라미터를 변경 또는 추가해 다시 최적화를 진행해보자 | . # n_estimators는 1000으로 증가시키고, learning_rate=0.02로 감소, reg_alpha=0.03으로 추가함. xgb_clf = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.02, max_depth=7, min_child_weight=1, colsample_bytree=0.75, reg_alpha=0.03) # evaluation metric을 auc로, early stopping은 200 으로 설정하고 학습 수행. xgb_clf.fit(X_train, y_train, early_stopping_rounds=200, eval_metric=&quot;auc&quot;,eval_set=[(X_train, y_train), (X_test, y_test)]) xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(xgb_roc_score)) . 이전 테스트보다 살짝 향상된 결과를 나타내고 있음. XGBoost가 GBM보다는 빠르지만 아무래도 GBM을 기반으로 하고 있기 때문에 수행시간이 상당히 요구된다. 이 때문에 하이퍼 파라미터를 다양하게 나열해 파라미터를 튜닝하는 것은 많은 시간이 소모. 앙상블 계열 알고리즘에서 하이퍼 파라미터 튜닝으로 성능 수치 개선이 급격하게 되긴 어렵다. . | 튜닝된 모델에서 각 feature의 중요도를 feature 중요도 그래프로 나타내보자 . | . from xgboost import plot_importance import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(1,1,figsize=(10,8)) plot_importance(xgb_clf, ax=ax , max_num_features=20,height=0.4) . LightGBM으로 학습을 수행하고 ROC-AUC를 측정해보자 | . from lightgbm import LGBMClassifier lgbm_clf = LGBMClassifier(n_estimators=500) evals = [(X_test, y_test)] lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;auc&quot;, eval_set=evals, verbose=True) lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(lgbm_roc_score)) . C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:726: UserWarning: &#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;early_stopping()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;early_stopping_rounds&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; C: Users ehfus Anaconda3 envs dv2021 lib site-packages lightgbm sklearn.py:736: UserWarning: &#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. Pass &#39;log_evaluation()&#39; callback via &#39;callbacks&#39; argument instead. _log_warning(&#34;&#39;verbose&#39; argument is deprecated and will be removed in a future release of LightGBM. &#34; . [1] valid_0&#39;s auc: 0.817384 valid_0&#39;s binary_logloss: 0.165046 [2] valid_0&#39;s auc: 0.818903 valid_0&#39;s binary_logloss: 0.160006 [3] valid_0&#39;s auc: 0.827707 valid_0&#39;s binary_logloss: 0.156323 [4] valid_0&#39;s auc: 0.832155 valid_0&#39;s binary_logloss: 0.153463 [5] valid_0&#39;s auc: 0.834677 valid_0&#39;s binary_logloss: 0.151256 [6] valid_0&#39;s auc: 0.834093 valid_0&#39;s binary_logloss: 0.149427 [7] valid_0&#39;s auc: 0.837046 valid_0&#39;s binary_logloss: 0.147961 [8] valid_0&#39;s auc: 0.837838 valid_0&#39;s binary_logloss: 0.146591 [9] valid_0&#39;s auc: 0.839435 valid_0&#39;s binary_logloss: 0.145455 [10] valid_0&#39;s auc: 0.83973 valid_0&#39;s binary_logloss: 0.144486 [11] valid_0&#39;s auc: 0.839799 valid_0&#39;s binary_logloss: 0.143769 [12] valid_0&#39;s auc: 0.840034 valid_0&#39;s binary_logloss: 0.143146 [13] valid_0&#39;s auc: 0.840271 valid_0&#39;s binary_logloss: 0.142533 [14] valid_0&#39;s auc: 0.840342 valid_0&#39;s binary_logloss: 0.142036 [15] valid_0&#39;s auc: 0.840928 valid_0&#39;s binary_logloss: 0.14161 [16] valid_0&#39;s auc: 0.840337 valid_0&#39;s binary_logloss: 0.141307 [17] valid_0&#39;s auc: 0.839901 valid_0&#39;s binary_logloss: 0.141152 [18] valid_0&#39;s auc: 0.839742 valid_0&#39;s binary_logloss: 0.141018 [19] valid_0&#39;s auc: 0.839818 valid_0&#39;s binary_logloss: 0.14068 [20] valid_0&#39;s auc: 0.839307 valid_0&#39;s binary_logloss: 0.140562 [21] valid_0&#39;s auc: 0.839662 valid_0&#39;s binary_logloss: 0.140353 [22] valid_0&#39;s auc: 0.840411 valid_0&#39;s binary_logloss: 0.140144 [23] valid_0&#39;s auc: 0.840522 valid_0&#39;s binary_logloss: 0.139983 [24] valid_0&#39;s auc: 0.840208 valid_0&#39;s binary_logloss: 0.139943 [25] valid_0&#39;s auc: 0.839578 valid_0&#39;s binary_logloss: 0.139898 [26] valid_0&#39;s auc: 0.83975 valid_0&#39;s binary_logloss: 0.139814 [27] valid_0&#39;s auc: 0.83988 valid_0&#39;s binary_logloss: 0.139711 [28] valid_0&#39;s auc: 0.839704 valid_0&#39;s binary_logloss: 0.139681 [29] valid_0&#39;s auc: 0.839432 valid_0&#39;s binary_logloss: 0.139662 [30] valid_0&#39;s auc: 0.839196 valid_0&#39;s binary_logloss: 0.139641 [31] valid_0&#39;s auc: 0.838891 valid_0&#39;s binary_logloss: 0.139654 [32] valid_0&#39;s auc: 0.838943 valid_0&#39;s binary_logloss: 0.1396 [33] valid_0&#39;s auc: 0.838632 valid_0&#39;s binary_logloss: 0.139642 [34] valid_0&#39;s auc: 0.838314 valid_0&#39;s binary_logloss: 0.139687 [35] valid_0&#39;s auc: 0.83844 valid_0&#39;s binary_logloss: 0.139668 [36] valid_0&#39;s auc: 0.839074 valid_0&#39;s binary_logloss: 0.139562 [37] valid_0&#39;s auc: 0.838806 valid_0&#39;s binary_logloss: 0.139594 [38] valid_0&#39;s auc: 0.839041 valid_0&#39;s binary_logloss: 0.139574 [39] valid_0&#39;s auc: 0.839081 valid_0&#39;s binary_logloss: 0.139587 [40] valid_0&#39;s auc: 0.839276 valid_0&#39;s binary_logloss: 0.139504 [41] valid_0&#39;s auc: 0.83951 valid_0&#39;s binary_logloss: 0.139481 [42] valid_0&#39;s auc: 0.839544 valid_0&#39;s binary_logloss: 0.139487 [43] valid_0&#39;s auc: 0.839673 valid_0&#39;s binary_logloss: 0.139478 [44] valid_0&#39;s auc: 0.839677 valid_0&#39;s binary_logloss: 0.139453 [45] valid_0&#39;s auc: 0.839703 valid_0&#39;s binary_logloss: 0.139445 [46] valid_0&#39;s auc: 0.839601 valid_0&#39;s binary_logloss: 0.139468 [47] valid_0&#39;s auc: 0.839318 valid_0&#39;s binary_logloss: 0.139529 [48] valid_0&#39;s auc: 0.839462 valid_0&#39;s binary_logloss: 0.139486 [49] valid_0&#39;s auc: 0.839288 valid_0&#39;s binary_logloss: 0.139492 [50] valid_0&#39;s auc: 0.838987 valid_0&#39;s binary_logloss: 0.139572 [51] valid_0&#39;s auc: 0.838845 valid_0&#39;s binary_logloss: 0.139603 [52] valid_0&#39;s auc: 0.838655 valid_0&#39;s binary_logloss: 0.139623 [53] valid_0&#39;s auc: 0.838783 valid_0&#39;s binary_logloss: 0.139609 [54] valid_0&#39;s auc: 0.838695 valid_0&#39;s binary_logloss: 0.139638 [55] valid_0&#39;s auc: 0.838868 valid_0&#39;s binary_logloss: 0.139625 [56] valid_0&#39;s auc: 0.838653 valid_0&#39;s binary_logloss: 0.139645 [57] valid_0&#39;s auc: 0.83856 valid_0&#39;s binary_logloss: 0.139688 [58] valid_0&#39;s auc: 0.838475 valid_0&#39;s binary_logloss: 0.139694 [59] valid_0&#39;s auc: 0.8384 valid_0&#39;s binary_logloss: 0.139682 [60] valid_0&#39;s auc: 0.838319 valid_0&#39;s binary_logloss: 0.13969 [61] valid_0&#39;s auc: 0.838209 valid_0&#39;s binary_logloss: 0.13973 [62] valid_0&#39;s auc: 0.83806 valid_0&#39;s binary_logloss: 0.139765 [63] valid_0&#39;s auc: 0.838096 valid_0&#39;s binary_logloss: 0.139749 [64] valid_0&#39;s auc: 0.838163 valid_0&#39;s binary_logloss: 0.139746 [65] valid_0&#39;s auc: 0.838183 valid_0&#39;s binary_logloss: 0.139805 [66] valid_0&#39;s auc: 0.838215 valid_0&#39;s binary_logloss: 0.139815 [67] valid_0&#39;s auc: 0.838268 valid_0&#39;s binary_logloss: 0.139822 [68] valid_0&#39;s auc: 0.83836 valid_0&#39;s binary_logloss: 0.139816 [69] valid_0&#39;s auc: 0.838114 valid_0&#39;s binary_logloss: 0.139874 [70] valid_0&#39;s auc: 0.83832 valid_0&#39;s binary_logloss: 0.139816 [71] valid_0&#39;s auc: 0.838256 valid_0&#39;s binary_logloss: 0.139818 [72] valid_0&#39;s auc: 0.838231 valid_0&#39;s binary_logloss: 0.139845 [73] valid_0&#39;s auc: 0.838028 valid_0&#39;s binary_logloss: 0.139888 [74] valid_0&#39;s auc: 0.837912 valid_0&#39;s binary_logloss: 0.139905 [75] valid_0&#39;s auc: 0.83772 valid_0&#39;s binary_logloss: 0.13992 [76] valid_0&#39;s auc: 0.837606 valid_0&#39;s binary_logloss: 0.139899 [77] valid_0&#39;s auc: 0.837521 valid_0&#39;s binary_logloss: 0.139925 [78] valid_0&#39;s auc: 0.837462 valid_0&#39;s binary_logloss: 0.139957 [79] valid_0&#39;s auc: 0.837541 valid_0&#39;s binary_logloss: 0.139944 [80] valid_0&#39;s auc: 0.838013 valid_0&#39;s binary_logloss: 0.13983 [81] valid_0&#39;s auc: 0.83789 valid_0&#39;s binary_logloss: 0.139874 [82] valid_0&#39;s auc: 0.837671 valid_0&#39;s binary_logloss: 0.139975 [83] valid_0&#39;s auc: 0.837707 valid_0&#39;s binary_logloss: 0.139972 [84] valid_0&#39;s auc: 0.837631 valid_0&#39;s binary_logloss: 0.140011 [85] valid_0&#39;s auc: 0.837496 valid_0&#39;s binary_logloss: 0.140023 [86] valid_0&#39;s auc: 0.83757 valid_0&#39;s binary_logloss: 0.140021 [87] valid_0&#39;s auc: 0.837284 valid_0&#39;s binary_logloss: 0.140099 [88] valid_0&#39;s auc: 0.837228 valid_0&#39;s binary_logloss: 0.140115 [89] valid_0&#39;s auc: 0.836964 valid_0&#39;s binary_logloss: 0.140172 [90] valid_0&#39;s auc: 0.836752 valid_0&#39;s binary_logloss: 0.140225 [91] valid_0&#39;s auc: 0.836833 valid_0&#39;s binary_logloss: 0.140221 [92] valid_0&#39;s auc: 0.836648 valid_0&#39;s binary_logloss: 0.140277 [93] valid_0&#39;s auc: 0.836648 valid_0&#39;s binary_logloss: 0.140315 [94] valid_0&#39;s auc: 0.836677 valid_0&#39;s binary_logloss: 0.140321 [95] valid_0&#39;s auc: 0.836729 valid_0&#39;s binary_logloss: 0.140307 [96] valid_0&#39;s auc: 0.8368 valid_0&#39;s binary_logloss: 0.140313 [97] valid_0&#39;s auc: 0.836797 valid_0&#39;s binary_logloss: 0.140331 [98] valid_0&#39;s auc: 0.836675 valid_0&#39;s binary_logloss: 0.140361 [99] valid_0&#39;s auc: 0.83655 valid_0&#39;s binary_logloss: 0.14039 [100] valid_0&#39;s auc: 0.836518 valid_0&#39;s binary_logloss: 0.1404 [101] valid_0&#39;s auc: 0.836998 valid_0&#39;s binary_logloss: 0.140294 [102] valid_0&#39;s auc: 0.836778 valid_0&#39;s binary_logloss: 0.140366 [103] valid_0&#39;s auc: 0.83694 valid_0&#39;s binary_logloss: 0.140333 [104] valid_0&#39;s auc: 0.836749 valid_0&#39;s binary_logloss: 0.14039 [105] valid_0&#39;s auc: 0.836752 valid_0&#39;s binary_logloss: 0.140391 [106] valid_0&#39;s auc: 0.837197 valid_0&#39;s binary_logloss: 0.140305 [107] valid_0&#39;s auc: 0.837141 valid_0&#39;s binary_logloss: 0.140329 [108] valid_0&#39;s auc: 0.8371 valid_0&#39;s binary_logloss: 0.140344 [109] valid_0&#39;s auc: 0.837136 valid_0&#39;s binary_logloss: 0.14033 [110] valid_0&#39;s auc: 0.837102 valid_0&#39;s binary_logloss: 0.140388 [111] valid_0&#39;s auc: 0.836957 valid_0&#39;s binary_logloss: 0.140426 [112] valid_0&#39;s auc: 0.836779 valid_0&#39;s binary_logloss: 0.14051 [113] valid_0&#39;s auc: 0.836831 valid_0&#39;s binary_logloss: 0.140526 [114] valid_0&#39;s auc: 0.836783 valid_0&#39;s binary_logloss: 0.14055 [115] valid_0&#39;s auc: 0.836672 valid_0&#39;s binary_logloss: 0.140585 ROC AUC: 0.8409 . 수행시간이 더 단축됐음을 알 수 있다. . | GridSearchCV로 좀 더 다양한 하이퍼 파라미터에 대한 튜닝을 수행해보자 . | . from sklearn.model_selection import GridSearchCV # 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소 LGBM_clf = LGBMClassifier(n_estimators=200) params = {&#39;num_leaves&#39;: [32, 64 ], &#39;max_depth&#39;:[128, 160], &#39;min_child_samples&#39;:[60, 100], &#39;subsample&#39;:[0.8, 1]} # 하이퍼 파라미터 테스트의 수행속도를 향상 시키기 위해 cv 를 지정하지 않습니다. gridcv = GridSearchCV(lgbm_clf, param_grid=params) gridcv.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=&quot;auc&quot;, eval_set=[(X_train, y_train), (X_test, y_test)]) print(&#39;GridSearchCV 최적 파라미터:&#39;, gridcv.best_params_) lgbm_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:,1], average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(lgbm_roc_score)) . 해당 하이퍼 파라미터를 LightGBM에 적용하고 다시 학습해 ROC-AUC 측정 결과를 도출해보자 | . lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=32, sumbsample=0.8, min_child_samples=100, max_depth=128) evals = [(X_test, y_test)] lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;auc&quot;, eval_set=evals, verbose=True) lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1],average=&#39;macro&#39;) print(&#39;ROC AUC: {0:.4f}&#39;.format(lgbm_roc_score)) . 캐글 신용카드 사기 검출 . 해당 데이터 세트의 레이블인 class 속성은 매우 불균형한 분포를 가지고 있다. 단 0.1%만이 사기 transaction이며 일반적으로 사기 검출(Fraud Detection)이나 이상 검출(Anomaly Detection)과 같은 데이터 세트는 극도로 불균형한 분포를 가질 수밖에 없음. 레이블이 불균형한 분포를 가진 데이터 세트를 학습시킬 때 예측 성능의 문제가 발생할 수 있는데 이는 이상 레이블을 가지는 데이터 건수가 정상 레이블을 가진 데이터 건수에 비해 너무 적기 때문에 발생. 즉 이상 레이블을 가지는 데이터 건수는 매우 적기 때문에 제대로 다양한 유형을 학습하지 못하는 반면 정상 레이블을 가지는 데이터 건수는 매우 많기 때문에 일방적으로 정상 레이블로 치우친 학습을 수행해 제대로 된 이상 데이터 검출이 어려워지기 쉽다. 지도 학습에서 극도로 불균형한 레이블 값 분포로 인한 문제점을 해결하기 위해서는 적절한 학습 데이터를 확보하는 방안이 필요한데 대표적으로 Oversampling과 Undersampling 방법이 있으며 Oversampling 방식이 예측성능상 더 유리한 경우가 많다. . Undersampling : 많은 데이터 세트를 적은 데이터 세트 수준으로 감소시키는 방식. 10,000 : 100 비율이라면 정상 데이터 건수를 100으로 감소시키는 것. 하지만 정상 레이블의 경우 오히려 제대로 된 학습을 수행할 수 없다는 단점이 있어 잘 적용 X . | Oversampling : 이상 데이터와 같이 적은 데이터 세트를 증식하여 학습을 위한 충분한 데이터를 확보하는 방법. 동일한 데이터를 단순 증식하는 것은 과적합(Overfitting) 되기 때문에 원본 데이터의 feature값들을 아주 약간만 변형하여 증식시킴. 대표적으로 SMOTE(Synthetic Minority Over-sampling Technique)이 있다. SMOTE는 적은 데이터 세트에 있는 개별 데이터들의 K 최근접 이웃을 찾아서 이 데이터와 K개 이웃들의 차이를 일정 값으로 만들어서 기존 데이터와 약간 차이가 나는 새로운 데이터들을 생성하는 방식이다. . | . . 데이터 세트를 로딩하고 신용카드 사기 검출 모델을 생성하자 | . import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(&quot;ignore&quot;) %matplotlib inline card_df = pd.read_csv(&#39;./creditcard.csv&#39;) card_df.head(3) . Time V1 V2 V3 V4 V5 V6 V7 V8 V9 ... V21 V22 V23 V24 V25 V26 V27 V28 Amount Class . 0 0.0 | -1.359807 | -0.072781 | 2.536347 | 1.378155 | -0.338321 | 0.462388 | 0.239599 | 0.098698 | 0.363787 | ... | -0.018307 | 0.277838 | -0.110474 | 0.066928 | 0.128539 | -0.189115 | 0.133558 | -0.021053 | 149.62 | 0 | . 1 0.0 | 1.191857 | 0.266151 | 0.166480 | 0.448154 | 0.060018 | -0.082361 | -0.078803 | 0.085102 | -0.255425 | ... | -0.225775 | -0.638672 | 0.101288 | -0.339846 | 0.167170 | 0.125895 | -0.008983 | 0.014724 | 2.69 | 0 | . 2 1.0 | -1.358354 | -1.340163 | 1.773209 | 0.379780 | -0.503198 | 1.800499 | 0.791461 | 0.247676 | -1.514654 | ... | 0.247998 | 0.771679 | 0.909412 | -0.689281 | -0.327642 | -0.139097 | -0.055353 | -0.059752 | 378.66 | 0 | . 3 rows × 31 columns . card_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 284807 entries, 0 to 284806 Data columns (total 31 columns): # Column Non-Null Count Dtype -- -- 0 Time 284807 non-null float64 1 V1 284807 non-null float64 2 V2 284807 non-null float64 3 V3 284807 non-null float64 4 V4 284807 non-null float64 5 V5 284807 non-null float64 6 V6 284807 non-null float64 7 V7 284807 non-null float64 8 V8 284807 non-null float64 9 V9 284807 non-null float64 10 V10 284807 non-null float64 11 V11 284807 non-null float64 12 V12 284807 non-null float64 13 V13 284807 non-null float64 14 V14 284807 non-null float64 15 V15 284807 non-null float64 16 V16 284807 non-null float64 17 V17 284807 non-null float64 18 V18 284807 non-null float64 19 V19 284807 non-null float64 20 V20 284807 non-null float64 21 V21 284807 non-null float64 22 V22 284807 non-null float64 23 V23 284807 non-null float64 24 V24 284807 non-null float64 25 V25 284807 non-null float64 26 V26 284807 non-null float64 27 V27 284807 non-null float64 28 V28 284807 non-null float64 29 Amount 284807 non-null float64 30 Class 284807 non-null int64 dtypes: float64(30), int64(1) memory usage: 67.4 MB . null count 없음. | . from sklearn.model_selection import train_test_split # 인자로 입력받은 DataFrame을 복사 한 뒤 Time 컬럼만 삭제하고 복사된 DataFrame 반환 def get_preprocessed_df(df=None): df_copy = df.copy() df_copy.drop(&#39;Time&#39;, axis=1, inplace=True) return df_copy # 사전 데이터 가공 후 학습과 테스트 데이터 세트를 반환하는 함수. def get_train_test_dataset(df=None): # 인자로 입력된 DataFrame의 사전 데이터 가공이 완료된 복사 DataFrame 반환 df_copy = get_preprocessed_df(df) # DataFrame의 맨 마지막 컬럼이 레이블, 나머지는 피처들 X_features = df_copy.iloc[:, :-1] y_target = df_copy.iloc[:, -1] # train_test_split( )으로 학습과 테스트 데이터 분할. stratify=y_target으로 Stratified 기반 분할 X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target) # 학습과 테스트 데이터 세트 반환 return X_train, X_test, y_train, y_test X_train, X_test, y_train, y_test = get_train_test_dataset(card_df) . 생성한 학습 데이터 세트와 테스트 데이터 세트의 레이블 값 비율이 비슷하게 분할된 것을 알 수 있다. . print(&#39;학습 데이터 레이블 값 비율&#39;) print(y_train.value_counts()/y_train.shape[0] * 100) print(&#39;테스트 데이터 레이블 값 비율&#39;) print(y_test.value_counts()/y_test.shape[0] * 100) . 학습 데이터 레이블 값 비율 0 99.827451 1 0.172549 Name: Class, dtype: float64 테스트 데이터 레이블 값 비율 0 99.826785 1 0.173215 Name: Class, dtype: float64 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/08/intro.html",
            "relUrl": "/2022/01/08/intro.html",
            "date": " • Jan 8, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "2022/01/07/FRI",
            "content": "Random Forest . 배깅은 앞서 소개한 보팅과는 달리 같은 알고리즘으로 여러 개의 분류기를 만들어서 보팅으로 최종 결정하는 알고리즘이다. 그 중 랜덤 포레스트의 기반 알고리즘은 결정 트리이다. 개별 트리가 학습하는 데이터 세트는 전체 데이터 세트에서 일부가 중첩되게 샘플링된 데이터 세트이다. 이렇게 여러 개의 데이터 세트를 중첩되게 분리하는 것을 부트스트래핑 분할 방식이라 한다. 이때 서브 데이터 건수는 전체 데이터 건수와 동일(218p참고),사이킷런은 RandomForestClassifier 클래스를 통해 랜덤 포레스트 기반의 분류를 지원한다. . from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score import pandas as pd import warnings warnings.filterwarnings(&#39;ignore&#39;) def get_human_dataset( ): # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당. feature_name_df = pd.read_csv(&#39;./human_activity/features.txt&#39;,sep=&#39; s+&#39;, header=None,names=[&#39;column_index&#39;,&#39;column_name&#39;]) # 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. new_feature_name_df = get_new_feature_name_df(feature_name_df) # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환 feature_name = new_feature_name_df.iloc[:, 1].values.tolist() # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용 X_train = pd.read_csv(&#39;./human_activity/train/X_train.txt&#39;,sep=&#39; s+&#39;, names=feature_name ) X_test = pd.read_csv(&#39;./human_activity/test/X_test.txt&#39;,sep=&#39; s+&#39;, names=feature_name) # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여 y_train = pd.read_csv(&#39;./human_activity/train/y_train.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) y_test = pd.read_csv(&#39;./human_activity/test/y_test.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) # 로드된 학습/테스트용 DataFrame을 모두 반환 return X_train, X_test, y_train, y_test def get_new_feature_name_df(old_feature_name_df): #column_name으로 중복된 컬럼명에 대해서는 중복 차수 부여, col1, col1과 같이 2개의 중복 컬럼이 있을 경우 1, 2 feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(&#39;column_name&#39;).cumcount(), columns=[&#39;dup_cnt&#39;]) # feature_dup_df의 index인 column_name을 reset_index()를 이용하여 컬럼으로 변환. feature_dup_df = feature_dup_df.reset_index() # 인자로 받은 features_txt의 컬럼명 DataFrame과 feature_dup_df를 조인. new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=&#39;outer&#39;) # 새로운 컬럼명은 앞에 중복 차수를 접미어로 결합. new_feature_name_df[&#39;column_name&#39;] = new_feature_name_df[[&#39;column_name&#39;, &#39;dup_cnt&#39;]].apply(lambda x : x[0]+&#39;_&#39;+str(x[1]) if x[1] &gt;0 else x[0] , axis=1) new_feature_name_df = new_feature_name_df.drop([&#39;index&#39;], axis=1) return new_feature_name_df # 결정 트리에서 사용한 get_human_dataset( )을 이용해 학습/테스트용 DataFrame 반환 X_train, X_test, y_train, y_test = get_human_dataset() # 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가 rf_clf = RandomForestClassifier(random_state=0) rf_clf.fit(X_train , y_train) pred = rf_clf.predict(X_test) accuracy = accuracy_score(y_test , pred) print(&#39;랜덤 포레스트 정확도: {0:.4f}&#39;.format(accuracy)) . 랜덤 포레스트 정확도: 0.9253 . . GridSearchCV를 이용해 랜덤 포레스트의 하이퍼 파라미터를 튜닝해보자 . from sklearn.model_selection import GridSearchCV params = { &#39;n_estimators&#39;:[100], &#39;max_depth&#39; : [6, 8, 10, 12], &#39;min_samples_leaf&#39; : [8, 12, 18 ], &#39;min_samples_split&#39; : [8, 16, 20] } # RandomForestClassifier 객체 생성 후 GridSearchCV 수행 rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1) grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 ) grid_cv.fit(X_train , y_train) print(&#39;최적 하이퍼 파라미터: n&#39;, grid_cv.best_params_) print(&#39;최고 예측 정확도: {0:.4f}&#39;.format(grid_cv.best_score_)) . 최적 하이퍼 파라미터: {&#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 8, &#39;min_samples_split&#39;: 8, &#39;n_estimators&#39;: 100} 최고 예측 정확도: 0.9180 . n_estimators를 300으로 증가시키고, 최적화 하이퍼 파라미터로 다시 RandomForestClassifier를 학습시킨 뒤 이번에는 별도의 데이터 테스트 세트에서 예측을 수행해보자 | . rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8, min_samples_split=8, random_state=0) rf_clf1.fit(X_train , y_train) pred = rf_clf1.predict(X_test) print(&#39;예측 정확도: {0:.4f}&#39;.format(accuracy_score(y_test , pred))) . 예측 정확도: 0.9165 . feature_importances_를 이용해 알고리즘이 선택한 feature의 중요도를 알아보자 | . import matplotlib.pyplot as plt import seaborn as sns ftr_importances_values = rf_clf1.feature_importances_ ftr_importances = pd.Series(ftr_importances_values,index=X_train.columns ) ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] plt.figure(figsize=(8,6)) plt.title(&#39;Feature importances Top 20&#39;) sns.barplot(x=ftr_top20 , y = ftr_top20.index) plt.show() . GBM(Gradient Boosting Machine) . 부스팅 알고리즘은 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식이다. 부스팅의 대표적인 구현은 AdaBoost(Adaptive boosting)와 그래디언트 부스트가 있다. 에이다 부스트는 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 대표적인 알고리즘이다. (223p의 그림을 통해 에이다 부스트의 진행 과정에 대해 알아보자) 예를 들어 첫번째 약한 학습기는 가중치 0.3을 부여하며, 두번째 학습기는 가중치 0.5 이런 식으로 가중치를 늘려가며 학습을 진행한다. . GBM도 에이다 부스트와 유사하나 가중치 업데이트를 경사 하강법(Gradient Descent)을 이용한다. 오류값은 실제 값-예측 값이며, 이 오류값을 최소화하는 방향성을 가지고 반복적으로 가중치 값을 업데이트 하는 것이 경사하강법이다. . 사이킷런은 GBM 기반의 분류를 위해서 GradientBoostingClassifier클래스를 제공. . from sklearn.ensemble import GradientBoostingClassifier import time import warnings warnings.filterwarnings(&#39;ignore&#39;) X_train, X_test, y_train, y_test = get_human_dataset() # GBM 수행 시간 측정을 위함. 시작 시간 설정. start_time = time.time() gb_clf = GradientBoostingClassifier(random_state=0) gb_clf.fit(X_train , y_train) gb_pred = gb_clf.predict(X_test) gb_accuracy = accuracy_score(y_test, gb_pred) print(&#39;GBM 정확도: {0:.4f}&#39;.format(gb_accuracy)) print(&quot;GBM 수행 시간: {0:.1f} 초 &quot;.format(time.time() - start_time)) . GBM 정확도: 0.9389 GBM 수행 시간: 508.8 초 . 8분이나 걸렸음 ; . (트리 기반 자체의 파라미터더 동일하게 적용되며, GBM에서 사용하는 하이퍼 파라미터 및 튜닝에 대해 225p 참고) . GridSearchCV를 이용해 하이퍼 파라미터를 최적화해보자. 다만 꽤 오랜 시간이 걸릴 것으로 예상되어 markdown처리 하겠다. . from sklearn.model_selection import GridSearchCV params = { &#39;n_estimators&#39;:[100, 500], &#39;learning_rate&#39; : [ 0.05, 0.1] } grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=2 ,verbose=1) grid_cv.fit(X_train , y_train) print(&#39;최적 하이퍼 파라미터: n&#39;, grid_cv.best_params_) print(&#39;최고 예측 정확도: {0:.4f}&#39;.format(grid_cv.best_score_)) . 이 설정을 그대로 테스트 데이터 세트에 적용해 예측 정확도를 확인해보자. 동일하게 markdown처리 하겠음 . # GridSearchCV를 이용하여 최적으로 학습된 estimator로 predict 수행. gb_pred = grid_cv.best_estimator_.predict(X_test) gb_accuracy = accuracy_score(y_test, gb_pred) print(&#39;GBM 정확도: {0:.4f}&#39;.format(gb_accuracy)) . 이처럼 GBM은 과적합에도 강한 뛰어난 예측성능을 가진 알고리즘이지만 수행 시간이 오래 걸린다. . 이제 머신러닝 세계에서 가장 각광 받고 있는 두 개의 그래디언트 부스팅 기반 패키지에 대해 알아보자 . . XGBoost . 트리 기반임. 분류에 있어서 일반적으로 다른 머신러닝보다 뛰어난 예측성능을 나타낸다. GBM에 기반하지만 더울 빠른 수행 시간 및 과적합 규제 부재 등의 문제를 해결해 각광받음. 병렬 CPU환경에서 병렬학습이 가능해 수행시간 UP,다만 일반적인 GBM에 비해 빠르다는 것이지 다른 머신러닝 알고리즘(예를 들어 랜덤 포레스트)에 비해 빠르다는 것은 아니다. . 파이썬 래퍼 XGBoost를 적용하여 위스콘신 유방암을 예측해보자 XGBoost의 파이썬 패키지인 xgboost는 자체적으로 교차 검증, 성능 평가, feature중요도등의 시각화 기능이 있음. 또한 조기 중단 기능이 있어 num_rounds로 지정한 부스팅 반복 횟수에 도달하지 않더라도 더 이상 예측 오류가 개선되지 않으면 반복을 끝까지 수행하지 않고 중지해 수행시간을 개선할 수 있음. . import xgboost as xgb from xgboost import plot_importance import pandas as pd import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split import warnings warnings.filterwarnings(&#39;ignore&#39;) dataset = load_breast_cancer() X_features= dataset.data y_label = dataset.target cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names) cancer_df[&#39;target&#39;]= y_label cancer_df.head(3) . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension ... worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension target . 0 17.99 | 10.38 | 122.8 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | 0.07871 | ... | 17.33 | 184.6 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | 0 | . 1 20.57 | 17.77 | 132.9 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | 0.05667 | ... | 23.41 | 158.8 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | 0 | . 2 19.69 | 21.25 | 130.0 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | 0.05999 | ... | 25.53 | 152.5 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | 0 | . 3 rows × 31 columns . 종양의 크기와 모양에 관한 많은 속성이 숫자형 값으로 돼 있음. 맨 마지막 column인 target label값의 종류는 악성인 &#39;malignant&#39;가 0값으로 양성인&#39;benign&#39;이 1값으로 돼있음. | . print(dataset.target_names) print(cancer_df[&#39;target&#39;].value_counts()) . [&#39;malignant&#39; &#39;benign&#39;] 1 357 0 212 Name: target, dtype: int64 . 전체 데이터 세트 중 80%를 학습용으로 20%를 테스트 용으로 분할해보자 | . X_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=156 ) print(X_train.shape , X_test.shape) . (455, 30) (114, 30) . 파이썬 래퍼 XGBoost는 사이킷런과 여러 차이가 있지만 눈에 띄는 차이는 학습용 테스트용 데이터 세트를 위해 별도의 객체인 DMatrix를 생성한다는 점이다. DMatrix는 주로 넘파이 입력 파라미터를 받아서 만들어지는 XGBoost만의 전용 데이터 세트이다. | . dtrain = xgb.DMatrix(data=X_train , label=y_train) dtest = xgb.DMatrix(data=X_test , label=y_test) . params = { &#39;max_depth&#39;:3, &#39;eta&#39;: 0.1, &#39;objective&#39;:&#39;binary:logistic&#39;, &#39;eval_metric&#39;:&#39;logloss&#39;, &#39;early_stoppings&#39;:100 } num_rounds = 400 . 이제 지정된 하이퍼 파라미터로 XGBoost 모델을 학습시켜보자 . # train 데이터 셋은 ‘train’ , evaluation(test) 데이터 셋은 ‘eval’ 로 명기합니다. wlist = [(dtrain,&#39;train&#39;),(dtest,&#39;eval&#39;) ] # 하이퍼 파라미터와 early stopping 파라미터를 train( ) 함수의 파라미터로 전달 xgb_model = xgb.train(params = params , dtrain=dtrain , num_boost_round=num_rounds , evals=wlist ) . train()으로 학습을 수행하면 반복 시 train-error와 eval_logloss가 지속적으로 감소함. xgboost를 이용해 모델의 학습이 완료됐으면 이를 이용해 테스트 데이터 세트에 예측을 수행. . pred_probs = xgb_model.predict(dtest) print(&#39;predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨&#39;) print(np.round(pred_probs[:10],3)) # 예측 확률이 0.5 보다 크면 1 , 그렇지 않으면 0 으로 예측값 결정하여 List 객체인 preds에 저장 preds = [ 1 if x &gt; 0.5 else 0 for x in pred_probs ] print(&#39;예측값 10개만 표시:&#39;,preds[:10]) . predict( ) 수행 결과값을 10개만 표시, 예측 확률 값으로 표시됨 [0.95 0.003 0.9 0.086 0.993 1. 1. 0.999 0.998 0. ] 예측값 10개만 표시: [1, 0, 1, 0, 1, 1, 1, 1, 1, 0] . from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.metrics import precision_score, recall_score from sklearn.metrics import f1_score, roc_auc_score # 수정된 get_clf_eval() 함수 def get_clf_eval(y_test, pred=None, pred_proba=None): confusion = confusion_matrix( y_test, pred) accuracy = accuracy_score(y_test , pred) precision = precision_score(y_test , pred) recall = recall_score(y_test , pred) f1 = f1_score(y_test,pred) # ROC-AUC 추가 roc_auc = roc_auc_score(y_test, pred_proba) print(&#39;오차 행렬&#39;) print(confusion) # ROC-AUC print 추가 print(&#39;정확도: {0:.4f}, 정밀도: {1:.4f}, 재현율: {2:.4f}, F1: {3:.4f}, AUC:{4:.4f}&#39;.format(accuracy, precision, recall, f1, roc_auc)) . . xgboost 패키지에 내장된 시각화 기능을 수행해보자 . import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(figsize=(10, 12)) plot_importance(xgb_model, ax=ax) . &lt;AxesSubplot:title={&#39;center&#39;:&#39;Feature importance&#39;}, xlabel=&#39;F score&#39;, ylabel=&#39;Features&#39;&gt; . 파이썬 래퍼 XGBoost는 사이킷런의 GridSearchCV와 유사하게 데이터 세트에 대한 교차 검증 수행 후 최적 파라미터를 구할 수 있는 방법을 cv() API로 제공. xgb.cv의 반환값은 DataFrame 형태임. . XGBoost를 위한 사이킷런 래퍼는 사이킷런과 호환돼 편리하게 사용할 수 있기 때문에 앞으로는 파이썬 래퍼XGBoost가 아닌 사이킷런 래퍼 XGBoost를 사용하겠음 . XGBoost는 크게 분류를 위한 XGBClassifier, 회귀를 위한 래퍼 클래스인 XGBRegressor가 있음 . from xgboost import XGBClassifier xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3) xgb_wrapper.fit(X_train , y_train) w_preds = xgb_wrapper.predict(X_test) w_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1] . [12:59:52] WARNING: .. src learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective &#39;multi:softprob&#39; was changed from &#39;merror&#39; to &#39;mlogloss&#39;. Explicitly set eval_metric if you&#39;d like to restore the old behavior. . . 앞 예제의 파이썬 래퍼 XGBoost와 동일한 평가 결과가 나옴을 알 수 있다. 사이킷런 래퍼 XGBoost에서도 조기 중단을 수행할 수 있다. 조기 중단 관련 파라미터를 fit()에 입력하면 된다. . from xgboost import XGBClassifier xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3) evals = [(X_test, y_test)] xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=&quot;logloss&quot;, eval_set=evals, verbose=True) ws100_preds = xgb_wrapper.predict(X_test) ws100_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1] . n_estimator를 400으로 설정해도 400번 수행하지 않고 311번 반복한 후 학습을 완료함. (211번에서 311번까지 early_stopping_rounds=100으로 지정된 100번의 반복 동안 성능 평가 지수가 향상되지 않았기 때문에) . get_clf_eval(y_test , ws100_preds, ws100_pred_proba) . 조기 중단이 적용되지 않은 결과보다 약간 저조한 성능을 나타냈지만, 큰 차이는 아님 . 조기 중단값을 너무 급격하게 줄이면 예측 성능이 저하될 우려가 크다. 만일 early_stopping_rounds를 10으로 하면 아직 성능이 향상될 여지가 있음에도 불구하고 10번 반복하는 동안 성능 평가 지표가 향상디지 않으면 반복이 멈춰 버려서 충분한 학습이 되지 않아 예측 성능이 나빠질 수 있다. . feature의 중요도를 시각화하는 모듈인 plot_importance() API에 사이킷런 래퍼 클래스를 입력해도 앞에서 파이썬 래퍼 클래스를 입력한 결과와 똑같이 시각화 결과를 얻을 수 있음 | . from xgboost import plot_importance import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(figsize=(10,12)) # 사이킷런 래퍼 클래스를 입력해도 무방. plot_importance(xgb_wrapper, ax=ax) . 그림이 상당히 지저분해서 markdown처리 하겠음 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/07/FRI.html",
            "relUrl": "/2022/01/07/FRI.html",
            "date": " • Jan 7, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "2022/01/06/THU",
            "content": "결정 트리를 이용해 데이터 세트에 대한 예측 분류를 수행해보자 . import pandas as pd import matplotlib.pyplot as plt # features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드. feature_name_df = pd.read_csv(&#39;./human_activity/features.txt&#39;,sep=&#39; s+&#39;, header=None,names=[&#39;column_index&#39;,&#39;column_name&#39;]) # 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출 feature_name = feature_name_df.iloc[:, 1].values.tolist() print(&#39;전체 피처명에서 10개만 추출:&#39;, feature_name[:10]) feature_name_df.head(20) . 전체 피처명에서 10개만 추출: [&#39;tBodyAcc-mean()-X&#39;, &#39;tBodyAcc-mean()-Y&#39;, &#39;tBodyAcc-mean()-Z&#39;, &#39;tBodyAcc-std()-X&#39;, &#39;tBodyAcc-std()-Y&#39;, &#39;tBodyAcc-std()-Z&#39;, &#39;tBodyAcc-mad()-X&#39;, &#39;tBodyAcc-mad()-Y&#39;, &#39;tBodyAcc-mad()-Z&#39;, &#39;tBodyAcc-max()-X&#39;] . column_index column_name . 0 1 | tBodyAcc-mean()-X | . 1 2 | tBodyAcc-mean()-Y | . 2 3 | tBodyAcc-mean()-Z | . 3 4 | tBodyAcc-std()-X | . 4 5 | tBodyAcc-std()-Y | . 5 6 | tBodyAcc-std()-Z | . 6 7 | tBodyAcc-mad()-X | . 7 8 | tBodyAcc-mad()-Y | . 8 9 | tBodyAcc-mad()-Z | . 9 10 | tBodyAcc-max()-X | . 10 11 | tBodyAcc-max()-Y | . 11 12 | tBodyAcc-max()-Z | . 12 13 | tBodyAcc-min()-X | . 13 14 | tBodyAcc-min()-Y | . 14 15 | tBodyAcc-min()-Z | . 15 16 | tBodyAcc-sma() | . 16 17 | tBodyAcc-energy()-X | . 17 18 | tBodyAcc-energy()-Y | . 18 19 | tBodyAcc-energy()-Z | . 19 20 | tBodyAcc-iqr()-X | . feature명을 보면 인체의 움직임과 관련된 속성의 평균/표준편차가 X,Y,Z축 값으로 돼 있음을 유추할 수 있다. . 주의 : 위에서 feature명을 가지고 있는 feature.txt 파일은 중복된 feature명을 가지고 있음. 이 중복된 feature명들을 이용해 데이터 파일을 데이터 세트 DataFrame에 로드하면 오류가 발생. 따라서 중복된 feature명에 대해서는 원본 feature명에 _1 또는 _2를 부여해 변경한 뒤에 이를 이용해서 데이터를 DataFrame에 로드하자 . 먼저 중복된 feature명이 얼마나 있는지 확인해보자 . feature_dup_df = feature_name_df.groupby(&#39;column_name&#39;).count() print(feature_dup_df[feature_dup_df[&#39;column_index&#39;]&gt;1].count()) feature_dup_df[feature_dup_df[&#39;column_index&#39;]&gt;1].head() . column_index 42 dtype: int64 . column_index . column_name . fBodyAcc-bandsEnergy()-1,16 3 | . fBodyAcc-bandsEnergy()-1,24 3 | . fBodyAcc-bandsEnergy()-1,8 3 | . fBodyAcc-bandsEnergy()-17,24 3 | . fBodyAcc-bandsEnergy()-17,32 3 | . 총 42개의 feature명이 중복돼 있다. . 이 중복된 feature명에 대해서 원본 feature명에 _1또는 _2를 추가 부여해 새로운 feature명을 가지는 DataFrame을 반환하는 함수를 정의하자 . def get_new_feature_name_df(old_feature_name_df): #column_name으로 중복된 컬럼명에 대해서는 중복 차수 부여, col1, col1과 같이 2개의 중복 컬럼이 있을 경우 1, 2 feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(&#39;column_name&#39;).cumcount(), columns=[&#39;dup_cnt&#39;]) # feature_dup_df의 index인 column_name을 reset_index()를 이용하여 컬럼으로 변환. feature_dup_df = feature_dup_df.reset_index() # 인자로 받은 features_txt의 컬럼명 DataFrame과 feature_dup_df를 조인. new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=&#39;outer&#39;) # 새로운 컬럼명은 앞에 중복 차수를 접미어로 결합. new_feature_name_df[&#39;column_name&#39;] = new_feature_name_df[[&#39;column_name&#39;, &#39;dup_cnt&#39;]].apply(lambda x : x[0]+&#39;_&#39;+str(x[1]) if x[1] &gt;0 else x[0] , axis=1) new_feature_name_df = new_feature_name_df.drop([&#39;index&#39;], axis=1) return new_feature_name_df . import pandas as pd def get_human_dataset( ): # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당. feature_name_df = pd.read_csv(&#39;./human_activity/features.txt&#39;,sep=&#39; s+&#39;, header=None,names=[&#39;column_index&#39;,&#39;column_name&#39;]) # 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. new_feature_name_df = get_new_feature_name_df(feature_name_df) # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환 feature_name = new_feature_name_df.iloc[:, 1].values.tolist() # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용 X_train = pd.read_csv(&#39;./human_activity/train/X_train.txt&#39;,sep=&#39; s+&#39;, names=feature_name ) X_test = pd.read_csv(&#39;./human_activity/test/X_test.txt&#39;,sep=&#39; s+&#39;, names=feature_name) # 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여 y_train = pd.read_csv(&#39;./human_activity/train/y_train.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) y_test = pd.read_csv(&#39;./human_activity/test/y_test.txt&#39;,sep=&#39; s+&#39;,header=None,names=[&#39;action&#39;]) # 로드된 학습/테스트용 DataFrame을 모두 반환 return X_train, X_test, y_train, y_test X_train, X_test, y_train, y_test = get_human_dataset() . load한 학습용 feature 데이터 세트를 간략히 살펴보자 . print(&#39; n## 학습 피처 데이터셋 info() n&#39;) print(X_train.info()) . ## 학습 피처 데이터셋 info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7352 entries, 0 to 7351 Columns: 561 entries, tBodyAcc-mean()-X to angle(Z,gravityMean) dtypes: float64(561) memory usage: 31.5 MB None . 학습 데이터 세트는 7352개의 레코드로 561개의 feature를 가지고 있다. 또한 feature가 전부 float 형의 숫자 형이므로 별도의 카테고리 인코딩은 수행할 필요가 없음. . print(y_train[&#39;action&#39;].value_counts()) . 6 1407 5 1374 4 1286 1 1226 2 1073 3 986 Name: action, dtype: int64 . 레이블 값은 1,2,3,4,5,6 즉, 6개 값이고 꽤 고르게 분포돼 있음 . 이제 사이킷런의 DecisionTreeClassifier를 이용해 동작 예측 분류를 수행하보자. 먼저 DecisionTreeClassifier의 하이퍼 파라미터는 모두 default값으로 설정해 수행하고, 이때의 하이퍼 파라미터 값을 모두 추출해보자 . from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정 dt_clf = DecisionTreeClassifier(random_state=156) dt_clf.fit(X_train , y_train) pred = dt_clf.predict(X_test) accuracy = accuracy_score(y_test , pred) print(&#39;결정 트리 예측 정확도: {0:.4f}&#39;.format(accuracy)) # DecisionTreeClassifier의 하이퍼 파라미터 추출 print(&#39;DecisionTreeClassifier 기본 하이퍼 파라미터: n&#39;, dt_clf.get_params()) . 결정 트리 예측 정확도: 0.8548 DecisionTreeClassifier 기본 하이퍼 파라미터: {&#39;ccp_alpha&#39;: 0.0, &#39;class_weight&#39;: None, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: None, &#39;max_leaf_nodes&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;random_state&#39;: 156, &#39;splitter&#39;: &#39;best&#39;} . . 이번에는 결정 트리의 트리 깊이(Tree Depth)가 예측 정확도에 주는 영향을 살펴보자. 결정 트리의 경우 분류를 위해 리프 노드(클래스 결정 노드)가 될 수 있는 적합한 수준이 될 때까지 지속해서 트리의 분할을 수행하면서 깊이가 깊어진다고 했음. 다음은 GridSearchCV를 이용해 사이킷런 결정 트리의 깊이를 조절할 수 있는 하이퍼 파라미터인 max_depth 값을 변화시키면서 예측 성능을 확인해보자. max_depth를 6~24까지 계속 늘리면서 예측 성능을 측정할 것이며, 교차 검증은 4개 세트이다. . from sklearn.model_selection import GridSearchCV params = { &#39;max_depth&#39; : [ 6, 8 ,10, 12, 16 ,20, 24] } grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=&#39;accuracy&#39;, cv=5, verbose=1 ) grid_cv.fit(X_train , y_train) print(&#39;GridSearchCV 최고 평균 정확도 수치:{0:.4f}&#39;.format(grid_cv.best_score_)) print(&#39;GridSearchCV 최적 하이퍼 파라미터:&#39;, grid_cv.best_params_) . Fitting 5 folds for each of 7 candidates, totalling 35 fits GridSearchCV 최고 평균 정확도 수치:0.8513 GridSearchCV 최적 하이퍼 파라미터: {&#39;max_depth&#39;: 16} . max_depth가 16일 때 5개의 폴드 세트의 최고 평균 정확도 결과가 약 85.13%로 도출됐다. max_depth 값 증가에 따라 예측 성능이 어떻게 변하는지 확인해보자 . cv_results_df = pd.DataFrame(grid_cv.cv_results_) # max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출 cv_results_df[[&#39;param_max_depth&#39;, &#39;mean_test_score&#39;]] . param_max_depth mean_test_score . 0 6 | 0.850791 | . 1 8 | 0.851069 | . 2 10 | 0.851209 | . 3 12 | 0.844135 | . 4 16 | 0.851344 | . 5 20 | 0.850800 | . 6 24 | 0.849440 | . mean_test_score는 5개 CV 세트에서 검증용 데이터 세트의 정확도 평균 수치이다. . 깊어진 트리는 학습 데이터 세트에는 올바른 예측 결과를 가져올지 모르지만 검증 데이터 세트에서는 오히려 과적합으로 인한 성능 저하를 유발하게 됨. . 별도의 테스트 데이터 세트에서 max_depth의 변화에 따른 값을 측정해보자 . max_depths = [ 6, 8 ,10, 12, 16 ,20, 24] # max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정 for depth in max_depths: dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156) dt_clf.fit(X_train , y_train) pred = dt_clf.predict(X_test) accuracy = accuracy_score(y_test , pred) print(&#39;max_depth = {0} 정확도: {1:.4f}&#39;.format(depth , accuracy)) . max_depth = 6 정확도: 0.8558 max_depth = 8 정확도: 0.8707 max_depth = 10 정확도: 0.8673 max_depth = 12 정확도: 0.8646 max_depth = 16 정확도: 0.8575 max_depth = 20 정확도: 0.8548 max_depth = 24 정확도: 0.8548 . max_depth = 8 일 때 가장 높은 정확도를 가짐. 8이후부터 정확도 계속 감소중. 즉 트리 깊이가 깊어질수록 테스트 데이터 세트의 정확도는 더 떨어진다. 이처럼 결정 트리는 깊이가 깊어질수록 과적합의 영향력이 커지므로 하이퍼 파라미터를 이용해 깊이를 제어해야 한다. . max_depth와 min_sample_split을 같이 변경하면서 정확도 성능을 튜닝해보자 . params = { &#39;max_depth&#39; : [ 8 , 12, 16 ,20], &#39;min_samples_split&#39; : [16,24], } grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=&#39;accuracy&#39;, cv=5, verbose=1 ) grid_cv.fit(X_train , y_train) print(&#39;GridSearchCV 최고 평균 정확도 수치: {0:.4f}&#39;.format(grid_cv.best_score_)) print(&#39;GridSearchCV 최적 하이퍼 파라미터:&#39;, grid_cv.best_params_) . Fitting 5 folds for each of 8 candidates, totalling 40 fits GridSearchCV 최고 평균 정확도 수치: 0.8549 GridSearchCV 최적 하이퍼 파라미터: {&#39;max_depth&#39;: 8, &#39;min_samples_split&#39;: 16} . max_depth가 8일 때, min_samples_split이 16일 때 가장 최고의 정확도. . 별도 분리된 테스트 데이터 세트에 해당 하이퍼 파라미터를 적용해보자 . (앞 예제의 GridSearchCV객체인 grid_cv의 속성인 best_estimator_는 최적 하이퍼 파라미터인 max_depth와 min_samples_split이 각각 8과 16으로 학습이 완료된 Estimator 객체이다) . best_df_clf = grid_cv.best_estimator_ pred1 = best_df_clf.predict(X_test) accuracy = accuracy_score(y_test , pred1) print(&#39;결정 트리 예측 정확도:{0:.4f}&#39;.format(accuracy)) . 결정 트리 예측 정확도:0.8717 . 마지막으로 결정 트리에서 각 feature의 중요도를 feature_importances_속성을 이용해 알아보자 . import seaborn as sns ftr_importances_values = best_df_clf.feature_importances_ # Top 중요도로 정렬을 쉽게 하고, 시본(Seaborn)의 막대그래프로 쉽게 표현하기 위해 Series변환 ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns ) # 중요도값 순으로 Series를 정렬 ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] plt.figure(figsize=(8,6)) plt.title(&#39;Feature importances Top 20&#39;) sns.barplot(x=ftr_top20 , y = ftr_top20.index) plt.show() . 막대 그래프상에서 확인해보면 이 중 가장 높은 중요도를 가진 Top5의 feature들이 매우 중요하게 규칙 생성에 영향을 미치고 있음을 알 수 있다. . . &#50521;&#49345;&#48660; &#54617;&#49845; . 앙상블 학습(Ensemble Learning)을 통한 분류는 여러 개의 분류기(Classifier)를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법. 단일 분류기(Classifier)보다 신뢰성이 높은 예측값을 얻을 수 있다. . 대부분의 정형 데이터 분류 시에는 앙상블이 뛰어난 성능을 나타내고 있다. . | 앙상블 학습의 유형은 전통적으로 보팅(Voting),배깅(Bagging), 부스팅(Boosting)의 세 가지로 나눌 수 있으며, 이외에도 스태킹(Stacking)을 포함한 다양한 앙상블 방법이 있음. . | . 보팅과 배깅은 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식. . . 보팅의 경우 일반적으로 서로 다른 알고리즘을 가진 분류기를 결합. 배깅은 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 보팅을 수행하는 것. | . 대표적인 배깅 방식이 랜덤 포레스트 알고리즘이다. . | 배깅은 학습하는 데이터 세트가 보팅 방식과는 다르다. 개별 분류기에 할당된 학습 데이터는 원본 학습 데이터를 샘플링해 추출하는데 이렇게 개별 Classifier에게 데이터를 샘플링해서 추출하는 방식을 부트스트래핑(Bootstrapping) 분할 방식이라고 부름 . | 교차 검증이 데이터 세트간 중첩을 허용하지 않는 것과 다르게 배깅 방식은 중첩을 허용. . | 앙상블 학습의 유형 중 부스팅은 여러개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측한 틀린 데이터에 대해서는 올바르게 예측할 수 있도록 다음 분류기에게는 가중치(weight)를 부여하면서 학습과 예측을 진행. . | 대표적 부스틴 모듈로 그래디언트 부스트,XGBoost, LightGBM이 있다. . | 앙상블 학습의 유형 중 스태킹은 여러 가지 다른 모델의 예측 결과값을 다시 학습 데이터로 만들어서 다른 모델(메타 모델)로 재학습시켜 결과를 예측하는 방법 . | . . &#54616;&#46300; &#48372;&#54021;&#44284; &#49548;&#54532;&#53944; &#48372;&#54021; . 하드 보팅 : 다수결, 예측한 결과값들중 다수의 분류기가 결정한 예측값을 최종 보팅 결과값으로 선정 . 소프트 보팅 : 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결과값으로 선정.(일반적으로 이 보팅방식이 보팅 방법으로 적용됨) . 사이킷런은 보팅 방식의 앙상블을 구현한 VotingClassifier 클래스를 제공 | . import pandas as pd from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score cancer = load_breast_cancer() data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names) data_df.head(3) . mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension ... worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension . 0 17.99 | 10.38 | 122.8 | 1001.0 | 0.11840 | 0.27760 | 0.3001 | 0.14710 | 0.2419 | 0.07871 | ... | 25.38 | 17.33 | 184.6 | 2019.0 | 0.1622 | 0.6656 | 0.7119 | 0.2654 | 0.4601 | 0.11890 | . 1 20.57 | 17.77 | 132.9 | 1326.0 | 0.08474 | 0.07864 | 0.0869 | 0.07017 | 0.1812 | 0.05667 | ... | 24.99 | 23.41 | 158.8 | 1956.0 | 0.1238 | 0.1866 | 0.2416 | 0.1860 | 0.2750 | 0.08902 | . 2 19.69 | 21.25 | 130.0 | 1203.0 | 0.10960 | 0.15990 | 0.1974 | 0.12790 | 0.2069 | 0.05999 | ... | 23.57 | 25.53 | 152.5 | 1709.0 | 0.1444 | 0.4245 | 0.4504 | 0.2430 | 0.3613 | 0.08758 | . 3 rows × 30 columns . 로지스틱 회귀와 KNN을 기반으로 하여 소프트 보팅 방식으로 새롭게 보팅 분류기를 만들어보자 . VotingClassifier 클래스를 이용해 보팅 분류기를 만들 수 있으며, VotingClassifier 클래스는 주요 생성 인자로 estimators와 voting값을 입력받음. estimators는 리스트 값으로 보팅에 사용될 여러 개의 Classifier 객체들을 튜플 형식으로 입력 받으며 voting은 hard,soft선택. default는 hard . lr_clf = LogisticRegression() knn_clf = KNeighborsClassifier(n_neighbors=8) # 개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기 vo_clf = VotingClassifier( estimators=[(&#39;LR&#39;,lr_clf),(&#39;KNN&#39;,knn_clf)] , voting=&#39;soft&#39; ) X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2 , random_state= 156) # VotingClassifier 학습/예측/평가. vo_clf.fit(X_train , y_train) pred = vo_clf.predict(X_test) print(&#39;Voting 분류기 정확도: {0:.4f}&#39;.format(accuracy_score(y_test , pred))) import warnings warnings.filterwarnings(&#39;ignore&#39;) # 개별 모델의 학습/예측/평가. classifiers = [lr_clf, knn_clf] for classifier in classifiers: classifier.fit(X_train , y_train) pred = classifier.predict(X_test) class_name= classifier.__class__.__name__ print(&#39;{0} 정확도: {1:.4f}&#39;.format(class_name, accuracy_score(y_test , pred))) . Voting 분류기 정확도: 0.9474 LogisticRegression 정확도: 0.9386 KNeighborsClassifier 정확도: 0.9386 . C: Users ehfus Anaconda3 envs dv2021 lib site-packages sklearn linear_model _logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression n_iter_i = _check_optimize_result( . 보팅 분류기의 정확도가 살짝 높음, 보팅으로 여러 개의 분류기를 결합한다고 해서 무조건 기반이 되는 분류기들 보다 예측 성능이 향상되진 않음. 데이터의 특성과 분포, 다양한 요건에 따라 오히려 기반 분류기 중 가장 좋은 분류기의 성능이 보팅했을 때보다 나을 수 있다. . 머신 러닝 모델의 성능은 이렇게 다양한 테스트 데이터에 의해 검증되므로 어떻게 높은 유연성을 가지고 현실에 대처할 수 있는가가 중요한 ML모델의 평가요소가 된다. 이런 관점에서 편향-분산 트레이드 오프는 ML이 극복해야할 중요 과제이다. .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/06/intro.html",
            "relUrl": "/2022/01/06/intro.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "2022/01/05/WED",
            "content": "지도학습은 레이블, 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식이다. 대표적인 유형으로는 분류(Classification)가 있으며 분류는 학습 데이터로 주어진 데이터의 feature와 레이블값(결정 값, 클래스 값)을 머신 러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것이다. 즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것이다. | . 분류는 다양한 머신러닝 알고리즘으로 구현할 수 있다. 베이즈 통계와 생성 모델에 기반한 나이브 베이즈 | 독립 변수와 종속 변수의 선형 관계성에 기반한 로지스틱 회귀 | 데이터 균일도에 따른 규칙 기반의 결정 트리 | 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신 | 근접 거리를 기준으로 하는 최소 근접 알고리즘 | 심층 연결 기반의 신경망 | 서로 다른 (또는 같은) 머신 러닝 알고리즘을 결합한 앙상블 | . | . 그 중 앙상블에 대해 배워보자 . | 앙상블에는 서로 다른 또는 서로 동일한 알고리즘을 단순히 결합한 형태도 있으나, 일반적으로는 배깅(Bagging)과 부스팅(Boosting) 방식으로 나뉜다. . | 앙상블은 서로 다른 또는 서로 동일한 알고리즘을 결합한다고 했는데 대부분은 동일한 알고리즘을 결합한다. . | . &#44208;&#51221;&#53944;&#47532; . :데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 것이다. : 스무고개 게임과 유사하며 룰 기반의 프로그램에 적용되는 if/else를 자동으로 찾아내 예측을 위한 규칙을 만드는 알고리즘을 이해하면 된다. : 따라서 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우한다. : 규칙 노드로 표시된 노드는 규칙 조건이 되는 것이며, 리프 노드로 표시된 노드는 결절된 라벨 값 즉 클래스 값을 의미한다. : 새러운 규칙 조건 마다 서브 트리가 생성된다. : 데이터 세트에 featrue가 있고 이러한 feature가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어진다. : 하지만 많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 얘기이며 이는 곧 과적합으로 이어지기 쉽다. : 즉 트리의 깊이가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높다. : 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙 필요 . 위에서 분류의 다양한 머신 러닝 알고리즘 중 결정 트리는 데이터 균일도에 따른 규칙 기반이라고 했다. 그렇다면 데이터 균일도는 무엇일까? 균일도라 하면 여러 자료가 균등하게 분포되어 있을수록 균일도가 높다고 착각할 수 있는데 그렇지 않다. 예를 들어 검은 공과 흰공이 있을 때 섞여있을 수록 균일도는 낮은 것이며 오로지 검은공으로만 이루어질 수록 균일도가 높다고 할 수 있다. | . | . 만약 눈을 감고 세 주머니에서 공을 뽑을 때 오로지 검은 공으로만 이루어진 주머니에서 공을 뽑을 때 우리는 검은 공을 쉽게 예측할 수 있다. 만약 검은공과 흰공이 섞여있는 혼잡도가 높고 균일도가 낮은 주머니에서 공을 뽑을 때 검은 공을 예측하려면 더 많은 정보가 필요로 할 것이다. | . 결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만든다. 예를 들어보자, 박스 안에 서른 개의 레고 블록이 있는데 각 레고 블록은 형태 속성으로 동그라미, 네모, 세모 색깔 속성으로 노랑, 빨강, 파랑이 있다. 이 중 노랑색 블록의 경우 모두 동그라미로 구성되고 빨강과 파랑의 경우 동그라미, 네모, 세모가 골고루 섞여 있다고 한다면 각 레고 블록을 형태와 색깔 속성으로 분류하고자 할 떄 가장 첫 번째로 만들어져야 하는 규칙 조건은 if 색깔==&#39;노란색&#39;이 될것이다. 왜냐하면 노란색 블록이면 모두 노란 동그라미 블록으로 가장 쉽게 예측할 수 있고, 그 다음 나머지 블록에 대해 다시 균일도 조건을 찾아 분류하는것이 가장 효율적인 분류 방식이기 때문이다. | . | . 이러한 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 정보 이득($Information$ $Gain$)지수와 지니 계수가 있다. . | 정보 이득 지수 : 1 - 엔트로피 지수(주어진 데이터 집합의 혼잡도) $ to$ 결정 트리는 이 정보 이득 지수로 분할 기준을 정한다. 정보 이득이 높은 속성을 기준으로 분할한다. . | 지니 계수 : 낮을수록 데이터 균일도가 높은 것으로 해석햐 지니 계수가 낮은 속성을 기준으로 분할한다. . | . 결정트리의 일반적인 알고리즘은 데이터 세트를 분할하는 데 가장 좋은 조건, 즉 정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정한다. . - 결정 트리의 가장 큰 단점은 과적합으로 적합도가 떨어진다. 트리의 깊이가 너무 깊어지면 깊어질수록 예측도는 낮아지기에 사전에 트리의 크기를 제한하는 것이 오히려 성능 튜닝에 더 도움이 된다. . 사이킷런은 결정 트리 알고리즘을 구현한 DecisionTreeClassifier(분류를 위한 클래스)와 DecisionTreeRegressor(회귀를 위한 클래스) 클래스를 제공한다. . 두 클래스 모두 동일한 파라미터를 사용하며 파라미터에 대한 설명은 188~189p 참고 . from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import warnings warnings.filterwarnings(&#39;ignore&#39;) # DecisionTree Classifier 생성 dt_clf = DecisionTreeClassifier(random_state=156) # 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리 iris_data = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=.2, random_state=11) # DecisionTreeClassifier 학습 dt_clf.fit(X_train, y_train) from sklearn.tree import export_graphviz # export_graphviz()의 호출결과로 out_file로 지정된 tree.dot 파일을 생성함 export_graphviz(dt_clf, out_file=&quot;tree.dot&quot;, class_names=iris_data.target_names, feature_names= iris_data.feature_names, impurity=True,filled=True) import graphviz # 위에서 생성된 tree.dot 파일을 Graphviz가 읽어서 주피터 노트북상에서 시각화 with open(&quot;tree.dot&quot;) as f: dot_graph = f.read() graphviz.Source(dot_graph) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 petal length (cm) &lt;= 2.45 gini = 0.667 samples = 120 value = [41, 40, 39] class = setosa 1 gini = 0.0 samples = 41 value = [41, 0, 0] class = setosa 0&#45;&gt;1 True 2 petal width (cm) &lt;= 1.55 gini = 0.5 samples = 79 value = [0, 40, 39] class = versicolor 0&#45;&gt;2 False 3 petal length (cm) &lt;= 5.25 gini = 0.051 samples = 38 value = [0, 37, 1] class = versicolor 2&#45;&gt;3 6 petal width (cm) &lt;= 1.75 gini = 0.136 samples = 41 value = [0, 3, 38] class = virginica 2&#45;&gt;6 4 gini = 0.0 samples = 37 value = [0, 37, 0] class = versicolor 3&#45;&gt;4 5 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 3&#45;&gt;5 7 sepal length (cm) &lt;= 5.45 gini = 0.5 samples = 4 value = [0, 2, 2] class = versicolor 6&#45;&gt;7 12 petal length (cm) &lt;= 4.85 gini = 0.053 samples = 37 value = [0, 1, 36] class = virginica 6&#45;&gt;12 8 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 7&#45;&gt;8 9 petal length (cm) &lt;= 5.45 gini = 0.444 samples = 3 value = [0, 2, 1] class = versicolor 7&#45;&gt;9 10 gini = 0.0 samples = 2 value = [0, 2, 0] class = versicolor 9&#45;&gt;10 11 gini = 0.0 samples = 1 value = [0, 0, 1] class = virginica 9&#45;&gt;11 13 sepal length (cm) &lt;= 5.95 gini = 0.444 samples = 3 value = [0, 1, 2] class = virginica 12&#45;&gt;13 16 gini = 0.0 samples = 34 value = [0, 0, 34] class = virginica 12&#45;&gt;16 14 gini = 0.0 samples = 1 value = [0, 1, 0] class = versicolor 13&#45;&gt;14 15 gini = 0.0 samples = 2 value = [0, 0, 2] class = virginica 13&#45;&gt;15 더 이상 자식 노드가 없는 노드는 리프 노드이다. 리프 노드는 최종 클래스(레이블) 값이 결정되는 노드이다. . | 리프 노드가 되려면 오직 하나의 클래스 값으로 최종 데이터가 구성되거나 리프 노드가 될 수 있는 하이퍼 파라미터 조건을 충족하면 된다. . | 자식 노드가 있는 노드는 브랜치 노드이며 자식 노드를 만들기 위한 분할 규칙 조건을 가지고 있다. . | . 루트 노드인 가장 상위 1번 노드를 설명해보자면, smaples :전체 데이터가 120개라는 의미, value의 값 각각은 레이블 0,1,2값이 가지는 데이터 수를 의미함, gini는 지니 계수, class=setosa는 하위 노드를 가질 경우에 setosa의 개수가 41개로 제일 많다는 의미임 . 이처럼 결정 트리는 규칙 생성 로직을 미리 제어하지 않으면 완벽하게 클래스 값을 구별해내기 위해 트리 노드를 계속해서 만들어간다. 이로 인해 결국 매우 복잡한 규칙 트리가 만들어져 모델이 쉽게 과적합되는 문제점을 갖게 됨, 이러한 이유로 결정 트리는 과적합이 상당히 높은 ML알고리즘이다. 이때문에 결정트리 알고리즘을 제어하는 대부분 하이퍼 파라미터는 복잡한 트리가 생성되는 것을 막기 위한 용도이다. . 따라서 결정 트리의 max_depth 하이퍼 파라미터를 제한 없음에서 3개로 설정하면 더 간단한 결정 트리가 된다. . $+$ 결정트리의 또 다른 하이퍼 파라미터 요소인 min_samples_split을 4로 설정하면 서로 다른 클래스가 혼재해 있더라도 데이터 개수가 4보다 낮아지면 더이상 Split하지 않는다. . $+$ 마지막으로 min_samples_leaf 하이퍼 파라미터 변경에 따른 결정 트리의 변화를 살펴보자, 더 이상 자식 노드가 없는 리프 노드는 클래스 결정 값이 되는데, min_samples_leaf는 이 리프 노드가 될 수 있는 샘플 데이터 건수의 최솟값을 지정함 . . 결정 트리는 균일도에 기반해 어떠한 속성을 규칙 조건으로 선택하느냐가 중요한 요건이다 . 사이킷런은 결정 트리 알고리즘이 학습을 통해 규칙을 정하는 데 있어 feature의 중요한 역할 지표를 DecisionTreeClassifier 객체의 featureimportances 속성으로 제공한다. . | 해보자 . | . import seaborn as sns import numpy as np # feature importance 추출 print(&#39;Feature importance: n{}&#39;.format(np.round(dt_clf.feature_importances_,3))) # feature 별 importance 매핑 for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_): print(&#39;{}:{:.3f}&#39;.format(name,value)) # feature importance를 column 별로 시각화 해보자 sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names) . Feature importance: [0.025 0. 0.555 0.42 ] sepal length (cm):0.025 sepal width (cm):0.000 petal length (cm):0.555 petal width (cm):0.420 . &lt;AxesSubplot:&gt; . 이들 중 petal length가 가장 feature importance가 높음을 알 수 있다. . &#44208;&#51221; &#53944;&#47532; &#44284;&#51201;&#54633; (Overfitiing) . 결정 트리가 어떻게 학습 데이터를 분할해 예측을 수행하는지와 이로 인한 과적합 문제를 시각화해보자 . # 이 메서드를 이용해 2개의 feature가 3가지 유형의 클래스 값을 가지는 데이터 세트를 만들고 이를 그래프 형태로 시각화하자. from sklearn.datasets import make_classification import matplotlib.pyplot as plt plt.title(&#39;3 Class values with 2 Features Sample data creation&#39;) # 2차원 시각화를 위해서 feature는 2개, 클래스는 3가지 유형의 분류 샘플 데이터 생성 X_features,y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=0) # 그래프 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스 값은 다른 색 plt.scatter(X_features[:,0],X_features[:,1],marker=&#39;o&#39;,c=y_labels,s=25,edgecolors=&#39;k&#39;) . &lt;matplotlib.collections.PathCollection at 0x165b900c700&gt; . x축 y축은 두 개의 feature가 각 나열된 것이며, 3개의 클래스 값 구분은 색으로 하였음 . 이제 결정 트리를 기반으로 학습해보자 . from sklearn.tree import DecisionTreeClassifier # 특정한 트리 생성 제약 없는 결정 트리의 학습과 결정 경계 시각화 dt_clf=DecisionTreeClassifier().fit(X_features, y_labels) # Classifier의 Decision Boundary를 시각화 하는 함수- 이건 몰라도 될듯 def visualize_boundary(model, X, y): fig,ax = plt.subplots() # 학습 데이타 scatter plot으로 나타내기 ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap=&#39;rainbow&#39;, edgecolor=&#39;k&#39;, clim=(y.min(), y.max()), zorder=3) ax.axis(&#39;tight&#39;) ax.axis(&#39;off&#39;) xlim_start , xlim_end = ax.get_xlim() ylim_start , ylim_end = ax.get_ylim() # 호출 파라미터로 들어온 training 데이타로 model 학습 . model.fit(X, y) # meshgrid 형태인 모든 좌표값으로 예측 수행. xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) # contourf() 를 이용하여 class boundary 를 visualization 수행. n_classes = len(np.unique(y)) contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap=&#39;rainbow&#39;, clim=(y.min(), y.max()), zorder=1) visualize_boundary(dt_clf, X_features,y_labels) . 이번에는 min_samples_leaf=6을 설정해 6개 이하의 데이터는 리프 노드를 생성할 수 잇도록 리프 노트 생성 규칙을 완화한 뒤 하이퍼 파라미터를 변경해 어떻게 결정 기준 경계가 변하는지 살펴보자 . dt_clf = DecisionTreeClassifier(min_samples_leaf=6).fit(X_features, y_labels) visualize_boundary(dt_clf, X_features, y_labels) . 이상치에 크게 반응하지 않으면서 좀 더 일반화 된 분류 규칙에 따라 분류됐음을 알 수 있다. .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/05/intro.html",
            "relUrl": "/2022/01/05/intro.html",
            "date": " • Jan 5, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "2022/01/04/TUE",
            "content": "F1 &#49828;&#53076;&#50612; . - 정밀도와 재현율이 어느 한 쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가진다. . $F1 = 2 * frac{precision * recall}{precision + recall}$ . API : f1_score() . (168~169p 코드 참고) . . ROC &#44257;&#49440;&#44284; AUC &#49828;&#53076;&#50612; . - 이진 분류의 예측 성능 측정에서 중요하게 사용됨 . ROC 곡선은 FPR을 0부터 1까지 변경하면서 TPR의 변화 값을 구함. 분류 결정 임계값을 변경하면 FPR을 0부터 1까지 변경할 수 있음. FPR을 0으로 만드려면 임계값을 1로 지정하면 된다. . API : roc_curve() . 171~172,174p 참고 . 일반적으로 ROC 곡선 자체는 FPR과 TPR의 변화값을 보는 데 이용하며 분류의 성능 지표로 사용되는 것을 ROC 곡선 면적에 기반한 AUC 값으로 결정한다. AUC가 1에 가까울수록 좋은 수치이다. AUC 수치가 커지려면 FPR이 작은 상태에서 얼마나 더 큰 TPR을 얻을 수 있느냐가 관건이다. | . . &#54588;&#47560; &#51064;&#46356;&#50616; &#45817;&#45544;&#48337; &#50696;&#52769; . import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression diabetes_data = pd.read_csv(&#39;diabetes.csv&#39;) print(diabetes_data[&#39;Outcome&#39;].value_counts()) diabetes_data.head(3) . 0 500 1 268 Name: Outcome, dtype: int64 . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . 0 6 | 148 | 72 | 35 | 0 | 33.6 | 0.627 | 50 | 1 | . 1 1 | 85 | 66 | 29 | 0 | 26.6 | 0.351 | 31 | 0 | . 2 8 | 183 | 64 | 0 | 0 | 23.3 | 0.672 | 32 | 1 | . Negative : 500, Positive : 268 | . diabetes_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Pregnancies 768 non-null int64 1 Glucose 768 non-null int64 2 BloodPressure 768 non-null int64 3 SkinThickness 768 non-null int64 4 Insulin 768 non-null int64 5 BMI 768 non-null float64 6 DiabetesPedigreeFunction 768 non-null float64 7 Age 768 non-null int64 8 Outcome 768 non-null int64 dtypes: float64(2), int64(7) memory usage: 54.1 KB . null값 없고, feature타입은 모두 숫자형 | 따라서 별도의 feature incoding은 불필요 | . &#47196;&#51648;&#49828;&#54001; &#54924;&#44480;&#47484; &#51060;&#50857;&#54644; &#50696;&#52769; &#47784;&#45944;&#51012; &#49373;&#49457;&#54644;&#48372;&#51088; . # feature 데이터 세트 X, 레이블 데이터 세트 y를 추출 # 맨 끝이 Outcome 칼럼으로서 레이블 값임. 따라서 그 칼럼의 위치인 -1을 이용해 빼줌 X=diabetes_data.iloc[:,:-1] # feature 데이터 세트 y=diabetes_data.iloc[:,-1] # 레이블 데이터 세트 X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state = 156, stratify=y) # 로지스틱 회귀로 학습, 예측 및 평가 lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) pred = lr_clf.predict(X_test) pred_proba=lr_clf.predict_proba(X_test)[:,1] get_clf_eval(y_test, pred, pred_proba) . 재현율 : 59.26%, 전체 데이터의 65%가Negative이므로 정확도보다는 재현율 성능에 조금 더 초점을 맞추기 위해 임곗값별 정밀도와 재현율 값의 변화를 확인해보자 . pred_proba_c1 = lr_clf.predict_proba(X_test)[:,1] precision_recall_curve_plot(y_test,pred_proba_c1) . 임계값 0.42 정도에서 정밀도와 재현율이 어느 정도 균형을 맞춤. 그렇지만 두 지표 모두 0.7이 안 되는 수치로서 아직도 낮은 지표 값임. 임계값을 임의적으로 조정하기 전에 데이터값을 다시 보자 . diabetes_data.describe() . Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome . count 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | 768.000000 | . mean 3.845052 | 120.894531 | 69.105469 | 20.536458 | 79.799479 | 31.992578 | 0.471876 | 33.240885 | 0.348958 | . std 3.369578 | 31.972618 | 19.355807 | 15.952218 | 115.244002 | 7.884160 | 0.331329 | 11.760232 | 0.476951 | . min 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.000000 | 0.078000 | 21.000000 | 0.000000 | . 25% 1.000000 | 99.000000 | 62.000000 | 0.000000 | 0.000000 | 27.300000 | 0.243750 | 24.000000 | 0.000000 | . 50% 3.000000 | 117.000000 | 72.000000 | 23.000000 | 30.500000 | 32.000000 | 0.372500 | 29.000000 | 0.000000 | . 75% 6.000000 | 140.250000 | 80.000000 | 32.000000 | 127.250000 | 36.600000 | 0.626250 | 41.000000 | 1.000000 | . max 17.000000 | 199.000000 | 122.000000 | 99.000000 | 846.000000 | 67.100000 | 2.420000 | 81.000000 | 1.000000 | . min행이 0이 굉장히 많음. 예를 들어 Glucose는 포도당 수치인데 0인 것은 말이 안 됨. . plt.hist(diabetes_data[&#39;Glucose&#39;],bins=10) . (array([ 5., 0., 4., 32., 156., 211., 163., 95., 56., 46.]), array([ 0. , 19.9, 39.8, 59.7, 79.6, 99.5, 119.4, 139.3, 159.2, 179.1, 199. ]), &lt;BarContainer object of 10 artists&gt;) . 0 값이 일정 수준 존재함을 알 수 있다. . min() 값이 0으로 돼 있는 feature에 대해 0 값의 건수 및 전체 데이터 건수 대비 및 몇 퍼센트의 비율로 존재하는지 확인해보자. . zero_features=[&#39;Glucose&#39;,&#39;BloodPressure&#39;,&#39;SkinThickness&#39;,&#39;Insulin&#39;,&#39;BMI&#39;] # 전체 데이터 건수 total_count=diabetes_data[&#39;Glucose&#39;].count() #feature별로 반복하면서 데이터 값이 0인 데이터 건수를 추출하고, 퍼센트 계산해보자 for feature in zero_features: zero_count = diabetes_data[diabetes_data[feature]==0][feature].count() print(&#39;{} 0건수는 {}, 퍼센트는 {:.2f}%&#39;.format(feature, zero_count, 100*zero_count/total_count)) . Glucose 0건수는 5, 퍼센트는 0.65% BloodPressure 0건수는 35, 퍼센트는 4.56% SkinThickness 0건수는 227, 퍼센트는 29.56% Insulin 0건수는 374, 퍼센트는 48.70% BMI 0건수는 11, 퍼센트는 1.43% . SkimThickness와 Insulin의 0 값은 각각 전체의 약 30,50%로 대단히 많다. 전체 데이터 건수가 약 800개 이므로 많지 않음, 따라서 0 데이터를 일괄적으로 삭제할 경우에는 학습을 효과적으로 수행하기 어렵다. 따라서 위 feature의 0값을 평균으로 대체해보자 | . mean_zero_features = diabetes_data[zero_features].mean() diabetes_data[zero_features]=diabetes_data[zero_features].replace(0,mean_zero_features) . 로지스틱 회귀의 경우 일반적으로 숫자 데이터에 스케일링을 적용하는 것이 좋음. 따라서 0값을 평균값으로 대체한 데이터 세트에 feature 스케일링을 적용해 변환해보자 . X=diabetes_data.iloc[:,:-1] y=diabetes_data.iloc[:,-1] # StandardScaler 클래스를 이용해 feature 데이터 세트에 일괄적으로 스케일링 적용 scaler = StandardScaler() X_scaled = scaler.fit_transform(X) X_train,X_test,y_train,y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=156,stratify=y) # 로지스틱 회귀로 학습, 예측 및 평가 수행 lr_clf=LogisticRegression() lr_clf.fit(X_train,y_train) pred = lr_clf.predict(X_test) pred_proba=lr_clf.predict_proba(X_test)[:,1] get_clf_eval(y_test,pred,pred_proba) . 데이터 변환과 스케일링을 통해 성능 수치가 일정 수준 개선됐지만 재현율 수치는 아직 개선이 더 필요, 분류 결정 임계값을 0.3에서 0.5까지 0.03씩 변화시키면서 재현율과 다른 평가 지표의 값 변화를 출력해보자 . thresholds = [0.3,0.33,0.36,0.39,0.42,0.45,0.5] pred_proba = lr_clf.predict_proba(X_test) get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1),thresholds) . 값을 반환했을 때 정확도와 정밀도를 어느 정도 희생하고 재현율을 높이는 데 가장 좋은 임계값은 0.33으로 재현율 값이 0.7963이다. 하지만 정밀도가 매우 저조해져서 극단적 선택임. 임계값 0.48이 전체적인 성능 평가 지표를 유지하면서 재현율을 약간 향상시키는 좋은 임계값으로 보임. . 181p 코드 참고 . &#51648;&#44552;&#44620;&#51648; &#48516;&#47448;&#50640; &#49324;&#50857;&#46104;&#45716; &#51221;&#54869;&#46020;, &#50724;&#52264; &#54665;&#47148;, &#51221;&#48128;&#46020;, &#51116;&#54788;&#50984;, F1 &#49828;&#53076;&#50612;, ROC-AUC&#50752; &#44057;&#51008; &#49457;&#45733; &#54217;&#44032; &#51648;&#54364;&#44032; &#51080;&#50632;&#51020;. .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/04/intro.html",
            "relUrl": "/2022/01/04/intro.html",
            "date": " • Jan 4, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "2022/01/03/MON",
            "content": "평가 process에 대해 알아보자 . - &#51221;&#54869;&#46020; ? = &#50696;&#52769; &#44208;&#44284;&#44032; &#46041;&#51068;&#54620; &#45936;&#51060;&#53552; &#44148;&#49688; / &#51204;&#52404; &#50696;&#52769; &#45936;&#51060;&#53552; &#44148;&#49688; . - 이진 부류의 경우 데이터 구성에 따라 ML 모델의 성능을 왜곡할 수 있기 때문에 정확도 수치 하나만 가지고 성능을 평가하는 건 위험함 - 그 예를 살펴보자 . from sklearn.base import BaseEstimator import numpy as np class MyDummyClassifier(BaseEstimator): # fit 메서드는 아무것도 학습하지 않음 def fit(self, X, y=None): pass # predict() 메서드는 단순히 Sex feature 1이면 0 그렇지 않으면 1로 예측함 def predict(self, X): pred = np.zeros((X.shape[0],1)) for i in range(X.shape[0]) : if X[&#39;Sex&#39;].iloc[i]==1 : pred[i]=0 else : pred[i]=1 return pred . import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # 원본 데이터를 재로딩, 데이터 가공, 학습 데이터/테스트 데이터 분할 titanic_df= pd.read_csv(&#39;C:/Users/ehfus/Downloads/titanic/train.csv&#39;) y_titanic_df=titanic_df[&#39;Survived&#39;] X_titanic_df=titanic_df.drop(&#39;Survived&#39;,axis=1) X_titanic_df=transform_features(X_titanic_df) # transform_features 함수 정의 안 했기 때문에 에러 발생(140p) x_train,X_test,y_train,y_test = train_test_split(X_titanic_df, y_titanic_df,test_size=.2,random_state=0) # 위에서 생성한 Dummy Classifier를 이용해 학습/예측/평가 수행 myclf=MydummyClassifier() myclf.fit(X_train,y_train) mypredictions=myclf.predict(X_train,y_train) mypredictions = myclf.predict(X_test) print(&#39;Dummy Classifier의 정확도는: {0:.4f}&#39;.format(accuracy_score(y_test,mypredictions))) . Dummy Classifier의 정확도는 0.7877정도 나온다 . 즉 이렇게 단순한 알고리즘으로 예측을 하더라도 데이터의 구성에 따라 정확도의 결과는 약 78%정도로 높은 수치가 나올 수 있음. 따라서 정확도를 지표로 사용할 때는 매우 신중해야 함. . 특히 불균형한 레이블 값 분포에서 ML 모델의 성능을 판단할 경우 적합한 평가 지표가 아님, 예를 들어보자 . from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.base import BaseEstimator from sklearn.metrics import accuracy_score import numpy as np import pandas as pd class MyFakeClassifier(BaseEstimator): def fit(self, X, y): pass # 입력값으로 들어오는 X 데이터 세트의 크기만큼 모두 0값으로 만들어서 반환 def predict(self, X): return np.zeros((len(X),1),dtype=bool) # 사이킷런의 내장 데이터 세트인 load_digits()를 이용해 MNIST 데이터 로딩 digits = load_digits() # digits 번호가 7번이면 True이고 이를 astype(int)로 1로 변환, 7번이 아니면 Fasle이고 0으로 변환. y=(digits.target==7).astype(int) X_train,X_test,y_train,y_test= train_test_split(digits.data,y,random_state=11) # 불균형한 레이블 데이터 분포도 확인 print(&#39;레이블 테스트 세트 크기: &#39;,y_test.shape) print(&#39;테스트 세트 레이블 0과 1의 분포도&#39;) print(pd.Series(y_test).value_counts()) # Dummy Classifier로 학습/예측/정확도 평가 fakeclf=MyFakeClassifier() fakeclf.fit(X_train,y_train) fakepred=fakeclf.predict(X_test) print(&#39;모든 예측을 0으로 하여도 정확도는:{:.3f}&#39;.format(accuracy_score(y_test,fakepred))) . 레이블 테스트 세트 크기: (450,) 테스트 세트 레이블 0과 1의 분포도 0 405 1 45 dtype: int64 모든 예측을 0으로 하여도 정확도는:0.900 . 이처럼 정확도 평가 지표는 불균형한 레이블 데이터 세트에서는 성능 수치로 사용 되어서는 안 된다. 여러가지 분류 지표와 함께 이용하자 | . - &#50724;&#52264;&#54665;&#47148; &#46608;&#45716; &#54844;&#46041;&#54665;&#47148; ? . - 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리고 있는지도 함께 보여주는 지표 - TN,FP,FN,TP로 나뉘며, 앞문자는 예측값과 실제값이 &#39;같은가/ 틀린가&#39;를 의미, 뒤 문자는 예측 결과 값이 부정(0)/긍정(1)을 의미 - 이 값을 조합해 정확도, 정밀도, 재현율 값을 알 수 있음 . from sklearn.metrics import confusion_matrix confusion_matrix(y_test, fakepred) . array([[405, 0], [ 45, 0]], dtype=int64) . TN은 405개, FN은 45개이다. | . 정확도 = 예측 결과와 실제 값이 동일한 건수/전체 데이터 수 = (TN + TP) / (TN + FP + FN + TP) | 정밀도 = TP / (FP + TP) | 재현율 = TP = (FN + TP) | . (158p 참고) . 분류하려는 업무의 특성상 정밀도 또는 재현율이 특별히 강조되어야 할 경우 분류의 결정 임계값(Threshold)을 조정해 정밀도 또는 재현율의 수치를 높일 수 있다. . 개별 데이터 별로 예측 확률을 반환하는 메서드 = predict_proba() predict_proba() 메서드는 학습이 완료된 사이킷런 Classifier 객체에서 호출이 가능하며 테스트 feature 데이터 세트를 파라미터로 입력해주면 테스트 feature 레코드의 개별 클래스 예측 확률을 반환함. predict() 메서드와 유사하지만 단지 반환 결과가 예측 결과 클래스값이 아닌 예측 확률 결과이다. . (160p 참고) . threshold 값을 조정해보자 | . from sklearn.preprocessing import Binarizer X=[[1,-1,2], [2,0,0], [0,1.1,1.2]] # X의 개별 원소들이 threshold 값보다 같거나 작으면 0을 크면 1을 반환 binarizer = Binarizer(threshold=1.1) print(binarizer.fit_transform(X)) . [[0. 0. 1.] [1. 0. 0.] [0. 0. 1.]] . 이제 이 Binarizer를 이용해 사이킷런 predict()의 의사(pseudo)코드를 만들어보자 . from sklearn.preprocessing import Binarizer # Binarizer의 threshold 설정값, 즉 분류 결정 임계값임. custom_threshold=0.5 # predict_proba() 반환값의 두 번째 칼럼, 즉 Positive 클래스 칼럼 하나만 추출해 Binirizer를 적용 pred_proba_1=pred_proba[:,1].reshape(-1,1) binarizer = Binarizer(threshold=cistom_threshold).fit(pred_proba_1) custom_predict = binarizer.transform(pred_proba_1) get_clf_eval(y_test, custom_predict) . 이때 threshold 설정값을 0.4로 설정, 즉 분류 결정 임계값을 낮추면 True값이 많아질 것이고 재현율 값이 올라가고 정밀도는 떨어질 것이다. . 임계값을 0.4에서부터 0.6까지 0.05씩 증가시키며 평가 지표를 조사해보자, 이를 위해 get_eval_by_threshold() 함수를 만듦 . # 테스트를 수행할 모든 임계값을 리스트 객체로 저장 thresholds = [0.4,0.45,0.5,0.55,0.6] def get_eval_by_threshold(y_test,pred_proba_c1,thresholds): # thresholds list 객체 내의 값을 차례로 iteration하면서 Evaluation 수행. for custom_threshold in thresholds: binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) custom_threshold = binarizer.transform(pres_proba_c1) print(&#39;임계값:&#39;,custom_threshold) get_clf_eval(y_test,custom_predict) get_eval_by_threshold(y_test,pred_proba[:,1].reshapep(-1,1), thresholds) . 임계값의 변경은 업무 환경에 맞게 두 개의 수치를 상호 보완할 수 있는 수준에서 적용돼야 한다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/03/intro.html",
            "relUrl": "/2022/01/03/intro.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "2022/01/02/SUN",
            "content": "&#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 데이터 인코딩 - 레이블 인코딩 . | . from sklearn.preprocessing import LabelEncoder items=[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;] # LabelEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행 encoder = LabelEncoder() # 객체 생성 encoder.fit(items) labels=encoder.transform(items) print(&#39;인코딩 변환값: &#39;, labels) print(&#39;-&#39;) print(&#39;인코딩 클래스: &#39;, encoder.classes_) print(&#39;차례대로 0부터 5까지 부여됨&#39;) print(&#39;-&#39;) print(&#39;디코딩 원본값: &#39;,encoder.inverse_transform([0,4,5,5,1,1,2,0])) print(&#39;이렇게 원하는 인코딩 값의 리스트를 통해 디코딩 할 수 있다&#39;) . 인코딩 변환값: [0 1 4 5 3 3 2 2] - 인코딩 클래스: [&#39;TV&#39; &#39;냉장고&#39; &#39;믹서&#39; &#39;선풍기&#39; &#39;전자레인지&#39; &#39;컴퓨터&#39;] 차례대로 0부터 5까지 부여됨 - 디코딩 원본값: [&#39;TV&#39; &#39;전자레인지&#39; &#39;컴퓨터&#39; &#39;컴퓨터&#39; &#39;냉장고&#39; &#39;냉장고&#39; &#39;믹서&#39; &#39;TV&#39;] 이렇게 원하는 인코딩 값의 리스트를 통해 디코딩 할 수 있다 . ***주의*** . 레이블 인코딩이 1,2일때 특정 ML알고리즘에서 가중치가 더 부여되거나 더 중요하게 인식할 가능성이 발생함. 하지만 단순 인코딩 숫자이기에 이러한 현상은 피해야함. 따라서 이러한 레이블 인코딩은 선형 회귀와 같은 ML 알고리즘에는 적용 X, 트리 계열의 알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 레이블 인코딩도 별 문제 X, 원-핫 인코딩은 레이블 인코딩의 이러한 문제점을 해결하기 위한 인코딩 방식임 . . 데이터 인코딩 - 원-핫 인코딩 . | . from sklearn.preprocessing import OneHotEncoder import numpy as np items=[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;] # 먼저 숫자 값으로 변환하기 위해 LabelEncoder로 변환해야함 encoder=LabelEncoder() # 객체 생성 encoder.fit(items) labels=encoder.transform(items) # 꼭 2차원 데이터로 변경해야 함 labels=labels.reshape(-1,1) # 이제 원-핫 인코딩 oh_encoder= OneHotEncoder() # 객체 생성 oh_encoder.fit(labels) oh_labels=oh_encoder.transform(labels) print(&#39;원-핫 인코딩 데이터&#39;) print(oh_labels.toarray()) # array 형태로 print(&#39;원-핫 인코딩 데이터 차원&#39;) print(oh_labels.shape) # 8행 6열의 행렬 형태 . 원-핫 인코딩 데이터 [[1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0.] [0. 0. 0. 1. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0.]] 원-핫 인코딩 데이터 차원 (8, 6) . 이러한 과정을 pandas를 통해 한 번에? = get_dummies . import pandas as pd df=pd.DataFrame({&#39;item&#39;:[&#39;TV&#39;,&#39;냉장고&#39;,&#39;전자레인지&#39;,&#39;컴퓨터&#39;,&#39;선풍기&#39;,&#39;선풍기&#39;,&#39;믹서&#39;,&#39;믹서&#39;]}) pd.get_dummies(df) . item_TV item_냉장고 item_믹서 item_선풍기 item_전자레인지 item_컴퓨터 . 0 1 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 1 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 1 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 1 | . 4 0 | 0 | 0 | 1 | 0 | 0 | . 5 0 | 0 | 0 | 1 | 0 | 0 | . 6 0 | 0 | 1 | 0 | 0 | 0 | . 7 0 | 0 | 1 | 0 | 0 | 0 | . get_dummies()를 이용하면 숫자형 값으로 변환 없이도 바로 변환이 가능함 | . . feature &#49828;&#52992;&#51068;&#47553;&#44284; &#51221;&#44508;&#54868; . feature scaling에는 표준화와 정규화가 있으며, 표준화는 feature각각이 평균0, 분산1인 가우시안 정규 분포를 가진 값으로 변환하는 것을 의미 . $x_i _new$ = $ frac{x_i-mean(x)}{stdev(x)}$ 이며 $stdev(x)$는 표준편차를 의미함. . 일반적으로 정규화는 서로 다른 feature의 크기를 통일하기 위해 크기는 변환해주는 개념. 0~1값으로 변환한다. 즉 개별 데이터의 크기를 모두 똑같은 단위로 변경하는 것. . $x_i _new$ = $ frac{x_i-min(x)}{max(x)-min(x)}$ . 주의 . 사이킷런의 전처리에서 제공하는 Normalizer 모듈과 일반적인 정규화는 약간의 차이가 있음. 사이킷런의 Normalizer 모듈은 선형 대수에서의 정규화 개념이 적용 됐으며, 개별 벡터의 크기를 맞추기 위해 변환하는 것을 의미함. . $x_i _new$ = $ frac{x_i}{ sqrt(x_i^2+y_i^2+z_i^2)}$ . 이를 벡터 정규화로 지칭하자 . . StandardScaler :표준화 . from sklearn.datasets import load_iris iris=load_iris() iris_data=iris.data iris_df=pd.DataFrame(data=iris_data,columns=iris.feature_names) print(&#39;feature들의 평균값&#39;) print(iris_df.mean()) print(&#39; nfeature들의 분산값&#39;) print(iris_df.var()) . feature들의 평균값 sepal length (cm) 5.843333 sepal width (cm) 3.057333 petal length (cm) 3.758000 petal width (cm) 1.199333 dtype: float64 feature들의 분산값 sepal length (cm) 0.685694 sepal width (cm) 0.189979 petal length (cm) 3.116278 petal width (cm) 0.581006 dtype: float64 . from sklearn.preprocessing import StandardScaler scaler=StandardScaler() # 객체 생성 scaler.fit(iris_df) iris_scaled=scaler.transform(iris_df) # transform()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DataFrame으로 변환 iris_df_scaled = pd.DataFrame(data=iris_scaled,columns=iris.feature_names) print(&#39;feature들의 평균값&#39;) print(iris_df_scaled.mean()) print(&#39; nfeature들의 분산값&#39;) print(iris_df_scaled.var()) . feature들의 평균값 sepal length (cm) -1.690315e-15 sepal width (cm) -1.842970e-15 petal length (cm) -1.698641e-15 petal width (cm) -1.409243e-15 dtype: float64 feature들의 분산값 sepal length (cm) 1.006711 sepal width (cm) 1.006711 petal length (cm) 1.006711 petal width (cm) 1.006711 dtype: float64 . 두 셀을 비교해보면 위 셀 $ to$ 아래 셀, 모든 칼럼 값의 평균이 0에 아주 가까운 값으로, 그리고 분산은 1에 아주 가까운 값으로 변환됐음을 알 수 있다. | . MinMaxScaler :정규화 . 데이터 값을 0과 1사이의 범위 값으로 변환한다. (음수 값이 있으면 -1에서 1값으로 변환) | 데이터의 분포가 가우시안 분포가 아닐 경우에 적용 가능 | . from sklearn.preprocessing import MinMaxScaler scaler=MinMaxScaler() scaler.fit(iris_df) iris_scaled=scaler.transform(iris_df) # transform()시 스케일 변환된 데이터 세트가 ndarray로 반환돼 이를 DF으로 변환 iris_df_scaled = pd.DataFrame(data=iris_scaled,columns=iris.feature_names) print(&#39;feature들의 최솟값&#39;) print(iris_df_scaled.min()) print(&#39; nfeature들의 최댓값&#39;) print(iris_df_scaled.max()) . feature들의 최솟값 sepal length (cm) 0.0 sepal width (cm) 0.0 petal length (cm) 0.0 petal width (cm) 0.0 dtype: float64 feature들의 최댓값 sepal length (cm) 1.0 sepal width (cm) 1.0 petal length (cm) 1.0 petal width (cm) 1.0 dtype: float64 . 모든 feature가 0에서 1사이의 값으로 변환되는 스케일링이 적용됐음을 알 수 있다. | . . 유의점 128p 참고 . 학습 데이터로 fit()이 적용된 스케일링 기준 정보를 그대로 테스트 데이터에 적용해야 하며, 그렇지 않으면 학습 데이터와 테스트 데이터의 스케일링 기준 정보가 서로 달라지기 때문에 올바른 예측 결과를 도출하지 못할 수 있다. . test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform()만으로 변환해야한다. . 즉 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리하던가 이것이 여의치 않다면 테스트 데이터 변환 시에는 fit()이나 fit_transform()을 적용하지 않고 학습 데이터로 이미 fit()된 scaler 객체를 이용해 transform()으로 변환 | . . 131p 사이킷런으로 수행하는 타이타닉 생존자 예측, 꼭 한 번 실습해보기 .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/02/intro.html",
            "relUrl": "/2022/01/02/intro.html",
            "date": " • Jan 2, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "2022/01/01/SAT(HappyNewYear)",
            "content": "datail-review 해보자 . feature_names = 높이,가로 길이 이런 것들, data = 각 featuredml 값들, target = 0,1,2...예를 들면 붓꽃의 이름을 대용한 것, target_names = 각 target이 가리키는 이름이 무엇인지? | . . model_selection 모듈은 학습 데이터와 테스트 데이터 세트를 분리하거나 교차 검증 분할 및 평가, 그리고 Estimator의 하이퍼 파라미터를 튜닝하기 위한 다양한 함수와 클래스를 제공, 전체 데이터를 학습 데이터와 테스트 데이터 세트로 분리해주는 train_test_split()부터 살펴보자 . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score iris=load_iris() # 붓꽃 데이터 세트 로딩 dt_clf=DecisionTreeClassifier() train_data=iris.data # 데이터 세트에서 feature만으로 구성된 데이터가 ndarray train_label=iris.target # 데이터 세트에서 label 데이터 dt_clf.fit(train_data, train_label) # 학습 수행중 pred=dt_clf.predict(train_data) # 예측 수행중 // 그런데 학습때 사용했던 train_data를 사용했음 -&gt; 예측도 1 나올 것 print(&#39;예측도: &#39;,accuracy_score(train_label,pred)) . 예측도: 1.0 . 정확도가 100% 나왔음 $ to$ 이미 학습한 학습 데이터 세트를 기반으로 예측했기 때문. 답을 알고 있는데 같은 문제를 낸 것이나 마찬가지 | 따라서 예측을 수행하는 데이터 세트는 학습을 수행한 학습용 데이터 세트가 아닌 전용의 테스트 데이터 세트여야 함. | . from sklearn.model_selection import train_test_split . dt_clf=DecisionTreeClassifier() iris=load_iris() # train_test_split()의 반환값은 튜플 형태이다. 순차적으로 네가지 요소들을 반환한다 X_train,X_test,y_train,y_test=train_test_split(iris.data, iris.target,test_size=0.3,random_state=121) dt_clf.fit(X_train,y_train) pred = dt_clf.predict(X_test) print(&#39;예측 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 예측 정확도: 0.9556 . . 지금까지의 방법은 모델이 학습 데이터에만 과도하게 최적화되어, 실제 예측을 다른 데이터로 수행할 경우에는 예측 성능이 과도하게 떨어지는 과적합이 발생할 수 있다. 즉 해당 테스트 데이터에만 과적합되는 학습 모델이 만들어져 다른 테스트용 데이터가들어올 경우에는 성능이 저하된다. $ to$ 개선하기 위해 교차검증을 이용해 다양한 학습과 평가를 수행해야 한다. . 교차검증? . : 본고사 치르기 전, 여러 모의고사를 치르는 것. 즉 본고사가 테스트 데이터 세트에 대해 평가하는 것이라면 모의고사는 교차 검증에서 많은 학습과 검증 세트에서 알고리즘 학습과 평가를 수행하는 것. . : 학습 데이터 세트를 검증 데이터 세트와 학습 데이터 세트로 분할하여 수행한 뒤, 모든 학습/검증 과정이 완료된 후 최종적으로 성능을 평가하기 위해 테스트 데이터 세트를 마련함. . K fold 교차 검증? . : K개의 데이터 폴드 세트를 만들어서 K번만큼 각 폴드 세트에 학습과 검증, 평가를 반복적으로 수행 / 개괄적 과정은 교재 104 참고 . 실습해보자 | . import numpy as np . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import KFold # 위에서는 trian_test_split을 import했었음 iris=load_iris() # 붓꽃 데이터 세트 로딩 features=iris.data label=iris.target dt_clf=DecisionTreeClassifier(random_state=156) kfold=KFold(n_splits=5) # KFold 객체 생성 cv_accuracy=[] # fold set별 정확도를 담을 리스트 객체 생성 print(&#39;붓꽃 데이터 세트 크기:&#39;,features.shape[0]) . 붓꽃 데이터 세트 크기: 150 . . kfold=KFold(n_splits=5) . 로 KFold객체를 생성했으니 객체의 split()을 호출해 전체 붓꽃 데이터를 5개의 fold 데이터 세트로 분리하자. 붓꽃 데이터 세트 크기가 150개니 120개는 학습용, 30개는 검증 테스트 데이터 세트이다. . n_iter=0 for train_index,test_index in kfold.split(features): # kfold.split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출 X_train, X_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] # 학습 및 예측 dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) n_iter+=1 # 반복 시마다 정확도 측정 accuracy = np.round(accuracy_score(y_test,pred),4) train_size = X_train.shape[0] test_size = X_test.shape[0] print(&#39; n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기 :{2}, 검증 데이터 크기 :{3}&#39;.format(n_iter,accuracy,train_size,test_size)) print(&#39;#{0} 검증 세트 인덱스:{1}&#39;.format(n_iter, test_index)) cv_accuracy.append(accuracy) # 개별 iteration별 정확도를 합하여 평균 정확도 계산 print(&#39; n *Conclusion* 평균 검증 정확도:&#39;, np.mean(cv_accuracy)) . #1 교차 검증 정확도 :1.0, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #1 검증 세트 인덱스:[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29] #2 교차 검증 정확도 :0.9667, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #2 검증 세트 인덱스:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59] #3 교차 검증 정확도 :0.8667, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #3 검증 세트 인덱스:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89] #4 교차 검증 정확도 :0.9333, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #4 검증 세트 인덱스:[ 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119] #5 교차 검증 정확도 :0.7333, 학습 데이터 크기 :120, 검증 데이터 크기 :30 #5 검증 세트 인덱스:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] *Conclusion* 평균 검증 정확도: 0.9 . . 교차 검증시마다 검증 세트의 인덱스가 달라짐을 알 수 있다. . | 검증세트 인덱스를 살펴보면 104p에서 설명한 그림의 설명과 유사함 . | . . Stratified K 폴드 . : 불균형한 분포도를가진 레이블(결정 클래스) 데이터 집합을 위한 K 폴드 방식이다. 불균형한 분포도를 가진 레이블 데이터 집합은 특정 레이블 값이 특이하게 많거나 또는 적어서 분포가 한쪽으로 치우치는 것을 말함 . 가령 대출 사기 데이터를 예측한다고 가정해보자, 이 데이터 세트는 1억건이고 수십개의 feature와 대출 사기 여부를 뜻하는 label(정상 대출0, 대출사기 : 1)로 구성돼 있다. K폴드로 랜덤하게 학습 및 테스트 세트의 인덱스를 고르더라도 레이블 값인 0과1의 비율을 제대로 반영하지 못하게 됨. 따라서 원본 데이터와 유사한 대출 사기 레이블 값의 분포를 학습/테스트 세트에도 유지하는 게 매우 중요 . Stratified K 폴드는 이처럼 K폴드가 레이블 데이터 집합이 원본 데이터 집합의 레이블 분포를 학습 및 테스트 세트에 제대로 분배하지 못하는 경우의 문제를 해결해줌 | . 붓꽃 데이터 세트를 DataFrame으로 생성하고 레이블 값의 분포도를 먼저 확인해보자 . import pandas as pd iris=load_iris() iris_df=pd.DataFrame(data=iris.data,columns=iris.feature_names) iris_df[&#39;label&#39;]=iris.target print(iris_df[&#39;label&#39;].value_counts(),&#39; n&#39;) . 0 50 1 50 2 50 Name: label, dtype: int64 . label값은 모두 50개로 분배되어 있음 | . kfold=KFold(n_splits=3) n_iter=0 for train_index, test_index in kfold.split(iris_df): n_iter+=1 label_train = iris_df[&#39;label&#39;].iloc[train_index] label_test=iris_df[&#39;label&#39;].iloc[test_index] print(&#39;## 교차 검증: {}&#39;.format(n_iter)) print(&#39;학습 레이블 데이터 분포: n&#39;, label_train.value_counts()) print(&#39;검증 레이블 데이터 분포: n&#39;, label_test.value_counts()) print(&#39;&#39;) . ## 교차 검증: 1 학습 레이블 데이터 분포: 1 50 2 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 50 Name: label, dtype: int64 ## 교차 검증: 2 학습 레이블 데이터 분포: 0 50 2 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 1 50 Name: label, dtype: int64 ## 교차 검증: 3 학습 레이블 데이터 분포: 0 50 1 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 2 50 Name: label, dtype: int64 . 교차 검증 시마다 3개의 폴드 세트로 만들어지는 학습 레이블과 검증 레이블이 완전히 다른 값으로 추출되었다. 예를 들어 첫번째 교차 검증에서는 학습 레이블의 1,2값이 각각 50개가 추출되었고 검증 레이블의 0값이 50개 추출되었음, 즉 학습레이블은 1,2 밖에 없으므로 0의 경우는 전혀 학습하지 못함. 반대로 검증 레이블은 0밖에 없으므로 학습 모델은 절대 0을 예측하지 못함. 이런 유형으로 교차 검증 데이터 세트를 분할하면 검증 예측 정확도는 0이 될 수밖에 없다. | . StratifiedKFold는 이렇게 KFold로 분할된 레이블 데이터 세트가 전체 레이블 값의 분포도를 반영하지 못하는 문제를 해결함. | . . 실습해보자 . from sklearn.model_selection import StratifiedKFold skf=StratifiedKFold(n_splits=3) n_iter=0 # split 메소드에 인자로 feature데이터 세트뿐만 아니라 레이블 데이터 세트도 반드시 넣어줘야함 for train_index,test_index in skf.split(iris_df,iris_df[&#39;label&#39;]): n_iter+=1 label_train=iris_df[&#39;label&#39;].iloc[train_index] label_test=iris_df[&#39;label&#39;].iloc[test_index] print(&#39;## 교차검증: {}&#39;.format(n_iter)) print(&#39;학습 레이블 데이터 분포: n&#39;, label_train.value_counts()) print(&#39;검증 레이블 데이터 분포: n&#39;, label_test.value_counts()) print(&#39;--&#39;) . ## 교차검증: 1 학습 레이블 데이터 분포: 2 34 0 33 1 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 17 1 17 2 16 Name: label, dtype: int64 -- ## 교차검증: 2 학습 레이블 데이터 분포: 1 34 0 33 2 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 17 2 17 1 16 Name: label, dtype: int64 -- ## 교차검증: 3 학습 레이블 데이터 분포: 0 34 1 33 2 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 1 17 2 17 0 16 Name: label, dtype: int64 -- . 학습 레이블과 검증 레이블 데이터 값의 분포도가 동일하게 할당됐음을 알 수 있다. 이렇게 분할이 되어야 레이블 값 0,1,2를 모두 학습할 수 있고 이에 기반해 검증을 수행할 수 있다. | . 이제 StratifiedKFold를 이용해 붓꽃 데이터를 교차 검증해보자 | . df_clf=DecisionTreeClassifier(random_state=156) skfold=StratifiedKFold(n_splits=3) n_iter=3 cv_accuracy=[] # StratifiedKFol의 split() 호출시 반드시 레이블 데이터 세트도 추가 입력 필요 for train_index, test_ondex in skfold.split(features, label): # split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출 X_train,X_test=features[train_index],features[test_index] y_train,y_test=label[train_index], label[test_index] # 학습 및 예측 df_clf.fit(X_train,y_train) pred=dt_clf.predict(X_test) # 반복시마다 정확도 측정 n_iter+=1 accuracy=np.around(accuracy_score(y_test,pred),4) train_size=X_train.shape[0] test_size = X_test.shape[0] print(&#39; n#{} 교차 검증 정확도 : {}, 학습 데이터 크기 : {}, 검증 데이터 크기 : {}&#39;.format(n_iter,accuracy,train_size,test_size)) print(&#39;#{} 검증 세트 인덱스: {}&#39;.format(n_iter, test_index)) cv_accuracy.append(accuracy) # 교차 검증별 정확도 및 평균 정확도 계산 print(&#39; n## 교차 검증별 정확도:&#39;, np.around(cv_accuracy,4)) print(&#39;## 평균 검증 정확도:&#39;,np.mean(cv_accuracy)) . #4 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #4 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92] ## 평균 검증 정확도: 0.92 #5 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #5 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92 0.92] ## 평균 검증 정확도: 0.92 #6 교차 검증 정확도 : 0.92, 학습 데이터 크기 : 100, 검증 데이터 크기 : 50 #6 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.92 0.92 0.92] ## 평균 검증 정확도: 0.92 . . &#44368;&#52264; &#44160;&#51613;&#51012; &#48372;&#45796; &#44036;&#54200;&#54616;&#44172; - cross_val_score() . from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score,cross_validate from sklearn.datasets import load_iris iris_data=load_iris() dt_clf = DecisionTreeClassifier(random_state=156) data= iris_data.data label=iris_data.target # 성능 지표는 정확도 (accuracy), 교차 검증 세트는 3개 scores = cross_val_score(dt_clf, data, label, scoring=&#39;accuracy&#39;, cv=3) print(&#39;교차 검증별 정확도: &#39;,np.round(scores,4)) print(&#39;평균 검증 정확도: &#39;,np.round(np.mean(scores),4)) . 교차 검증별 정확도: [0.98 0.94 0.98] 평균 검증 정확도: 0.9667 . cv로 지정된 횟수만큼 scoring 파라미터로 지정된 평가지표로 평가 결과값을 배열로 반환 | . . GridSearchCV - &#44368;&#52264; &#44160;&#51613;&#44284; &#52572;&#51201; &#54616;&#51060;&#54140; &#54028;&#46972;&#48120;&#53552; &#53916;&#45789;&#51012; &#46041;&#49884;&#50640; . 하이퍼 파라미터? 머신러닝 알고리즘을 구성하는 주요 구성 요소이며, 이 값을 조정해 알고리즘의 예측 성능을 개선할 수 있음 | . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import GridSearchCV # 데이터를 로딩하고 학습 데이터와 테스트 데이터 분리 iris_data = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data,iris_data.target, test_size=0.2, random_state=121) dtree= DecisionTreeClassifier() # 파라미터를 딕셔너리 형태로 설정 parameters = {&#39;max_depth&#39; : [1,2,3], &#39;min_samples_split&#39; : [2,3]} import pandas as pd # param_grid의 하이퍼 파라미터를 3개의 train, test set fold로 나누어 테스트 수행 설정 # rifit=True가 default이며, 이때 가장 젛은 파라미터 설정으로 재학습시킴 grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True) # 붓꽃 학습 데이터로 param_grid의 하이퍼 파라미터를 순차적으로 학습/평가 grid_dtree.fit(X_train,y_train) #GridSearchCV 결과를 추출해 DataFrame으로 변환 scores_df = pd.DataFrame(grid_dtree.cv_results_) scores_df[[&#39;params&#39;,&#39;mean_test_score&#39;,&#39;rank_test_score&#39;,&#39;split0_test_score&#39;,&#39;split1_test_score&#39;,&#39;split2_test_score&#39;]] . params mean_test_score rank_test_score split0_test_score split1_test_score split2_test_score . 0 {&#39;max_depth&#39;: 1, &#39;min_samples_split&#39;: 2} | 0.700000 | 5 | 0.700 | 0.7 | 0.70 | . 1 {&#39;max_depth&#39;: 1, &#39;min_samples_split&#39;: 3} | 0.700000 | 5 | 0.700 | 0.7 | 0.70 | . 2 {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 2} | 0.958333 | 3 | 0.925 | 1.0 | 0.95 | . 3 {&#39;max_depth&#39;: 2, &#39;min_samples_split&#39;: 3} | 0.958333 | 3 | 0.925 | 1.0 | 0.95 | . 4 {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2} | 0.975000 | 1 | 0.975 | 1.0 | 0.95 | . 5 {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 3} | 0.975000 | 1 | 0.975 | 1.0 | 0.95 | . print(&#39;GridSearchCV 최적 파라미터:&#39;, grid_dtree.best_params_) print(&#39;GridSearchCV 최고 정확도:{:4f}&#39;.format(grid_dtree.best_score_)) . GridSearchCV 최적 파라미터: {&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2} GridSearchCV 최고 정확도:0.975000 . 인덱스 4,5rk rank_test_score가 1인 것으로 보아 공동 1위이며 예측 성능 1등을 의미함. | 열 4,5,6은 cv=3 이라서 열2는 그 세개의 평균을 의미 | . estimator = grid_dtree.best_estimator_ # GridSearchCV의 best_estimator_는 이미 최적 학습이 됐으므로 별도 학습이 필요없음 pred = estimator.predict(X_test) print(&#39;테스트 데이터 세트 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 테스트 데이터 세트 정확도: 0.9667 . 일반적으로 학습 데이터를 GridSearchCV를 이용해 최적 하이퍼 파라미터 튜닝을 수행한 뒤에 별도의 테스트 세트에서 이를 평가하는 것이 일반적인 머신 러닝 모델 적용 방법이다. | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2022/01/01/intro.html",
            "relUrl": "/2022/01/01/intro.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "2021/12/31/FRI",
            "content": "Scikit-Learn :파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리 $ to$ 머신러닝을 위한 다양한 알고리즘과 편리한 프레임 워크, API를 제공 . import sklearn . &#48531;&#44867; &#54408;&#51333; &#50696;&#52769;&#54616;&#44592; . 붓꽃 데이터 세트로 붓꽃의 품종을 분류 . | 분류(Classification)는 대표적인 지도학습(Supervised Learning) 방법 중 하나 . | 지도학습은 학습을 위한 다양한 feature와 분류 결정값인 레이블 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터 세트에서 미지의 레이블을 예측 . | 학습을 위해 주어진 데이터 세트를 학습 데이터 세트, 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세트를 테스트 데이터 세트라 함 . | . . sklearn.datasets내의 모듈은 사이킷런에서 자체적으로 제공하는 데이터 세트를 생성하는 모듈의 모임 . | sklearn.tree내의 모듈은 트리 기반 ML 알고리즘을 구현한 클래스의 모임 . | sklearn.model_selection은 학습 데이터와 검증 데이터, 예측 데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가하기 위한 다양한 모듈의 모임 . | 하이퍼 파라미터 : 머신 러닝 알고리즘별로 최적의 학습을 위해 직접 입력하는 파라미터를 통칭, 하이퍼 파라미터를 통해 머신 러닝 알고리즘의 성능을 튜닝할 수 있다. . | . from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split import pandas as pd . iris = load_iris() # 붓꽃 데이터 세트 로딩 . iris.keys() # 따라서 iris.키이름 또는 iris[&#39;키이름&#39;]을 통해 해당 값들을 확인할 수 있다. . dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;, &#39;data_module&#39;]) . iris_data = iris.data # iris 데이터 세트에서 feature만으로 구성된 데이터를 numpy로 로딩 . iris_label = iris.target # 데이터 세트에서 레이블(결정값) 데이터를 numpy로 로딩 . iris.target_names . array([&#39;setosa&#39;, &#39;versicolor&#39;, &#39;virginica&#39;], dtype=&#39;&lt;U10&#39;) . 붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환 | . iris.feature_names . [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] . iris_df = pd.DataFrame(data=iris_data,columns=iris.feature_names) . iris_df . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) . 0 5.1 | 3.5 | 1.4 | 0.2 | . 1 4.9 | 3.0 | 1.4 | 0.2 | . 2 4.7 | 3.2 | 1.3 | 0.2 | . 3 4.6 | 3.1 | 1.5 | 0.2 | . 4 5.0 | 3.6 | 1.4 | 0.2 | . ... ... | ... | ... | ... | . 145 6.7 | 3.0 | 5.2 | 2.3 | . 146 6.3 | 2.5 | 5.0 | 1.9 | . 147 6.5 | 3.0 | 5.2 | 2.0 | . 148 6.2 | 3.4 | 5.4 | 2.3 | . 149 5.9 | 3.0 | 5.1 | 1.8 | . 150 rows × 4 columns . iris_df[&#39;label&#39;]=iris_label . iris_df.head(3) . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) label . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . feature : columns를 의미함 | label(결정값) : 0,1,2 세 가지 값응로 돼 있으며 순서대로 Setosa,versicolor,virginica 품종을 의미 | . . 학습용 데이터와 테스트용 데이터를 분리해보자 | . 학습용 데이터와 테스트용 데이터는 반드시 분리해야 함, 이를 위해 Scikit-learn에선 train_test_split() API를 제공, 해당 API를 이용하면 학습 데이터와 테스트 데이터를 test_size 파라미터 입력 값의 비율로 쉽게 분할함 | . 예를 들어보자, teat_size=0.2로 입력 파라미터를 설정하면 전체 데이터 중 테스트 데이터가 20%, 학습 데이터가 80%로 데이터를 분할함 | . X_train,X_test,y_train,y_test=train_test_split(iris_data,iris_label,test_size=0.2, random_state=11) . train_test_split의 첫 번째 파라미터인 iris_data는 feature 데이터 세트이며, 두 번째 파라미터인 iris_label은 Label 데이터 세트. random_state=11은 호출할 때마다 같은 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 발생 값. . | train_test_split은 호출 시 무작위로 데이터를 분리하므로 random_state를 지정하지 않으면 수행할 때마다 다른 학습/테스트 용 데이터를 만듦. . | X_train,X_test,y_train,y_test = 학습용 feature데이터 세트, 테스트용 feature데이터 세트, 학습용 레이블 데이터 세트, 테스트용 레이블 데이터 세트를 의미 . | . . 이제 이 데이터를 기반으로 머신 러닝 분류 알고리즘의 하나인 의사 결정 트리를 이용해 학습과 예측을 수행 . dt_clf=DecisionTreeClassifier(random_state=11) . dt_clf.fit(X_train,y_train) # 학습용 feature 데이터 세트와 학습용 레이블 데이터 세트를 입력해 학습 수행중 . DecisionTreeClassifier(random_state=11) . 학습 완료/ 예측을 수행해야하는데 학습 데이터가 아닌 다른 데이터를 이용해야 하며, 일반적으로 테스트 데이터 세트를 이용함 | . pred=dt_clf.predict(X_test) # 예측 수행 중 # 예측 label data set . 예측 성능 평가, 여러 평가 방법 중 정확도를 측정해보자. 정확도는 예측 결과가 실제 레이블 값과 얼마나 정확하게 맞는지를 평가하는 지표 | . from sklearn.metrics import accuracy_score print(&#39;예측 정확도: {:.4f}&#39;.format(accuracy_score(y_test,pred))) . 예측 정확도: 0.9333 . 학습한 의사 결정 트리의 알고리즘 예측 정확도가 약 93.33% | . . Conclusion . 1) 데이터 세트 분리 : 데이터를 학습 데이터와 테스트 데이터로 분리 2) 모델 학습 : 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습 3) 예측 수행 : 학습된 ML 모델을 이용해 테스트 데이터의 분류(즉, 붓꽃 종류)를 예측 4) 평가 : 이렇게 예측된 결과값과 테스트 데이터의 실제 결과값을 비교해 ML 모델 성능을 평가 . . 간단한 실습을 해보았으니 전체적 틀을 review해보자 . ML 모델 학습을 위해서 fit(), 학습된 모델의 예측을 위해 predict()를 사용 | Scikit Learn에서는 분류 알고리즘을 구현한 클래스를 Classifier로, 그리고 회귀 알고리즘을 구현한 클래스를 Regressor로 지칭 | Classifier와 Regressor를 합쳐서 Estimator 클래스라고 부름. 즉, 지도학습의 모든 알고리즘을 구현한 클래스를 통칭해서 Estimator라고 부름 | . Scikit-Learn의 다양한 모듈은 교재 94p,95p 참고 . 머신러닝 모델을 구축하는 주요 프로세스 = feature의 가공, 변경, 추출을 수행하는 feature processing, ML 알고리즙 학습/예측 수행, 그리고 모델 평가 단계를 반복적으로 수행하는 것 | . 사이킷런에 내장되어 있는 데이터 세트는 일반적으로 dict형태 | 이때 key는 data(feature의 데이터 세트),target(레이블 값, 숫자 결과값 데이터 세트),target_names(개별 레이블의 이름), feature_names(feature의 이름), DESCR 데이터 세트에 대한 설명과 각 feature의 설명 . | 앞에 두 key는 ndarray, 다음 두개는 ndarray 또는 list 그 다음은 str . | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2021/12/31/intro.html",
            "relUrl": "/2021/12/31/intro.html",
            "date": " • Dec 31, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "2021/12/30/THU",
            "content": "&#45936;&#51060;&#53552;&#50640; &#47784;&#45944;&#51012; &#47582;&#52632;&#45796; . 애플리케이션을 수정하지 않고도 데이터를 기반으로 패턴을 학습하고 결과를 예측하는 알고리즘 기법을 통칭 | . 데이터 기반으로 통계적인 신뢰도를 강화하고 예측 오류를 최소화하기 위한 다양한 수학적 기법을 적용해 데이터 내의 패턴을 스스로 인지하고 신뢰도 있는 예측 결과를 도출 | . 데이터를 관통하는 패턴을 학습, 이에 기반한 예측 수행 | . 모델을 맞춘다? 모델을 학습시키는 기법들에는 딥러닝, 나이브베이즈, 디시전트리 등이 있음, 모델을 맞추는 행위를 하지 않으면 이 모델은 어떤 문제도 해결할 수가 없음, 따라서 데이터를 모델에 맞추는 행위가 필요함 | . 따라서 이 데이터에 문제를 최대한 많이 맞출 수 있도록 모델을 최적화하여야 하는데 이렇게 최적화가 된 모델이 그 데이터에 해당된 문제를 해결할 수 있게 됨 | . 분류 :지도 학습(Supervised Learning), 비지도 학습(Un-supervised Learning), 강화 학습(Reinforcement Learning) 지도학습 $ to$ 분류(Classification)와 회귀(Regression)로 나눌 수 있음 . . data? Garbage In $ to$ Garbage Out : data도 질이 중요하다 | . 데이터를 이해하고 효율적으로 가공,처리,추출하여 최적의 데이터를 기반으로 알고리즘을 구동할 수 있도록 준비하는 능력 필요 | . . 만약 온도, 습도, 풍속을 정리해놓은 데이터가 있을 때 눈이 오는 여부를 다양한 기법으로 해결할 수 있음 . 1) 조건을 정해서 해결한다.(decision tree 등) . 2) 수식(가중치)으로 해결한다.(선형 회귀, 딥러닝 등) . 이러한 접근 방식을 통해서 가지고 있는 데이터를 50%정도만 해결했다면 이 접근 방식들은 썩 좋지 않은 방식일 것임 . 따라서 머신러닝을 학습한다는 것은 이 정답을 최대한 맞출 수 있도록 모델을 최대한 최적화한다는 의미임. 이렇게 가장 좋은 성능이 나올 수 있는 식과 조건을 찾아나가는 것을 기계가 스스로 하는 것을 머신러닝이라고 생각할 수도 있겠음. . . 딥러닝 등 머신러닝 기법들 전반적으로 공부할 필요가 있다 . 딥러닝 : 자연어와 이미지 처리에 강하다. 그렇지만 다른 과업처리에 있어서도 항상 우수한 결과를 도출해내는 것은 아니다. 대표적으로 KAGGLE에 있는 TITANIC자료에서 실제로 산 승객과 죽은 승객을 처리해내는 과업을 수행할 땐 딥러닝보다 머신러닝의 모델이 더 좋은 결과를 도출해냈음 | . &#44208;&#44397; &#47785;&#51201;&#51008; &#45936;&#51060;&#53552;&#47484; &#47784;&#45944;&#50640; &#52572;&#51201;&#54868; &#49884;&#53412;&#45716; &#44163;&#51060;&#45796; . 머신러닝 논문에 머신러닝 기법들간에 성능을 비교한 표도 있음. 즉, 머신러닝 기법들은 다양한 기법들이 있기 때문에 그 것들간의 차이점과, 각각의 알고리즘이 무엇을 최적화하려는 것인지의 관점에서 이해해보아야함 . . . &#54028;&#51060;&#50028; &#47672;&#49888;&#47084;&#45789; &#49373;&#53468;&#44228;&#47484; &#44396;&#49457;&#54616;&#45716; &#51452;&#50836; &#54056;&#53412;&#51648; . 머신러닝 패키지 : Scikit-Learn . | 행렬/ 선형대수/ 통계 패키지 : Numpy, SciPy . | 데이터 핸들링 : Pandas(Numpy는 행렬 기반의 데이터 처리에 특화) $ to$ 2차원 데이터 처리에 특화,Matplotlib . | 시각화 : matplotlib(너무 세분화 되어 있어서 익히기 어려움, 시각적인 면에서도 투박),Seaborn(matplotlib의 대안이 될 것) . | .",
            "url": "https://rhkrehtjd.github.io/INTROml/2021/12/28/intro.html",
            "relUrl": "/2021/12/28/intro.html",
            "date": " • Dec 28, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://rhkrehtjd.github.io/INTROml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rhkrehtjd.github.io/INTROml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}